{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: loss.html\n",
    "title: Loss\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/antonpf/mlmvn/blob/main/mlmvn/loss.py#L14){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ComplexMSELoss\n",
       "\n",
       ">      ComplexMSELoss (*args, **kwargs)\n",
       "\n",
       "Base class to create custom `autograd.Function`\n",
       "\n",
       "To create a custom `autograd.Function`, subclass this class and implement\n",
       "the :meth:`forward` and :meth:`backward` static methods. Then, to use your custom\n",
       "op in the forward pass, call the class method ``apply``. Do not call\n",
       ":meth:`forward` directly.\n",
       "\n",
       "To ensure correctness and best performance, make sure you are calling the\n",
       "correct methods on ``ctx`` and validating your backward function using\n",
       ":func:`torch.autograd.gradcheck`.\n",
       "\n",
       "See :ref:`extending-autograd` for more details on how to use this class.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> class Exp(Function):\n",
       "    >>>     @staticmethod\n",
       "    >>>     def forward(ctx, i):\n",
       "    >>>         result = i.exp()\n",
       "    >>>         ctx.save_for_backward(result)\n",
       "    >>>         return result\n",
       "    >>>\n",
       "    >>>     @staticmethod\n",
       "    >>>     def backward(ctx, grad_output):\n",
       "    >>>         result, = ctx.saved_tensors\n",
       "    >>>         return grad_output * result\n",
       "    >>>\n",
       "    >>> # Use it by calling the apply method:\n",
       "    >>> output = Exp.apply(input)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/antonpf/mlmvn/blob/main/mlmvn/loss.py#L14){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ComplexMSELoss\n",
       "\n",
       ">      ComplexMSELoss (*args, **kwargs)\n",
       "\n",
       "Base class to create custom `autograd.Function`\n",
       "\n",
       "To create a custom `autograd.Function`, subclass this class and implement\n",
       "the :meth:`forward` and :meth:`backward` static methods. Then, to use your custom\n",
       "op in the forward pass, call the class method ``apply``. Do not call\n",
       ":meth:`forward` directly.\n",
       "\n",
       "To ensure correctness and best performance, make sure you are calling the\n",
       "correct methods on ``ctx`` and validating your backward function using\n",
       ":func:`torch.autograd.gradcheck`.\n",
       "\n",
       "See :ref:`extending-autograd` for more details on how to use this class.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> class Exp(Function):\n",
       "    >>>     @staticmethod\n",
       "    >>>     def forward(ctx, i):\n",
       "    >>>         result = i.exp()\n",
       "    >>>         ctx.save_for_backward(result)\n",
       "    >>>         return result\n",
       "    >>>\n",
       "    >>>     @staticmethod\n",
       "    >>>     def backward(ctx, grad_output):\n",
       "    >>>         result, = ctx.saved_tensors\n",
       "    >>>         return grad_output * result\n",
       "    >>>\n",
       "    >>> # Use it by calling the apply method:\n",
       "    >>> output = Exp.apply(input)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |output: asis\n",
    "# | echo: false\n",
    "show_doc(ComplexMSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/antonpf/mlmvn/blob/main/mlmvn/loss.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ComplexMSE_adjusted_error\n",
       "\n",
       ">      ComplexMSE_adjusted_error (*args, **kwargs)\n",
       "\n",
       "Base class to create custom `autograd.Function`\n",
       "\n",
       "To create a custom `autograd.Function`, subclass this class and implement\n",
       "the :meth:`forward` and :meth:`backward` static methods. Then, to use your custom\n",
       "op in the forward pass, call the class method ``apply``. Do not call\n",
       ":meth:`forward` directly.\n",
       "\n",
       "To ensure correctness and best performance, make sure you are calling the\n",
       "correct methods on ``ctx`` and validating your backward function using\n",
       ":func:`torch.autograd.gradcheck`.\n",
       "\n",
       "See :ref:`extending-autograd` for more details on how to use this class.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> class Exp(Function):\n",
       "    >>>     @staticmethod\n",
       "    >>>     def forward(ctx, i):\n",
       "    >>>         result = i.exp()\n",
       "    >>>         ctx.save_for_backward(result)\n",
       "    >>>         return result\n",
       "    >>>\n",
       "    >>>     @staticmethod\n",
       "    >>>     def backward(ctx, grad_output):\n",
       "    >>>         result, = ctx.saved_tensors\n",
       "    >>>         return grad_output * result\n",
       "    >>>\n",
       "    >>> # Use it by calling the apply method:\n",
       "    >>> output = Exp.apply(input)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/antonpf/mlmvn/blob/main/mlmvn/loss.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ComplexMSE_adjusted_error\n",
       "\n",
       ">      ComplexMSE_adjusted_error (*args, **kwargs)\n",
       "\n",
       "Base class to create custom `autograd.Function`\n",
       "\n",
       "To create a custom `autograd.Function`, subclass this class and implement\n",
       "the :meth:`forward` and :meth:`backward` static methods. Then, to use your custom\n",
       "op in the forward pass, call the class method ``apply``. Do not call\n",
       ":meth:`forward` directly.\n",
       "\n",
       "To ensure correctness and best performance, make sure you are calling the\n",
       "correct methods on ``ctx`` and validating your backward function using\n",
       ":func:`torch.autograd.gradcheck`.\n",
       "\n",
       "See :ref:`extending-autograd` for more details on how to use this class.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> class Exp(Function):\n",
       "    >>>     @staticmethod\n",
       "    >>>     def forward(ctx, i):\n",
       "    >>>         result = i.exp()\n",
       "    >>>         ctx.save_for_backward(result)\n",
       "    >>>         return result\n",
       "    >>>\n",
       "    >>>     @staticmethod\n",
       "    >>>     def backward(ctx, grad_output):\n",
       "    >>>         result, = ctx.saved_tensors\n",
       "    >>>         return grad_output * result\n",
       "    >>>\n",
       "    >>> # Use it by calling the apply method:\n",
       "    >>> output = Exp.apply(input)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |output: asis\n",
    "# | echo: false\n",
    "show_doc(ComplexMSE_adjusted_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
