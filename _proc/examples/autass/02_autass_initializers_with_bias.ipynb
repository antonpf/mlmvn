{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: In this example, the main focus is the classification of individual states\n",
    "  of a motor.\n",
    "output-file: autass_initializers_with_bias.html\n",
    "title: Sensorless Drive Diagnosis\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\n",
    "    \"data/autass_data2.csv\",\n",
    "    header=None,\n",
    "    dtype=np.double,\n",
    ")\n",
    "data = np.array(train_csv.values[:, 1:50])\n",
    "del train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "X = data[:, 0:48]\n",
    "y = data[:, 48].astype(int) - 1\n",
    "\n",
    "yt = copy.copy(y)\n",
    "yt[yt == 0] = 20\n",
    "yt[yt == 1] = 21\n",
    "yt[yt == 2] = 22\n",
    "yt[yt == 3] = 23\n",
    "yt[yt == 4] = 26\n",
    "yt[yt == 5] = 24\n",
    "yt[yt == 6] = 27\n",
    "yt[yt == 7] = 29\n",
    "yt[yt == 8] = 30\n",
    "yt[yt == 9] = 25\n",
    "yt[yt == 10] = 28\n",
    "yt -= 20\n",
    "y = yt\n",
    "del yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 538\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "    def initialize_weights(self, initilizer=\"uniform\"):\n",
    "        if initilizer == \"uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n",
    "        elif initilizer == \"normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_normal_independent_(\n",
    "                            m.bias,\n",
    "                        )\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_normal_independent_(\n",
    "                            m.bias,\n",
    "                        )\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_normal_independent_(\n",
    "                            m.bias,\n",
    "                        )\n",
    "        elif initilizer == \"ones\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"zeros\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_normal_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_normal_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_normal_(m.bias)\n",
    "        elif initilizer == \"kaiming_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n",
    "        elif initilizer == \"xavier_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_normal_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_normal_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_normal_(m.bias)\n",
    "        elif initilizer == \"xavier_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_uniform_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_uniform_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_uniform_(m.bias)\n",
    "        elif initilizer == \"trabelsi_standard_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n",
    "        elif initilizer == \"trabelsi_independent_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n",
    "        elif initilizer == \"trabelsi_standard_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n",
    "        elif initilizer == \"trabelsi_independent_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n",
    "        elif initilizer == \"trabelsi_standard_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n",
    "        elif initilizer == \"trabelsi_independent_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n",
    "        elif initilizer == \"trabelsi_standard_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n",
    "        elif initilizer == \"trabelsi_independent_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "initilizers = [\n",
    "    \"uniform\",\n",
    "    \"normal\",\n",
    "    # \"zeros\",\n",
    "    # \"ones\",\n",
    "    \"kaiming_normal\",\n",
    "    \"kaiming_uniform\",\n",
    "    \"xavier_normal\",\n",
    "    \"xavier_uniform\",\n",
    "    \"trabelsi_standard_glorot\",\n",
    "    \"trabelsi_independent_glorot\",\n",
    "    \"trabelsi_standard_xavier\",\n",
    "    \"trabelsi_independent_xavier\",\n",
    "    \"trabelsi_standard_kaiming\",\n",
    "    \"trabelsi_independent_kaiming\",\n",
    "    \"trabelsi_standard_he\",\n",
    "    \"trabelsi_independent_he\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=8c4dc60250654161a4455e9fb49f6061\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/8c4dc60250654161a4455e9fb49f6061/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 20:03:11,212 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.13520533966014758\n",
      "Epoch 19 loss is 0.09242117989507001\n",
      "Epoch 29 loss is 0.06890200001316225\n",
      "Epoch 39 loss is 0.06940312129715898\n",
      "Epoch 49 loss is 0.0739187122910833\n",
      "Train Acc.:  0.9514164850660172\n",
      "Val Acc.:  0.9457404084422798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      1074\n",
      "           1       0.95      0.91      0.93      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.98      0.98      0.98      1048\n",
      "           4       0.95      0.96      0.96      1057\n",
      "           5       0.93      0.94      0.94      1072\n",
      "           6       0.93      0.90      0.92      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.93      0.95      1030\n",
      "          10       0.99      0.96      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.96     11703\n",
      "   macro avg       0.97      0.96      0.96     11703\n",
      "weighted avg       0.97      0.96      0.96     11703\n",
      " samples avg       0.95      0.96      0.95     11703\n",
      "\n",
      "ClearML Task: created new task id=671fc38623e64b04b2183783d6660d82\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/671fc38623e64b04b2183783d6660d82/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.6364161542642706\n",
      "Epoch 19 loss is 0.4933384342885658\n",
      "Epoch 29 loss is 0.2978411113611267\n",
      "Epoch 39 loss is 0.17510296008646237\n",
      "Epoch 49 loss is 0.136787017185157\n",
      "Train Acc.:  0.8634576763662778\n",
      "Val Acc.:  0.8490985217465608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1074\n",
      "           1       0.87      0.85      0.86      1089\n",
      "           2       0.93      0.84      0.88      1044\n",
      "           3       0.96      0.92      0.94      1048\n",
      "           4       0.90      0.84      0.87      1057\n",
      "           5       0.89      0.85      0.87      1072\n",
      "           6       0.89      0.83      0.86      1066\n",
      "           7       0.99      0.99      0.99      1103\n",
      "           8       0.98      1.00      0.99      1108\n",
      "           9       0.91      0.91      0.91      1030\n",
      "          10       0.89      0.77      0.83      1012\n",
      "\n",
      "   micro avg       0.92      0.89      0.91     11703\n",
      "   macro avg       0.92      0.89      0.90     11703\n",
      "weighted avg       0.92      0.89      0.91     11703\n",
      " samples avg       0.87      0.89      0.88     11703\n",
      "\n",
      "ClearML Task: created new task id=cbd1148dc06c4bb484e287631dbecfe1\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/cbd1148dc06c4bb484e287631dbecfe1/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.13064789010576158\n",
      "Epoch 19 loss is 0.08065432691827415\n",
      "Epoch 29 loss is 0.07580616387918908\n",
      "Epoch 39 loss is 0.06079223146693188\n",
      "Epoch 49 loss is 0.05756999806851636\n",
      "Train Acc.:  0.9545784728453617\n",
      "Val Acc.:  0.9430915149961548\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1074\n",
      "           1       0.95      0.94      0.95      1089\n",
      "           2       0.99      0.97      0.98      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.94      0.94      0.94      1057\n",
      "           5       0.95      0.94      0.94      1072\n",
      "           6       0.94      0.92      0.93      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.95      0.93      0.94      1030\n",
      "          10       0.99      0.94      0.96      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.96     11703\n",
      "   macro avg       0.97      0.96      0.96     11703\n",
      "weighted avg       0.97      0.96      0.96     11703\n",
      " samples avg       0.95      0.96      0.95     11703\n",
      "\n",
      "ClearML Task: created new task id=24fb62f34d6045dc84880b3cdb4ff51e\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/24fb62f34d6045dc84880b3cdb4ff51e/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.13044388983633648\n",
      "Epoch 19 loss is 0.08460253840021804\n",
      "Epoch 29 loss is 0.07930393311991951\n",
      "Epoch 39 loss is 0.06527583675469127\n",
      "Epoch 49 loss is 0.05000301778597409\n",
      "Train Acc.:  0.9599196684185788\n",
      "Val Acc.:  0.948645646415449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1074\n",
      "           1       0.95      0.94      0.95      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.95      0.96      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.93      0.92      0.92      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.95      0.94      1030\n",
      "          10       0.99      0.96      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=76f869fd23be4c139728384b8cde4c7a\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/76f869fd23be4c139728384b8cde4c7a/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09251221922659182\n",
      "Epoch 19 loss is 0.06227124288757633\n",
      "Epoch 29 loss is 0.059843142229535094\n",
      "Epoch 39 loss is 0.06101339361081321\n",
      "Epoch 49 loss is 0.05014295614441572\n",
      "Train Acc.:  0.9669059522283467\n",
      "Val Acc.:  0.9546270187131505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.93      0.93      0.93      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.98      0.98      0.98      1048\n",
      "           4       0.97      0.95      0.96      1057\n",
      "           5       0.94      0.93      0.93      1072\n",
      "           6       0.96      0.94      0.95      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.97      0.96      0.96      1030\n",
      "          10       0.99      0.98      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=2f0b154634c74df4adf05315bed073df\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2f0b154634c74df4adf05315bed073df/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.10127652630034135\n",
      "Epoch 19 loss is 0.10470796996730673\n",
      "Epoch 29 loss is 0.061063666924023825\n",
      "Epoch 39 loss is 0.06377833585016535\n",
      "Epoch 49 loss is 0.046801240935354885\n",
      "Train Acc.:  0.9597487501602359\n",
      "Val Acc.:  0.9475348201315902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.95      0.93      0.94      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.96      0.96      0.96      1057\n",
      "           5       0.94      0.94      0.94      1072\n",
      "           6       0.95      0.93      0.94      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.95      0.93      0.94      1030\n",
      "          10       0.99      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=cf0a0472af3947029f29e2ec9a6d83ae\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/cf0a0472af3947029f29e2ec9a6d83ae/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09907775976141676\n",
      "Epoch 19 loss is 0.07054048413195367\n",
      "Epoch 29 loss is 0.05308170731832958\n",
      "Epoch 39 loss is 0.049473569621797074\n",
      "Epoch 49 loss is 0.04445638241988443\n",
      "Train Acc.:  0.9670127761398112\n",
      "Val Acc.:  0.9555669486456464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.96      0.94      0.95      1089\n",
      "           2       1.00      0.99      0.99      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.96      0.95      0.96      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.97      0.91      0.94      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.99      0.96      0.98      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=33833dedc47446eeb1262956c279de8b\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/33833dedc47446eeb1262956c279de8b/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09330733755305037\n",
      "Epoch 19 loss is 0.07534531581462517\n",
      "Epoch 29 loss is 0.05881492036979832\n",
      "Epoch 39 loss is 0.05424361085964104\n",
      "Epoch 49 loss is 0.0515420224426341\n",
      "Train Acc.:  0.9599837627654574\n",
      "Val Acc.:  0.9484747500640861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1074\n",
      "           1       0.94      0.94      0.94      1089\n",
      "           2       0.99      0.98      0.98      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.95      0.95      0.95      1057\n",
      "           5       0.94      0.94      0.94      1072\n",
      "           6       0.94      0.93      0.93      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.95      0.95      1030\n",
      "          10       0.98      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=d2a3805e299c4aabb89fc97016b870fe\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d2a3805e299c4aabb89fc97016b870fe/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09907775976141676\n",
      "Epoch 19 loss is 0.07054048413195367\n",
      "Epoch 29 loss is 0.05308170731832958\n",
      "Epoch 39 loss is 0.049473569621797074\n",
      "Epoch 49 loss is 0.04445638241988443\n",
      "Train Acc.:  0.9670127761398112\n",
      "Val Acc.:  0.9555669486456464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.96      0.94      0.95      1089\n",
      "           2       1.00      0.99      0.99      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.96      0.95      0.96      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.97      0.91      0.94      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.99      0.96      0.98      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=71209c3ecb8f4c288e9ae2fc47316b43\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/71209c3ecb8f4c288e9ae2fc47316b43/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09330733755305037\n",
      "Epoch 19 loss is 0.07534531581462517\n",
      "Epoch 29 loss is 0.05881492036979832\n",
      "Epoch 39 loss is 0.05424361085964104\n",
      "Epoch 49 loss is 0.0515420224426341\n",
      "Train Acc.:  0.9599837627654574\n",
      "Val Acc.:  0.9484747500640861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1074\n",
      "           1       0.94      0.94      0.94      1089\n",
      "           2       0.99      0.98      0.98      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.95      0.95      0.95      1057\n",
      "           5       0.94      0.94      0.94      1072\n",
      "           6       0.94      0.93      0.93      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.95      0.95      1030\n",
      "          10       0.98      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=0d5788d998d14b34b4db688e7e454039\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/0d5788d998d14b34b4db688e7e454039/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12687696724868183\n",
      "Epoch 19 loss is 0.08206688501021006\n",
      "Epoch 29 loss is 0.0666472617605075\n",
      "Epoch 39 loss is 0.05526746285190691\n",
      "Epoch 49 loss is 0.05272579846101851\n",
      "Train Acc.:  0.9609024484040508\n",
      "Val Acc.:  0.9495855763479449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.96      0.91      0.93      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.98      0.97      0.98      1048\n",
      "           4       0.95      0.94      0.94      1057\n",
      "           5       0.94      0.94      0.94      1072\n",
      "           6       0.97      0.93      0.95      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.98      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=dbd3f157afbf49a89e470c1f289309a7\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/dbd3f157afbf49a89e470c1f289309a7/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12607126522572562\n",
      "Epoch 19 loss is 0.08197840312344751\n",
      "Epoch 29 loss is 0.06853510060918785\n",
      "Epoch 39 loss is 0.06352552227624415\n",
      "Epoch 49 loss is 0.05255373344230446\n",
      "Train Acc.:  0.9568431397684057\n",
      "Val Acc.:  0.9483893018884046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1074\n",
      "           1       0.96      0.92      0.94      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.98      0.97      0.98      1048\n",
      "           4       0.96      0.95      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.96      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.94      0.94      1030\n",
      "          10       0.98      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=c9f5e2d5eae84023829ccbb07f66f1c6\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/c9f5e2d5eae84023829ccbb07f66f1c6/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12687696724868183\n",
      "Epoch 19 loss is 0.08206688501021006\n",
      "Epoch 29 loss is 0.0666472617605075\n",
      "Epoch 39 loss is 0.05526746285190691\n",
      "Epoch 49 loss is 0.05272579846101851\n",
      "Train Acc.:  0.9609024484040508\n",
      "Val Acc.:  0.9495855763479449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.96      0.91      0.93      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.98      0.97      0.98      1048\n",
      "           4       0.95      0.94      0.94      1057\n",
      "           5       0.94      0.94      0.94      1072\n",
      "           6       0.97      0.93      0.95      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.98      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=18941c1c20824769905082794a13375e\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/18941c1c20824769905082794a13375e/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12607126522572562\n",
      "Epoch 19 loss is 0.08197840312344751\n",
      "Epoch 29 loss is 0.06853510060918785\n",
      "Epoch 39 loss is 0.06352552227624415\n",
      "Epoch 49 loss is 0.05255373344230446\n",
      "Train Acc.:  0.9568431397684057\n",
      "Val Acc.:  0.9483893018884046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1074\n",
      "           1       0.96      0.92      0.94      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.98      0.97      0.98      1048\n",
      "           4       0.96      0.95      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.96      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.94      0.94      1030\n",
      "          10       0.98      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for initilizer in initilizers:\n",
    "    model = Model(categories=categories, periodicity=periodicity)\n",
    "    model.initialize_weights(initilizer=initilizer)\n",
    "    criterion = ComplexMSELoss.apply\n",
    "    optimizer = ECL(model.parameters(), lr=lr)\n",
    "\n",
    "    task = Task.init(\n",
    "        project_name=\"mlmvn\",\n",
    "        task_name=\"SDD-mlmvn-[48-100-11]\",\n",
    "        tags=[\"mlmvn\", \"SDD\", \"initilizer_with_bias\"],\n",
    "    )\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    #  capture a dictionary of hyperparameters with config\n",
    "    config_dict = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optim\": \"ECL\",\n",
    "        \"categories\": categories,\n",
    "        \"periodicity\": periodicity,\n",
    "        \"layer\": \"[48-100-11]\",\n",
    "        \"initilizer\": initilizer,\n",
    "    }\n",
    "    task.connect(config_dict)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Train Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    print(\"Val Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Val Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "\n",
    "    task.mark_completed()\n",
    "    task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
