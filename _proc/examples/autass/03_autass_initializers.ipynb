{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: In this example, the main focus is the classification of individual states\n",
    "  of a motor.\n",
    "output-file: autass_initializers.html\n",
    "title: Sensorless Drive Diagnosis\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\n",
    "    \"data/autass_data2.csv\",\n",
    "    header=None,\n",
    "    dtype=np.double,\n",
    ")\n",
    "data = np.array(train_csv.values[:, 1:50])\n",
    "del train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "X = data[:, 0:48]\n",
    "y = data[:, 48].astype(int) - 1\n",
    "\n",
    "yt = copy.copy(y)\n",
    "yt[yt == 0] = 20\n",
    "yt[yt == 1] = 21\n",
    "yt[yt == 2] = 22\n",
    "yt[yt == 3] = 23\n",
    "yt[yt == 4] = 26\n",
    "yt[yt == 5] = 24\n",
    "yt[yt == 6] = 27\n",
    "yt[yt == 7] = 29\n",
    "yt[yt == 8] = 30\n",
    "yt[yt == 9] = 25\n",
    "yt[yt == 10] = 28\n",
    "yt -= 20\n",
    "y = yt\n",
    "del yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 538\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "    def initialize_weights(self, initilizer=\"uniform\"):\n",
    "        if initilizer == \"uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"ones\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"zeros\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"xavier_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"xavier_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"standard\":\n",
    "            pass\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "initilizers = [\n",
    "    \"uniform\",\n",
    "    \"normal\",\n",
    "    # \"zeros\",\n",
    "    # \"ones\",\n",
    "    \"kaiming_normal\",\n",
    "    \"kaiming_uniform\",\n",
    "    \"xavier_normal\",\n",
    "    \"xavier_uniform\",\n",
    "    \"trabelsi_standard_glorot\",\n",
    "    \"trabelsi_independent_glorot\",\n",
    "    \"trabelsi_standard_xavier\",\n",
    "    \"trabelsi_independent_xavier\",\n",
    "    \"trabelsi_standard_kaiming\",\n",
    "    \"trabelsi_independent_kaiming\",\n",
    "    \"trabelsi_standard_he\",\n",
    "    \"trabelsi_independent_he\",\n",
    "    \"standard\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=44d8ea5cbf604878b92f402cc8b0eddc\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/44d8ea5cbf604878b92f402cc8b0eddc/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 19:22:00,485 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.12282228621301199\n",
      "Epoch 19 loss is 0.09224345408146964\n",
      "Epoch 29 loss is 0.10864849515390713\n",
      "Epoch 39 loss is 0.0810078859787397\n",
      "Epoch 49 loss is 0.0898406030411643\n",
      "Train Acc.:  0.9418023330342263\n",
      "Val Acc.:  0.9291634623600786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      1074\n",
      "           1       0.92      0.91      0.91      1089\n",
      "           2       0.98      0.96      0.97      1044\n",
      "           3       0.98      0.97      0.98      1048\n",
      "           4       0.96      0.92      0.94      1057\n",
      "           5       0.93      0.92      0.92      1072\n",
      "           6       0.93      0.91      0.92      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.93      0.93      1030\n",
      "          10       0.99      0.94      0.96      1012\n",
      "\n",
      "   micro avg       0.96      0.95      0.96     11703\n",
      "   macro avg       0.96      0.95      0.96     11703\n",
      "weighted avg       0.96      0.95      0.96     11703\n",
      " samples avg       0.94      0.95      0.94     11703\n",
      "\n",
      "ClearML Task: created new task id=2775d88ebe644c9eb35a314339d589e5\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2775d88ebe644c9eb35a314339d589e5/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.6195961883261095\n",
      "Epoch 19 loss is 0.4368921566730075\n",
      "Epoch 29 loss is 0.26458930689298554\n",
      "Epoch 39 loss is 0.18611469218135965\n",
      "Epoch 49 loss is 0.1550662377159612\n",
      "Train Acc.:  0.8358543776438918\n",
      "Val Acc.:  0.8256002734341622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      1074\n",
      "           1       0.85      0.76      0.80      1089\n",
      "           2       0.93      0.79      0.86      1044\n",
      "           3       0.93      0.92      0.92      1048\n",
      "           4       0.91      0.84      0.88      1057\n",
      "           5       0.83      0.80      0.82      1072\n",
      "           6       0.90      0.86      0.88      1066\n",
      "           7       0.98      0.98      0.98      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.88      0.88      0.88      1030\n",
      "          10       0.90      0.80      0.84      1012\n",
      "\n",
      "   micro avg       0.92      0.87      0.89     11703\n",
      "   macro avg       0.92      0.87      0.89     11703\n",
      "weighted avg       0.92      0.87      0.89     11703\n",
      " samples avg       0.85      0.87      0.86     11703\n",
      "\n",
      "ClearML Task: created new task id=8173403b25a34259a0174431dc02b39f\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/8173403b25a34259a0174431dc02b39f/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.13417986131187382\n",
      "Epoch 19 loss is 0.0787341998435113\n",
      "Epoch 29 loss is 0.07534739460774055\n",
      "Epoch 39 loss is 0.05243504478153372\n",
      "Epoch 49 loss is 0.049628723627474024\n",
      "Train Acc.:  0.9610092723155151\n",
      "Val Acc.:  0.9474493719559087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.95      0.95      0.95      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.98      0.97      0.98      1048\n",
      "           4       0.95      0.93      0.94      1057\n",
      "           5       0.96      0.93      0.95      1072\n",
      "           6       0.94      0.92      0.93      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.94      0.95      1030\n",
      "          10       0.98      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=baafad649ee94838be2247186b0ec755\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/baafad649ee94838be2247186b0ec755/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.1285782571954889\n",
      "Epoch 19 loss is 0.09804718683267999\n",
      "Epoch 29 loss is 0.07395459259074805\n",
      "Epoch 39 loss is 0.060897612847309844\n",
      "Epoch 49 loss is 0.07065708804235743\n",
      "Train Acc.:  0.9491945477075588\n",
      "Val Acc.:  0.9430060668204734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.94      0.92      0.93      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.98      0.96      0.97      1048\n",
      "           4       0.95      0.95      0.95      1057\n",
      "           5       0.94      0.93      0.93      1072\n",
      "           6       0.96      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.93      0.94      1030\n",
      "          10       0.99      0.96      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.96     11703\n",
      "   macro avg       0.97      0.96      0.96     11703\n",
      "weighted avg       0.97      0.96      0.96     11703\n",
      " samples avg       0.95      0.96      0.95     11703\n",
      "\n",
      "ClearML Task: created new task id=743c4265b9124dc5aa3806305df1c7cd\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/743c4265b9124dc5aa3806305df1c7cd/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12129981536387262\n",
      "Epoch 19 loss is 0.07506399494782308\n",
      "Epoch 29 loss is 0.06446026524658817\n",
      "Epoch 39 loss is 0.06043589024096862\n",
      "Epoch 49 loss is 0.0524866021964434\n",
      "Train Acc.:  0.9561594667350339\n",
      "Val Acc.:  0.9452277193881911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1074\n",
      "           1       0.96      0.91      0.93      1089\n",
      "           2       0.99      0.98      0.98      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.97      0.95      0.96      1057\n",
      "           5       0.95      0.93      0.94      1072\n",
      "           6       0.96      0.94      0.95      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.95      0.95      1030\n",
      "          10       0.99      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.95     11703\n",
      "\n",
      "ClearML Task: created new task id=b744dddaa16c4529b914b6b3de300c96\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b744dddaa16c4529b914b6b3de300c96/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.1182056404424841\n",
      "Epoch 19 loss is 0.07147076034313422\n",
      "Epoch 29 loss is 0.05620083264376806\n",
      "Epoch 39 loss is 0.05679742307294903\n"
     ]
    }
   ],
   "source": [
    "for initilizer in initilizers:\n",
    "    model = Model(categories=categories, periodicity=periodicity)\n",
    "    model.initialize_weights(initilizer=initilizer)\n",
    "    criterion = ComplexMSELoss.apply\n",
    "    optimizer = ECL(model.parameters(), lr=lr)\n",
    "\n",
    "    task = Task.init(\n",
    "        project_name=\"mlmvn\",\n",
    "        task_name=\"SDD-mlmvn-[48-100-11]\",\n",
    "        tags=[\"mlmvn\", \"SDD\", \"initilizer\"],\n",
    "    )\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    #  capture a dictionary of hyperparameters with config\n",
    "    config_dict = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optim\": \"ECL\",\n",
    "        \"categories\": categories,\n",
    "        \"periodicity\": periodicity,\n",
    "        \"layer\": \"[48-100-11]\",\n",
    "        \"initilizer\": initilizer,\n",
    "    }\n",
    "    task.connect(config_dict)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Train Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    print(\"Val Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Val Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "\n",
    "    task.mark_completed()\n",
    "    task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e1f8f26967ef702a6cdfc7d68710b1723c6bc3c98dd14b3269ab68dc4ed91bd"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
