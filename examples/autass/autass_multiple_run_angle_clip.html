<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="In this example, the main focus is the classification of individual states of a motor.">

<title>mlmvn - Sensorless Drive Diagnosis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="mlmvn - Sensorless Drive Diagnosis">
<meta property="og:description" content="In this example, the main focus is the classification of individual states of a motor.">
<meta property="og:site-name" content="mlmvn">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">mlmvn</span>
    </a>
  </div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Sensorless Drive Diagnosis</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">MLMVN</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../layers.html" class="sidebar-item-text sidebar-link">Layers</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../loss.html" class="sidebar-item-text sidebar-link">Loss</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../optim.html" class="sidebar-item-text sidebar-link">Optimizer</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Examples</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../examples/xor/xor.html" class="sidebar-item-text sidebar-link">XOR</a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Moons</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../examples/moons/moons.html" class="sidebar-item-text sidebar-link">Building a Binary Classifier</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">SDD</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  examples/autass/00_autass.ipynb
  </li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#load-data" id="toc-load-data" class="nav-link active" data-scroll-target="#load-data">Load Data</a></li>
  <li><a href="#config" id="toc-config" class="nav-link" data-scroll-target="#config">Config</a></li>
  <li><a href="#single-layer" id="toc-single-layer" class="nav-link" data-scroll-target="#single-layer">Single Layer</a>
  <ul>
  <li><a href="#mlmvn-48-10-11" id="toc-mlmvn-48-10-11" class="nav-link" data-scroll-target="#mlmvn-48-10-11">MLMVN [48-10-11]</a></li>
  <li><a href="#mlmvn-48-20-11" id="toc-mlmvn-48-20-11" class="nav-link" data-scroll-target="#mlmvn-48-20-11">MLMVN [48-20-11]</a></li>
  <li><a href="#mlmvn-48-50-11" id="toc-mlmvn-48-50-11" class="nav-link" data-scroll-target="#mlmvn-48-50-11">MLMVN [48-50-11]</a></li>
  <li><a href="#mlmvn-48-100-11" id="toc-mlmvn-48-100-11" class="nav-link" data-scroll-target="#mlmvn-48-100-11">MLMVN [48-100-11]</a></li>
  </ul></li>
  <li><a href="#multi-layer" id="toc-multi-layer" class="nav-link" data-scroll-target="#multi-layer">Multi Layer</a>
  <ul>
  <li><a href="#mlmvn-48-10-10-11" id="toc-mlmvn-48-10-10-11" class="nav-link" data-scroll-target="#mlmvn-48-10-10-11">MLMVN [48-10-10-11]</a></li>
  <li><a href="#mlmvn-48-20-20-11" id="toc-mlmvn-48-20-20-11" class="nav-link" data-scroll-target="#mlmvn-48-20-20-11">MLMVN [48-20-20-11]</a></li>
  <li><a href="#mlmvn-48-50-50-11" id="toc-mlmvn-48-50-50-11" class="nav-link" data-scroll-target="#mlmvn-48-50-50-11">MLMVN [48-50-50-11]</a></li>
  <li><a href="#mlmvn-48-100-100-11" id="toc-mlmvn-48-100-100-11" class="nav-link" data-scroll-target="#mlmvn-48-100-100-11">MLMVN [48-100-100-11]</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/antonpf/mlmvn/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Sensorless Drive Diagnosis</h1>
</div>

<div>
  <div class="description">
    In this example, the main focus is the classification of individual states of a motor.
  </div>
</div>


<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="load-data" class="level2">
<h2 class="anchored" data-anchor-id="load-data">Load Data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>train_csv <span class="op">=</span> pd.read_csv(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"data/autass_data2.csv"</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    header<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span>np.double,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(train_csv.values[:, <span class="dv">1</span>:<span class="dv">50</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> train_csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[:, <span class="dv">0</span>:<span class="dv">48</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[:, <span class="dv">48</span>].astype(<span class="bu">int</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>yt <span class="op">=</span> copy.copy(y)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> <span class="dv">21</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">2</span>] <span class="op">=</span> <span class="dv">22</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">3</span>] <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">4</span>] <span class="op">=</span> <span class="dv">26</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">5</span>] <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">6</span>] <span class="op">=</span> <span class="dv">27</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">7</span>] <span class="op">=</span> <span class="dv">29</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">8</span>] <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">9</span>] <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">10</span>] <span class="op">=</span> <span class="dv">28</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>yt <span class="op">-=</span> <span class="dv">20</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> yt</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> yt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="config" class="level2">
<h2 class="anchored" data-anchor-id="config">Config</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">538</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>clip_angle_value <span class="op">=</span> <span class="dv">1000000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="single-layer" class="level2">
<h2 class="anchored" data-anchor-id="single-layer">Single Layer</h2>
<section id="mlmvn-48-10-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-10-11">MLMVN [48-10-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-10-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">10</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-10-11]"</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-10-11]"</span>,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=854e9bc7dc7e496ea841a13f617da653
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/854e9bc7dc7e496ea841a13f617da653/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-10-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 14:25:57,872 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.
Epoch 9 loss is 0.312786317016275
Epoch 19 loss is 0.25568547822538934
Epoch 29 loss is 0.3389669520974066
Epoch 39 loss is 0.28586026849296975
Epoch 49 loss is 0.2642590965695329
Epoch 59 loss is 0.2561917205608428
Epoch 69 loss is 0.2538907377290161
Epoch 79 loss is 0.25494624541893784
Epoch 89 loss is 0.2586601196421547
Epoch 99 loss is 0.26136219948247824
Epoch 109 loss is 0.25920406138033625
Epoch 119 loss is 0.257771638513778
Epoch 129 loss is 0.256717965344817
Epoch 139 loss is 0.2562337465502512
Epoch 149 loss is 0.25599395140123643
Epoch 159 loss is 0.2555283782128066
Epoch 169 loss is 0.25500644305516845
Epoch 179 loss is 0.2546311936985765
Epoch 189 loss is 0.2545335270219525
Epoch 199 loss is 0.2544423936517196
Train Acc.:  0.7605486358877945
              precision    recall  f1-score   support

           0       0.93      0.85      0.89      1063
           1       0.83      0.75      0.79      1064
           2       0.90      0.72      0.80      1064
           3       0.84      0.73      0.78      1064
           4       0.78      0.68      0.73      1064
           5       0.82      0.88      0.85      1063
           6       0.75      0.59      0.66      1064
           7       0.99      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.81      0.81      0.81      1064
          10       0.89      0.87      0.88      1064

   micro avg       0.87      0.81      0.84     11702
   macro avg       0.87      0.81      0.83     11702
weighted avg       0.87      0.81      0.83     11702
 samples avg       0.78      0.81      0.79     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.23875833944773336
Epoch 19 loss is 0.35046091409537855
Epoch 29 loss is 0.3474688264639837
Epoch 39 loss is 0.3460845462685393
Epoch 49 loss is 0.34521615010787904
Epoch 59 loss is 0.34480239729514767
Epoch 69 loss is 0.3445531601110381
Epoch 79 loss is 0.344414452281505
Epoch 89 loss is 0.3443463357528307
Epoch 99 loss is 0.34433482782616937
Epoch 109 loss is 0.3443202594365852
Epoch 119 loss is 0.3443419606632198
Epoch 129 loss is 0.34439574951313695
Epoch 139 loss is 0.34447653188306476
Epoch 149 loss is 0.34455745559313744
Epoch 159 loss is 0.3446490512911633
Epoch 169 loss is 0.34472025490019265
Epoch 179 loss is 0.34483666034343385
Epoch 189 loss is 0.3449189385351294
Epoch 199 loss is 0.34502282438946064
Train Acc.:  0.7681329715640823
              precision    recall  f1-score   support

           0       0.84      0.86      0.85      1063
           1       0.88      0.68      0.77      1064
           2       0.90      0.73      0.81      1064
           3       0.86      0.74      0.80      1063
           4       0.76      0.75      0.76      1064
           5       0.81      0.74      0.77      1064
           6       0.80      0.72      0.75      1064
           7       0.99      0.97      0.98      1064
           8       1.00      0.99      1.00      1064
           9       0.88      0.84      0.86      1064
          10       0.92      0.80      0.86      1064

   micro avg       0.88      0.80      0.84     11702
   macro avg       0.88      0.80      0.84     11702
weighted avg       0.88      0.80      0.84     11702
 samples avg       0.78      0.80      0.79     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2971094108173737
Epoch 19 loss is 0.3869108816673947
Epoch 29 loss is 0.4365067504356039
Epoch 39 loss is 0.4177013765514301
Epoch 49 loss is 0.4124413908694452
Epoch 59 loss is 0.40500117037391986
Epoch 69 loss is 0.41131196047303786
Epoch 79 loss is 0.4181249931351551
Epoch 89 loss is 0.41661642708626356
Epoch 99 loss is 0.4181854146942287
Epoch 109 loss is 0.4133348665799685
Epoch 119 loss is 0.4099116159030149
Epoch 129 loss is 0.4199283736599332
Epoch 139 loss is 0.417771504949945
Epoch 149 loss is 0.4188665644250296
Epoch 159 loss is 0.42150588067988737
Epoch 169 loss is 0.4230460352811647
Epoch 179 loss is 0.4250528931966397
Epoch 189 loss is 0.4252225452870114
Epoch 199 loss is 0.42203162628683427
Train Acc.:  0.7921464738180187
              precision    recall  f1-score   support

           0       0.95      0.88      0.92      1064
           1       0.85      0.66      0.75      1064
           2       0.92      0.80      0.86      1064
           3       0.88      0.83      0.86      1063
           4       0.71      0.83      0.77      1064
           5       0.78      0.88      0.83      1064
           6       0.79      0.72      0.75      1063
           7       1.00      0.97      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.84      0.79      0.81      1064
          10       0.84      0.82      0.83      1064

   micro avg       0.87      0.84      0.85     11702
   macro avg       0.87      0.84      0.85     11702
weighted avg       0.87      0.84      0.85     11702
 samples avg       0.81      0.84      0.82     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2795195632281697
Epoch 19 loss is 0.3375428780831629
Epoch 29 loss is 0.33256474318305307
Epoch 39 loss is 0.33185831863511417
Epoch 49 loss is 0.3295307025439074
Epoch 59 loss is 0.3210891922650079
Epoch 69 loss is 0.32034934104938245
Epoch 79 loss is 0.3163362890807787
Epoch 89 loss is 0.315736489078309
Epoch 99 loss is 0.3155383143134653
Epoch 109 loss is 0.31553327463043673
Epoch 119 loss is 0.31600845927522986
Epoch 129 loss is 0.3150536779122853
Epoch 139 loss is 0.3150594287707738
Epoch 149 loss is 0.31524582921067934
Epoch 159 loss is 0.3153440366348357
Epoch 169 loss is 0.31513214139197515
Epoch 179 loss is 0.3152703075228755
Epoch 189 loss is 0.3215461540110862
Epoch 199 loss is 0.31620214863132784
Train Acc.:  0.7109833999188155
              precision    recall  f1-score   support

           0       0.87      0.95      0.91      1063
           1       0.84      0.76      0.80      1064
           2       0.82      0.71      0.76      1064
           3       0.87      0.84      0.85      1063
           4       0.82      0.53      0.64      1064
           5       0.86      0.74      0.80      1064
           6       0.90      0.02      0.04      1064
           7       1.00      0.97      0.98      1064
           8       0.99      1.00      1.00      1064
           9       0.63      0.93      0.75      1064
          10       0.91      0.84      0.87      1064

   micro avg       0.85      0.75      0.80     11702
   macro avg       0.87      0.75      0.76     11702
weighted avg       0.87      0.75      0.76     11702
 samples avg       0.73      0.75      0.74     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.39677247203032356
Epoch 19 loss is 0.35580593764225016
Epoch 29 loss is 0.42506711288150867
Epoch 39 loss is 0.4258715088317278
Epoch 49 loss is 0.42243423247066575
Epoch 59 loss is 0.4237868170495762
Epoch 69 loss is 0.41925568150605863
Epoch 79 loss is 0.439859722214063
Epoch 89 loss is 0.4537169174860011
Epoch 99 loss is 0.4516644332667944
Epoch 109 loss is 0.4505254539904648
Epoch 119 loss is 0.44988270158956056
Epoch 129 loss is 0.4494370519612547
Epoch 139 loss is 0.449528904207753
Epoch 149 loss is 0.44943834853116904
Epoch 159 loss is 0.449371091649806
Epoch 169 loss is 0.4491654165153552
Epoch 179 loss is 0.4491931206111527
Epoch 189 loss is 0.44876226191301677
Epoch 199 loss is 0.44890430281994864
Train Acc.:  0.6543465720939176
              precision    recall  f1-score   support

           0       0.70      0.86      0.77      1064
           1       0.00      0.00      0.00      1064
           2       0.95      0.68      0.79      1064
           3       0.88      0.71      0.78      1064
           4       0.69      0.60      0.64      1064
           5       0.63      0.84      0.72      1064
           6       0.79      0.65      0.71      1063
           7       0.97      0.98      0.98      1064
           8       1.00      1.00      1.00      1063
           9       0.75      0.75      0.75      1064
          10       0.91      0.83      0.87      1064

   micro avg       0.81      0.72      0.76     11702
   macro avg       0.75      0.72      0.73     11702
weighted avg       0.75      0.72      0.73     11702
 samples avg       0.69      0.72      0.70     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-20-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-20-11">MLMVN [48-20-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-20-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">20</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">20</span>, <span class="dv">11</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-72"><a href="#cb22-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb22-73"><a href="#cb22-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-74"><a href="#cb22-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb22-75"><a href="#cb22-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb22-76"><a href="#cb22-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb22-77"><a href="#cb22-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb22-78"><a href="#cb22-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb22-79"><a href="#cb22-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-80"><a href="#cb22-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb22-81"><a href="#cb22-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb22-82"><a href="#cb22-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb22-83"><a href="#cb22-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-84"><a href="#cb22-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb22-85"><a href="#cb22-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb22-86"><a href="#cb22-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb22-87"><a href="#cb22-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-88"><a href="#cb22-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb22-89"><a href="#cb22-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb22-90"><a href="#cb22-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-91"><a href="#cb22-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb22-92"><a href="#cb22-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb22-93"><a href="#cb22-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-94"><a href="#cb22-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb22-95"><a href="#cb22-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-96"><a href="#cb22-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb22-97"><a href="#cb22-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-20-11]"</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-20-11]"</span>,</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=47852b7bd30c475a9fbb69e5802aea36
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/47852b7bd30c475a9fbb69e5802aea36/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-20-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-76"><a href="#cb26-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-77"><a href="#cb26-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb26-78"><a href="#cb26-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb26-79"><a href="#cb26-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-80"><a href="#cb26-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-81"><a href="#cb26-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb26-82"><a href="#cb26-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb26-83"><a href="#cb26-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-84"><a href="#cb26-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-85"><a href="#cb26-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb26-86"><a href="#cb26-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb26-87"><a href="#cb26-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-88"><a href="#cb26-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb26-89"><a href="#cb26-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb26-90"><a href="#cb26-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb26-91"><a href="#cb26-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 14:42:43,693 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.
Epoch 9 loss is 0.15395791001477882
Epoch 19 loss is 0.15815942352420895
Epoch 29 loss is 0.1521475958209485
Epoch 39 loss is 0.22264992876002446
Epoch 49 loss is 0.2434950175800985
Epoch 59 loss is 0.3000683165649394
Epoch 69 loss is 0.2986102485389353
Epoch 79 loss is 0.2978965348099431
Epoch 89 loss is 0.2976461313968712
Epoch 99 loss is 0.297615187920047
Epoch 109 loss is 0.2975814279553468
Epoch 119 loss is 0.2973692060082367
Epoch 129 loss is 0.2971646093759705
Epoch 139 loss is 0.2969823604569539
Epoch 149 loss is 0.2968226945940804
Epoch 159 loss is 0.29669339181528953
Epoch 169 loss is 0.2965860987914452
Epoch 179 loss is 0.2964960589856526
Epoch 189 loss is 0.296397997745145
Epoch 199 loss is 0.2963285508675669
Train Acc.:  0.8742068494028671
              precision    recall  f1-score   support

           0       0.94      0.90      0.92      1063
           1       0.91      0.89      0.90      1064
           2       0.97      0.95      0.96      1064
           3       0.95      0.93      0.94      1064
           4       0.79      0.76      0.78      1064
           5       0.89      0.90      0.90      1063
           6       0.84      0.84      0.84      1064
           7       1.00      1.00      1.00      1064
           8       1.00      0.99      1.00      1064
           9       0.89      0.87      0.88      1064
          10       0.92      0.96      0.94      1064

   micro avg       0.92      0.91      0.91     11702
   macro avg       0.92      0.91      0.91     11702
weighted avg       0.92      0.91      0.91     11702
 samples avg       0.89      0.91      0.89     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.23371587049756218
Epoch 19 loss is 0.2171053793431578
Epoch 29 loss is 0.2117481726413607
Epoch 39 loss is 0.20548662762905423
Epoch 49 loss is 0.20055165801015304
Epoch 59 loss is 0.19926006465921117
Epoch 69 loss is 0.20121967169245297
Epoch 79 loss is 0.2012524994483403
Epoch 89 loss is 0.2089158789663104
Epoch 99 loss is 0.2069847307533938
Epoch 109 loss is 0.20158804575222558
Epoch 119 loss is 0.19908199121184547
Epoch 129 loss is 0.1983263363973267
Epoch 139 loss is 0.1981246632369139
Epoch 149 loss is 0.19843244794655257
Epoch 159 loss is 0.19990613128620335
Epoch 169 loss is 0.20346991661240654
Epoch 179 loss is 0.20220646457032931
Epoch 189 loss is 0.20162040661533578
Epoch 199 loss is 0.20261586962160896
Train Acc.:  0.7864421988164163
              precision    recall  f1-score   support

           0       0.95      0.91      0.93      1063
           1       0.90      0.78      0.84      1064
           2       0.98      0.66      0.79      1064
           3       0.88      0.91      0.90      1063
           4       0.73      0.60      0.66      1064
           5       0.81      0.92      0.86      1064
           6       0.75      0.77      0.76      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.85      0.87      0.86      1064
          10       0.84      0.82      0.83      1064

   micro avg       0.88      0.84      0.86     11702
   macro avg       0.88      0.84      0.86     11702
weighted avg       0.88      0.84      0.86     11702
 samples avg       0.81      0.84      0.82     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.17359070471532762
Epoch 19 loss is 0.19923888591987557
Epoch 29 loss is 0.22116873452047683
Epoch 39 loss is 0.2029670658449249
Epoch 49 loss is 0.20419508592690006
Epoch 59 loss is 0.19794629333818395
Epoch 69 loss is 0.1952743740865991
Epoch 79 loss is 0.19510413114878267
Epoch 89 loss is 0.19646997157487037
Epoch 99 loss is 0.19606822933983284
Epoch 109 loss is 0.19653955121411834
Epoch 119 loss is 0.19678779104367372
Epoch 129 loss is 0.19675274303142729
Epoch 139 loss is 0.19700162446357028
Epoch 149 loss is 0.1971595217735474
Epoch 159 loss is 0.19742537431793827
Epoch 169 loss is 0.19757406298027266
Epoch 179 loss is 0.1976568692427292
Epoch 189 loss is 0.19808705126678153
Epoch 199 loss is 0.19829968689615954
Train Acc.:  0.8086610976990621
              precision    recall  f1-score   support

           0       0.93      0.90      0.91      1064
           1       0.87      0.82      0.85      1064
           2       0.97      0.91      0.94      1064
           3       0.96      0.92      0.94      1063
           4       0.65      0.71      0.68      1064
           5       0.86      0.87      0.87      1064
           6       0.78      0.75      0.77      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.82      0.80      0.81      1064
          10       0.95      0.90      0.92      1064

   micro avg       0.89      0.87      0.88     11702
   macro avg       0.89      0.87      0.88     11702
weighted avg       0.89      0.87      0.88     11702
 samples avg       0.84      0.87      0.85     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.16241861081194287
Epoch 19 loss is 0.142375697260239
Epoch 29 loss is 0.15385840834181844
Epoch 39 loss is 0.2138264016307598
Epoch 49 loss is 0.2365240586587247
Epoch 59 loss is 0.2929398953187324
Epoch 69 loss is 0.2761381170166951
Epoch 79 loss is 0.3456885180336249
Epoch 89 loss is 0.34346247018042997
Epoch 99 loss is 0.3458485532239313
Epoch 109 loss is 0.3444044925757802
Epoch 119 loss is 0.3440097919960899
Epoch 129 loss is 0.34428306482555965
Epoch 139 loss is 0.3449518206799149
Epoch 149 loss is 0.3461375016053481
Epoch 159 loss is 0.3476872522585145
Epoch 169 loss is 0.34796445004321847
Epoch 179 loss is 0.3482418109346652
Epoch 189 loss is 0.34871055688690134
Epoch 199 loss is 0.3490575960919955
Train Acc.:  0.8840558036191168
              precision    recall  f1-score   support

           0       0.94      0.94      0.94      1063
           1       0.92      0.90      0.91      1064
           2       0.97      0.93      0.95      1064
           3       0.96      0.96      0.96      1063
           4       0.93      0.78      0.85      1064
           5       0.94      0.90      0.92      1064
           6       0.90      0.77      0.83      1064
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.85      0.87      0.86      1064
          10       0.94      0.92      0.93      1064

   micro avg       0.94      0.91      0.92     11702
   macro avg       0.94      0.91      0.92     11702
weighted avg       0.94      0.91      0.92     11702
 samples avg       0.89      0.91      0.90     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.15193523247470855
Epoch 19 loss is 0.1408373010379282
Epoch 29 loss is 0.18472082419796915
Epoch 39 loss is 0.1959035029241411
Epoch 49 loss is 0.21619177741693082
Epoch 59 loss is 0.21511564006011144
Epoch 69 loss is 0.20399205995454375
Epoch 79 loss is 0.20022756204760384
Epoch 89 loss is 0.20033638712455112
Epoch 99 loss is 0.19893451606025606
Epoch 109 loss is 0.19786037468191467
Epoch 119 loss is 0.19890569036312059
Epoch 129 loss is 0.1990134917790664
Epoch 139 loss is 0.19872859569206974
Epoch 149 loss is 0.19751614804725498
Epoch 159 loss is 0.19739665962479105
Epoch 169 loss is 0.19784372444886006
Epoch 179 loss is 0.19111926939075988
Epoch 189 loss is 0.19073564886846614
Epoch 199 loss is 0.18976225840289762
Train Acc.:  0.8771764906958361
              precision    recall  f1-score   support

           0       0.95      0.96      0.95      1064
           1       0.89      0.79      0.84      1064
           2       0.97      0.93      0.95      1064
           3       0.99      0.94      0.96      1064
           4       0.90      0.85      0.88      1064
           5       0.87      0.89      0.88      1064
           6       0.89      0.77      0.83      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.86      0.87      0.87      1064
          10       0.97      0.90      0.93      1064

   micro avg       0.93      0.90      0.92     11702
   macro avg       0.93      0.90      0.92     11702
weighted avg       0.93      0.90      0.92     11702
 samples avg       0.89      0.90      0.89     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-50-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-50-11">MLMVN [48-50-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-50-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">50</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">50</span>, <span class="dv">11</span>)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb39-52"><a href="#cb39-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb39-53"><a href="#cb39-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb39-54"><a href="#cb39-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb39-55"><a href="#cb39-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-56"><a href="#cb39-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb39-57"><a href="#cb39-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb39-58"><a href="#cb39-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb39-59"><a href="#cb39-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-60"><a href="#cb39-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-61"><a href="#cb39-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb39-62"><a href="#cb39-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb39-63"><a href="#cb39-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb39-64"><a href="#cb39-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb39-65"><a href="#cb39-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-66"><a href="#cb39-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-67"><a href="#cb39-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb39-68"><a href="#cb39-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb39-69"><a href="#cb39-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb39-70"><a href="#cb39-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb39-71"><a href="#cb39-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-72"><a href="#cb39-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb39-73"><a href="#cb39-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-74"><a href="#cb39-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb39-75"><a href="#cb39-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb39-76"><a href="#cb39-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb39-77"><a href="#cb39-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb39-78"><a href="#cb39-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb39-79"><a href="#cb39-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-80"><a href="#cb39-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb39-81"><a href="#cb39-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb39-82"><a href="#cb39-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb39-83"><a href="#cb39-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-84"><a href="#cb39-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb39-85"><a href="#cb39-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb39-86"><a href="#cb39-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb39-87"><a href="#cb39-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-88"><a href="#cb39-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb39-89"><a href="#cb39-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb39-90"><a href="#cb39-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-91"><a href="#cb39-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb39-92"><a href="#cb39-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb39-93"><a href="#cb39-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-94"><a href="#cb39-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb39-95"><a href="#cb39-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb39-96"><a href="#cb39-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb39-97"><a href="#cb39-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-50-11]"</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-50-11]"</span>,</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=0e2885b9c6934f79849153fe8c7149d5
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/0e2885b9c6934f79849153fe8c7149d5/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-50-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb43-44"><a href="#cb43-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-45"><a href="#cb43-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb43-46"><a href="#cb43-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb43-47"><a href="#cb43-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-48"><a href="#cb43-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-49"><a href="#cb43-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb43-50"><a href="#cb43-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb43-51"><a href="#cb43-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb43-52"><a href="#cb43-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb43-53"><a href="#cb43-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-54"><a href="#cb43-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb43-55"><a href="#cb43-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb43-56"><a href="#cb43-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb43-57"><a href="#cb43-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb43-58"><a href="#cb43-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb43-59"><a href="#cb43-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-60"><a href="#cb43-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-61"><a href="#cb43-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb43-62"><a href="#cb43-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb43-63"><a href="#cb43-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-64"><a href="#cb43-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-65"><a href="#cb43-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb43-66"><a href="#cb43-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb43-67"><a href="#cb43-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-68"><a href="#cb43-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-69"><a href="#cb43-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb43-70"><a href="#cb43-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb43-71"><a href="#cb43-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-72"><a href="#cb43-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-73"><a href="#cb43-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb43-74"><a href="#cb43-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb43-75"><a href="#cb43-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-76"><a href="#cb43-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-77"><a href="#cb43-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb43-78"><a href="#cb43-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb43-79"><a href="#cb43-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-80"><a href="#cb43-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-81"><a href="#cb43-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb43-82"><a href="#cb43-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb43-83"><a href="#cb43-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-84"><a href="#cb43-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-85"><a href="#cb43-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb43-86"><a href="#cb43-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb43-87"><a href="#cb43-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-88"><a href="#cb43-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb43-89"><a href="#cb43-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb43-90"><a href="#cb43-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb43-91"><a href="#cb43-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 14:56:58,474 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.
Epoch 9 loss is 0.11677723785531834
Epoch 19 loss is 0.08727784719556735
Epoch 29 loss is 0.09157292581128637
Epoch 39 loss is 0.07421681373324716
Epoch 49 loss is 0.07697537707769428
Epoch 59 loss is 0.06723158685548768
Epoch 69 loss is 0.06324904770714807
Epoch 79 loss is 0.0635766536591303
Epoch 89 loss is 0.0689652033514836
Epoch 99 loss is 0.09650615274908997
Epoch 109 loss is 0.07453832805905608
Epoch 119 loss is 0.07929171747710538
Epoch 129 loss is 0.07981405980555767
Epoch 139 loss is 0.09314429308579714
Epoch 149 loss is 0.1007599468837455
Epoch 159 loss is 0.1594258365172202
Epoch 169 loss is 0.16438126926324473
Epoch 179 loss is 0.1535026421169871
Epoch 189 loss is 0.15247239398711698
Epoch 199 loss is 0.1501105166766776
Train Acc.:  0.9464609994231632
              precision    recall  f1-score   support

           0       0.96      0.98      0.97      1063
           1       0.95      0.92      0.93      1064
           2       0.98      0.99      0.99      1064
           3       0.98      0.98      0.98      1064
           4       0.96      0.91      0.94      1064
           5       0.94      0.93      0.93      1063
           6       0.94      0.91      0.92      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.96      0.93      0.94      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.97      0.95      0.96     11702
   macro avg       0.97      0.95      0.96     11702
weighted avg       0.97      0.95      0.96     11702
 samples avg       0.95      0.95      0.95     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.10269231647718571
Epoch 19 loss is 0.0828424365293163
Epoch 29 loss is 0.0755315495091804
Epoch 39 loss is 0.07214960396708685
Epoch 49 loss is 0.06101909084585404
Epoch 59 loss is 0.06839280821097835
Epoch 69 loss is 0.0599125453507835
Epoch 79 loss is 0.07335060013207284
Epoch 89 loss is 0.07783843407293703
Epoch 99 loss is 0.07579400460243982
Epoch 109 loss is 0.08774453313883374
Epoch 119 loss is 0.07845111177806056
Epoch 129 loss is 0.08304869556239905
Epoch 139 loss is 0.09663094311768118
Epoch 149 loss is 0.09574537896135903
Epoch 159 loss is 0.10745339520496501
Epoch 169 loss is 0.12221249713581007
Epoch 179 loss is 0.1238504916702643
Epoch 189 loss is 0.12257097729943099
Epoch 199 loss is 0.12221594344523189
Train Acc.:  0.9447732176811161
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1063
           1       0.96      0.94      0.95      1064
           2       0.98      0.98      0.98      1064
           3       0.99      0.97      0.98      1063
           4       0.92      0.90      0.91      1064
           5       0.94      0.93      0.93      1064
           6       0.94      0.92      0.93      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.96      1064
          10       0.99      0.96      0.98      1064

   micro avg       0.97      0.96      0.96     11702
   macro avg       0.97      0.96      0.96     11702
weighted avg       0.97      0.96      0.96     11702
 samples avg       0.95      0.96      0.95     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.11407119840787741
Epoch 19 loss is 0.0782730350170469
Epoch 29 loss is 0.07717022981852589
Epoch 39 loss is 0.07464963948878549
Epoch 49 loss is 0.07757185254979934
Epoch 59 loss is 0.06594024636591442
Epoch 69 loss is 0.06721012903498128
Epoch 79 loss is 0.06679680268106901
Epoch 89 loss is 0.06374956514109469
Epoch 99 loss is 0.06762114545661016
Epoch 109 loss is 0.06522387791331746
Epoch 119 loss is 0.0730944025786924
Epoch 129 loss is 0.07782279538990577
Epoch 139 loss is 0.08467630408643206
Epoch 149 loss is 0.09103032341738909
Epoch 159 loss is 0.09966637200152119
Epoch 169 loss is 0.09320824764012735
Epoch 179 loss is 0.12129874941876952
Epoch 189 loss is 0.14117765266074325
Epoch 199 loss is 0.1418275397130545
Train Acc.:  0.9381716409938684
              precision    recall  f1-score   support

           0       0.97      0.96      0.96      1064
           1       0.94      0.92      0.93      1064
           2       0.99      0.96      0.98      1064
           3       0.98      0.97      0.97      1063
           4       0.92      0.88      0.90      1064
           5       0.94      0.95      0.95      1064
           6       0.91      0.88      0.89      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.93      0.92      0.92      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.96      0.95      0.95     11702
   macro avg       0.96      0.95      0.95     11702
weighted avg       0.96      0.95      0.95     11702
 samples avg       0.94      0.95      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.11413860654953449
Epoch 19 loss is 0.09837500329198573
Epoch 29 loss is 0.09358380064606914
Epoch 39 loss is 0.09880923285039081
Epoch 49 loss is 0.0952057750942828
Epoch 59 loss is 0.08008669991948515
Epoch 69 loss is 0.08619386331155282
Epoch 79 loss is 0.07746320343579995
Epoch 89 loss is 0.0841977986411612
Epoch 99 loss is 0.08913268603148011
Epoch 109 loss is 0.08841460231699025
Epoch 119 loss is 0.08445271805732564
Epoch 129 loss is 0.09042461018240787
Epoch 139 loss is 0.08969012531848465
Epoch 149 loss is 0.10162866889312593
Epoch 159 loss is 0.09608768953639517
Epoch 169 loss is 0.11210143946301826
Epoch 179 loss is 0.11975461444394792
Epoch 189 loss is 0.11591744085113626
Epoch 199 loss is 0.1207325082889982
Train Acc.:  0.92321661289978
              precision    recall  f1-score   support

           0       0.98      0.95      0.96      1063
           1       0.93      0.93      0.93      1064
           2       0.99      0.99      0.99      1064
           3       0.98      0.96      0.97      1063
           4       0.92      0.86      0.89      1064
           5       0.96      0.94      0.95      1064
           6       0.90      0.83      0.86      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.92      0.92      0.92      1064
          10       0.97      0.96      0.96      1064

   micro avg       0.96      0.94      0.95     11702
   macro avg       0.96      0.94      0.95     11702
weighted avg       0.96      0.94      0.95     11702
 samples avg       0.93      0.94      0.93     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.10585059616812989
Epoch 19 loss is 0.08375488261089946
Epoch 29 loss is 0.0842819689311398
Epoch 39 loss is 0.07327244133203635
Epoch 49 loss is 0.06713747510860289
Epoch 59 loss is 0.06443358422771639
Epoch 69 loss is 0.061379027698900954
Epoch 79 loss is 0.06024192039466105
Epoch 89 loss is 0.06170128209518335
Epoch 99 loss is 0.05731691560047303
Epoch 109 loss is 0.06712788845175123
Epoch 119 loss is 0.06511834431841233
Epoch 129 loss is 0.06352879035121482
Epoch 139 loss is 0.06082366798673906
Epoch 149 loss is 0.07462425566574347
Epoch 159 loss is 0.07087942434442313
Epoch 169 loss is 0.09351090855570425
Epoch 179 loss is 0.08463663381504655
Epoch 189 loss is 0.09110312651864295
Epoch 199 loss is 0.09962031710031462
Train Acc.:  0.951139786784028
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1064
           1       0.96      0.91      0.93      1064
           2       0.98      0.98      0.98      1064
           3       0.98      0.97      0.97      1064
           4       0.95      0.92      0.94      1064
           5       0.94      0.95      0.95      1064
           6       0.93      0.90      0.91      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.96      0.94      0.95      1064
          10       0.97      0.97      0.97      1064

   micro avg       0.97      0.96      0.96     11702
   macro avg       0.97      0.96      0.96     11702
weighted avg       0.97      0.96      0.96     11702
 samples avg       0.95      0.96      0.95     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-100-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-100-11">MLMVN [48-100-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-100-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">100</span>)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">100</span>, <span class="dv">11</span>)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-60"><a href="#cb56-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-61"><a href="#cb56-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb56-62"><a href="#cb56-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb56-63"><a href="#cb56-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb56-64"><a href="#cb56-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb56-65"><a href="#cb56-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-66"><a href="#cb56-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-67"><a href="#cb56-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb56-68"><a href="#cb56-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb56-69"><a href="#cb56-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb56-70"><a href="#cb56-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb56-71"><a href="#cb56-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-72"><a href="#cb56-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb56-73"><a href="#cb56-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-74"><a href="#cb56-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb56-75"><a href="#cb56-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb56-76"><a href="#cb56-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb56-77"><a href="#cb56-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb56-78"><a href="#cb56-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb56-79"><a href="#cb56-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-80"><a href="#cb56-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb56-81"><a href="#cb56-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb56-82"><a href="#cb56-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb56-83"><a href="#cb56-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-84"><a href="#cb56-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb56-85"><a href="#cb56-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb56-86"><a href="#cb56-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb56-87"><a href="#cb56-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-88"><a href="#cb56-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb56-89"><a href="#cb56-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb56-90"><a href="#cb56-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb56-91"><a href="#cb56-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb56-92"><a href="#cb56-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb56-93"><a href="#cb56-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-94"><a href="#cb56-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb56-95"><a href="#cb56-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb56-96"><a href="#cb56-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb56-97"><a href="#cb56-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-100-11]"</span>,</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-100-11]"</span>,</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=02f388d5c7e34bd6b473b117ea6509ff
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/02f388d5c7e34bd6b473b117ea6509ff/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-100-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-45"><a href="#cb60-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb60-46"><a href="#cb60-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb60-47"><a href="#cb60-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb60-48"><a href="#cb60-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb60-49"><a href="#cb60-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb60-50"><a href="#cb60-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb60-51"><a href="#cb60-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb60-52"><a href="#cb60-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb60-53"><a href="#cb60-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb60-54"><a href="#cb60-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb60-55"><a href="#cb60-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb60-56"><a href="#cb60-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb60-57"><a href="#cb60-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb60-58"><a href="#cb60-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb60-59"><a href="#cb60-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-60"><a href="#cb60-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-61"><a href="#cb60-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb60-62"><a href="#cb60-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb60-63"><a href="#cb60-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-64"><a href="#cb60-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-65"><a href="#cb60-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb60-66"><a href="#cb60-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb60-67"><a href="#cb60-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-68"><a href="#cb60-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-69"><a href="#cb60-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb60-70"><a href="#cb60-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb60-71"><a href="#cb60-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-72"><a href="#cb60-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-73"><a href="#cb60-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb60-74"><a href="#cb60-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb60-75"><a href="#cb60-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-76"><a href="#cb60-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-77"><a href="#cb60-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb60-78"><a href="#cb60-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb60-79"><a href="#cb60-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-80"><a href="#cb60-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-81"><a href="#cb60-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb60-82"><a href="#cb60-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb60-83"><a href="#cb60-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-84"><a href="#cb60-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-85"><a href="#cb60-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb60-86"><a href="#cb60-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb60-87"><a href="#cb60-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-88"><a href="#cb60-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb60-89"><a href="#cb60-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb60-90"><a href="#cb60-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb60-91"><a href="#cb60-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 15:16:17,894 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.
Epoch 9 loss is 0.08736984969525521
Epoch 19 loss is 0.07819826650816623
Epoch 29 loss is 0.06782916854065665
Epoch 39 loss is 0.04816816897126411
Epoch 49 loss is 0.05163560574118214
Epoch 59 loss is 0.04627741363342768
Epoch 69 loss is 0.04440861685817132
Epoch 79 loss is 0.03960527436632251
Epoch 89 loss is 0.040994377833916226
Epoch 99 loss is 0.03771855142496925
Epoch 109 loss is 0.037066781398096174
Epoch 119 loss is 0.03917588771049437
Epoch 129 loss is 0.03853848459391027
Epoch 139 loss is 0.039490334296889244
Epoch 149 loss is 0.03568498748548023
Epoch 159 loss is 0.037056578648648794
Epoch 169 loss is 0.03705010800859619
Epoch 179 loss is 0.03907089468945371
Epoch 189 loss is 0.0396113535151825
Epoch 199 loss is 0.04512131274939002
Train Acc.:  0.9723759266776337
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1063
           1       0.95      0.95      0.95      1064
           2       0.99      0.98      0.98      1064
           3       0.99      0.98      0.99      1064
           4       0.97      0.95      0.96      1064
           5       0.96      0.94      0.95      1063
           6       0.96      0.95      0.96      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.95      0.96      1064
          10       0.98      0.97      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.10348894731392981
Epoch 19 loss is 0.06993990700555251
Epoch 29 loss is 0.06444708635591645
Epoch 39 loss is 0.06056610582579465
Epoch 49 loss is 0.04929898078856764
Epoch 59 loss is 0.05208108105377289
Epoch 69 loss is 0.055182737210477444
Epoch 79 loss is 0.053781560456890025
Epoch 89 loss is 0.041558242582498066
Epoch 99 loss is 0.04481064647571699
Epoch 109 loss is 0.04177463603363158
Epoch 119 loss is 0.03897786006229341
Epoch 129 loss is 0.042479025262170365
Epoch 139 loss is 0.04130484690661847
Epoch 149 loss is 0.043655684630288405
Epoch 159 loss is 0.044328333606525876
Epoch 169 loss is 0.04163741938029155
Epoch 179 loss is 0.03787117794917678
Epoch 189 loss is 0.04063373131211509
Epoch 199 loss is 0.045135088320266366
Train Acc.:  0.9656247997094451
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1063
           1       0.96      0.94      0.95      1064
           2       0.99      0.98      0.99      1064
           3       0.98      0.97      0.98      1063
           4       0.94      0.94      0.94      1064
           5       0.96      0.97      0.96      1064
           6       0.95      0.95      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.93      0.95      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.97      0.97      0.97     11702
   macro avg       0.97      0.97      0.97     11702
weighted avg       0.97      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.09721067817250643
Epoch 19 loss is 0.0659285953883801
Epoch 29 loss is 0.05613079095748313
Epoch 39 loss is 0.053254430131788696
Epoch 49 loss is 0.050631655931046286
Epoch 59 loss is 0.043587857389422176
Epoch 69 loss is 0.04570086010022542
Epoch 79 loss is 0.039479428673811066
Epoch 89 loss is 0.03776955057557153
Epoch 99 loss is 0.04120841258334559
Epoch 109 loss is 0.05813870002363725
Epoch 119 loss is 0.04555968065778827
Epoch 129 loss is 0.03607066082124637
Epoch 139 loss is 0.03758934199215175
Epoch 149 loss is 0.033209230217276724
Epoch 159 loss is 0.02871578698680781
Epoch 169 loss is 0.029964328920410958
Epoch 179 loss is 0.03051551401962286
Epoch 189 loss is 0.031670732550214034
Epoch 199 loss is 0.03487335515855483
Train Acc.:  0.9769478924092551
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1064
           1       0.97      0.97      0.97      1064
           2       0.99      0.98      0.99      1064
           3       0.98      0.98      0.98      1063
           4       0.97      0.96      0.97      1064
           5       0.97      0.97      0.97      1064
           6       0.96      0.94      0.95      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.95      0.96      1064
          10       0.98      0.97      0.98      1064

   micro avg       0.98      0.97      0.98     11702
   macro avg       0.98      0.97      0.98     11702
weighted avg       0.98      0.97      0.98     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.09506221192044154
Epoch 19 loss is 0.06567186224064278
Epoch 29 loss is 0.05404001884322595
Epoch 39 loss is 0.05249845497810033
Epoch 49 loss is 0.04073885316623995
Epoch 59 loss is 0.040838370071218995
Epoch 69 loss is 0.03615846274442404
Epoch 79 loss is 0.030689715662826193
Epoch 89 loss is 0.03348275354964435
Epoch 99 loss is 0.04012077295457501
Epoch 109 loss is 0.034105905516408
Epoch 119 loss is 0.032124319295064915
Epoch 129 loss is 0.03206602824675486
Epoch 139 loss is 0.0336496624931189
Epoch 149 loss is 0.030495164592908134
Epoch 159 loss is 0.0336733646851268
Epoch 169 loss is 0.03706312464641834
Epoch 179 loss is 0.034242405249922364
Epoch 189 loss is 0.03487617796112035
Epoch 199 loss is 0.031548788032238144
Train Acc.:  0.9773965432520777
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      1063
           1       0.95      0.96      0.95      1064
           2       0.99      0.99      0.99      1064
           3       0.99      0.98      0.98      1063
           4       0.97      0.97      0.97      1064
           5       0.96      0.94      0.95      1064
           6       0.96      0.95      0.96      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.98      0.97      0.97      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.98      0.97      0.98     11702
   macro avg       0.98      0.97      0.98     11702
weighted avg       0.98      0.97      0.98     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.09597225538158832
Epoch 19 loss is 0.07305103076821956
Epoch 29 loss is 0.06610023701716355
Epoch 39 loss is 0.058439643653002006
Epoch 49 loss is 0.047609421255355604
Epoch 59 loss is 0.050110073609477744
Epoch 69 loss is 0.044684631060736904
Epoch 79 loss is 0.041153442952646245
Epoch 89 loss is 0.03982352230327579
Epoch 99 loss is 0.036985448655858864
Epoch 109 loss is 0.04060346388820211
Epoch 119 loss is 0.0447469210668093
Epoch 129 loss is 0.03394044590851765
Epoch 139 loss is 0.03358597226108189
Epoch 149 loss is 0.027238770870941995
Epoch 159 loss is 0.029254705460162258
Epoch 169 loss is 0.05193828537228054
Epoch 179 loss is 0.030730149318143475
Epoch 189 loss is 0.03086909795063045
Epoch 199 loss is 0.03218570903393944
Train Acc.:  0.9781229303309334
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1064
           1       0.95      0.97      0.96      1064
           2       0.99      0.99      0.99      1064
           3       0.99      0.98      0.99      1064
           4       0.97      0.96      0.97      1064
           5       0.97      0.96      0.97      1064
           6       0.96      0.95      0.96      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.98      0.97      0.97      1064
          10       0.98      0.98      0.98      1064

   micro avg       0.98      0.98      0.98     11702
   macro avg       0.98      0.98      0.98     11702
weighted avg       0.98      0.98      0.98     11702
 samples avg       0.97      0.98      0.97     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="multi-layer" class="level2">
<h2 class="anchored" data-anchor-id="multi-layer">Multi Layer</h2>
<section id="mlmvn-48-10-10-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-10-10-11">MLMVN [48-10-10-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-10-10-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">10</span>)</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-38"><a href="#cb73-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb73-39"><a href="#cb73-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb73-40"><a href="#cb73-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-41"><a href="#cb73-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb73-42"><a href="#cb73-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb73-43"><a href="#cb73-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb73-44"><a href="#cb73-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-45"><a href="#cb73-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb73-46"><a href="#cb73-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb73-47"><a href="#cb73-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb73-48"><a href="#cb73-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-49"><a href="#cb73-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb73-50"><a href="#cb73-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb73-51"><a href="#cb73-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb73-52"><a href="#cb73-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-53"><a href="#cb73-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb73-54"><a href="#cb73-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb73-55"><a href="#cb73-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb73-56"><a href="#cb73-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-57"><a href="#cb73-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb73-58"><a href="#cb73-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb73-59"><a href="#cb73-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb73-60"><a href="#cb73-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb73-61"><a href="#cb73-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb73-62"><a href="#cb73-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-63"><a href="#cb73-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb73-64"><a href="#cb73-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb73-65"><a href="#cb73-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb73-66"><a href="#cb73-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-67"><a href="#cb73-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-68"><a href="#cb73-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb73-69"><a href="#cb73-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb73-70"><a href="#cb73-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb73-71"><a href="#cb73-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb73-72"><a href="#cb73-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb73-73"><a href="#cb73-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-74"><a href="#cb73-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb73-75"><a href="#cb73-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb73-76"><a href="#cb73-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb73-77"><a href="#cb73-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb73-78"><a href="#cb73-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-79"><a href="#cb73-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb73-80"><a href="#cb73-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-81"><a href="#cb73-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb73-82"><a href="#cb73-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb73-83"><a href="#cb73-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb73-84"><a href="#cb73-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb73-85"><a href="#cb73-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb73-86"><a href="#cb73-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-87"><a href="#cb73-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb73-88"><a href="#cb73-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb73-89"><a href="#cb73-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb73-90"><a href="#cb73-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-91"><a href="#cb73-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb73-92"><a href="#cb73-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb73-93"><a href="#cb73-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb73-94"><a href="#cb73-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-95"><a href="#cb73-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb73-96"><a href="#cb73-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb73-97"><a href="#cb73-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb73-98"><a href="#cb73-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb73-99"><a href="#cb73-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb73-100"><a href="#cb73-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-101"><a href="#cb73-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb73-102"><a href="#cb73-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb73-103"><a href="#cb73-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb73-104"><a href="#cb73-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-10-10-11]"</span>,</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-10-10-11]"</span>,</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=d09b15b39d9d46629a9dd600951fbd03
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d09b15b39d9d46629a9dd600951fbd03/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-10-10-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb77-29"><a href="#cb77-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb77-30"><a href="#cb77-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb77-31"><a href="#cb77-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-32"><a href="#cb77-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb77-33"><a href="#cb77-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-34"><a href="#cb77-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb77-35"><a href="#cb77-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb77-36"><a href="#cb77-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-37"><a href="#cb77-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb77-38"><a href="#cb77-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb77-39"><a href="#cb77-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb77-40"><a href="#cb77-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-41"><a href="#cb77-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb77-42"><a href="#cb77-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb77-43"><a href="#cb77-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb77-44"><a href="#cb77-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-45"><a href="#cb77-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb77-46"><a href="#cb77-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb77-47"><a href="#cb77-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb77-48"><a href="#cb77-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb77-49"><a href="#cb77-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb77-50"><a href="#cb77-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb77-51"><a href="#cb77-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb77-52"><a href="#cb77-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb77-53"><a href="#cb77-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb77-54"><a href="#cb77-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb77-55"><a href="#cb77-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb77-56"><a href="#cb77-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb77-57"><a href="#cb77-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb77-58"><a href="#cb77-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb77-59"><a href="#cb77-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-60"><a href="#cb77-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-61"><a href="#cb77-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb77-62"><a href="#cb77-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb77-63"><a href="#cb77-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-64"><a href="#cb77-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-65"><a href="#cb77-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb77-66"><a href="#cb77-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb77-67"><a href="#cb77-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-68"><a href="#cb77-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-69"><a href="#cb77-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb77-70"><a href="#cb77-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb77-71"><a href="#cb77-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-72"><a href="#cb77-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-73"><a href="#cb77-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb77-74"><a href="#cb77-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb77-75"><a href="#cb77-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-76"><a href="#cb77-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-77"><a href="#cb77-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb77-78"><a href="#cb77-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb77-79"><a href="#cb77-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-80"><a href="#cb77-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-81"><a href="#cb77-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb77-82"><a href="#cb77-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb77-83"><a href="#cb77-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-84"><a href="#cb77-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-85"><a href="#cb77-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb77-86"><a href="#cb77-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb77-87"><a href="#cb77-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-88"><a href="#cb77-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb77-89"><a href="#cb77-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb77-90"><a href="#cb77-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb77-91"><a href="#cb77-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 15:46:41,689 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.
Epoch 9 loss is 0.36936323919586267
Epoch 19 loss is 0.4417360333030594
Epoch 29 loss is 0.4639713755873071
Epoch 39 loss is 0.47806268155361914
Epoch 49 loss is 0.46751608711861314
Epoch 59 loss is 0.4455429524911341
Epoch 69 loss is 0.44326834924033726
Epoch 79 loss is 0.4069394754230474
Epoch 89 loss is 0.41501924604142476
Epoch 99 loss is 0.4060409507954031
Epoch 109 loss is 0.4149785299989848
Epoch 119 loss is 0.4198482804118088
Epoch 129 loss is 0.4451440663334436
Epoch 139 loss is 0.48754778360762985
Epoch 149 loss is 0.512868387767623
Epoch 159 loss is 0.4912468525778975
Epoch 169 loss is 0.4819266422257403
Epoch 179 loss is 0.503401190978166
Epoch 189 loss is 0.475582148562929
Epoch 199 loss is 0.47280443258388355
Train Acc.:  0.6744931313692396
              precision    recall  f1-score   support

           0       0.86      0.75      0.80      1063
           1       0.85      0.75      0.80      1064
           2       0.81      0.84      0.82      1064
           3       0.85      0.79      0.82      1064
           4       0.67      0.40      0.50      1064
           5       0.77      0.86      0.82      1063
           6       0.67      0.43      0.53      1064
           7       0.98      0.96      0.97      1064
           8       1.00      1.00      1.00      1064
           9       0.68      0.41      0.51      1064
          10       0.81      0.86      0.83      1064

   micro avg       0.83      0.73      0.78     11702
   macro avg       0.81      0.73      0.76     11702
weighted avg       0.81      0.73      0.76     11702
 samples avg       0.70      0.73      0.71     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3120843412146201
Epoch 19 loss is 0.338843757118815
Epoch 29 loss is 0.29489357968814983
Epoch 39 loss is 0.3142865816273397
Epoch 49 loss is 0.33635124551797807
Epoch 59 loss is 0.37067740493614026
Epoch 69 loss is 0.336927943126264
Epoch 79 loss is 0.30800579708110587
Epoch 89 loss is 0.30535664010364083
Epoch 99 loss is 0.2919258725294692
Epoch 109 loss is 0.2816264724044177
Epoch 119 loss is 0.310759231961488
Epoch 129 loss is 0.31188458260685364
Epoch 139 loss is 0.3267504583368058
Epoch 149 loss is 0.32820414042444124
Epoch 159 loss is 0.3265853992645182
Epoch 169 loss is 0.30502072256656815
Epoch 179 loss is 0.31455467220775174
Epoch 189 loss is 0.2898721081722409
Epoch 199 loss is 0.31343407820932145
Train Acc.:  0.6747922319311214
              precision    recall  f1-score   support

           0       0.91      0.87      0.89      1063
           1       0.86      0.73      0.79      1064
           2       0.92      0.57      0.70      1064
           3       0.88      0.81      0.85      1063
           4       0.72      0.70      0.71      1064
           5       0.83      0.80      0.81      1064
           6       0.39      0.97      0.56      1064
           7       0.99      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.83      0.83      0.83      1064
          10       0.77      0.78      0.77      1064

   micro avg       0.77      0.82      0.80     11702
   macro avg       0.83      0.82      0.81     11702
weighted avg       0.83      0.82      0.81     11702
 samples avg       0.75      0.82      0.77     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.5730218218958721
Epoch 19 loss is 0.5197506032800867
Epoch 29 loss is 0.5308330161877794
Epoch 39 loss is 0.5457173549411902
Epoch 49 loss is 0.4674426359824832
Epoch 59 loss is 0.48413304547490904
Epoch 69 loss is 0.5169540288793292
Epoch 79 loss is 0.46700861763673146
Epoch 89 loss is 0.42243541114495425
Epoch 99 loss is 0.441465108668188
Epoch 109 loss is 0.420622401289878
Epoch 119 loss is 0.48287951270030366
Epoch 129 loss is 0.4898719847934528
Epoch 139 loss is 0.511257985603324
Epoch 149 loss is 0.5320171382337141
Epoch 159 loss is 0.482352293969556
Epoch 169 loss is 0.5216527805105815
Epoch 179 loss is 0.5349811485542204
Epoch 189 loss is 0.5128652697548396
Epoch 199 loss is 0.5348841235805307
Train Acc.:  0.4734120964812955
              precision    recall  f1-score   support

           0       0.47      0.97      0.63      1064
           1       0.52      0.46      0.49      1064
           2       0.74      0.67      0.71      1064
           3       0.74      0.56      0.63      1063
           4       0.55      0.25      0.34      1064
           5       0.68      0.79      0.73      1064
           6       0.31      0.02      0.03      1063
           7       0.99      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.45      0.01      0.02      1064
          10       0.83      0.63      0.72      1064

   micro avg       0.70      0.58      0.63     11702
   macro avg       0.66      0.58      0.57     11702
weighted avg       0.66      0.58      0.57     11702
 samples avg       0.52      0.58      0.54     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.4130547185457221
Epoch 19 loss is 0.28443910819247403
Epoch 29 loss is 0.2840811952740083
Epoch 39 loss is 0.3201059445456724
Epoch 49 loss is 0.28015486618309876
Epoch 59 loss is 0.31602913086881074
Epoch 69 loss is 0.27090680930739386
Epoch 79 loss is 0.2440252615249398
Epoch 89 loss is 0.2710016949262824
Epoch 99 loss is 0.27676315756716174
Epoch 109 loss is 0.26114457901684446
Epoch 119 loss is 0.3384359963182098
Epoch 129 loss is 0.3536610052551853
Epoch 139 loss is 0.32877881416825233
Epoch 149 loss is 0.3546631938786416
Epoch 159 loss is 0.3654947728190782
Epoch 169 loss is 0.34189606918074955
Epoch 179 loss is 0.34633550078607206
Epoch 189 loss is 0.3490021138615275
Epoch 199 loss is 0.3347815447852944
Train Acc.:  0.7526438353237763
              precision    recall  f1-score   support

           0       0.73      0.98      0.84      1063
           1       0.90      0.66      0.76      1064
           2       0.93      0.83      0.88      1064
           3       0.94      0.82      0.87      1063
           4       0.66      0.60      0.63      1064
           5       0.80      0.89      0.84      1064
           6       0.89      0.43      0.58      1064
           7       1.00      0.97      0.98      1064
           8       1.00      0.97      0.99      1064
           9       0.75      0.94      0.83      1064
          10       0.84      0.71      0.77      1064

   micro avg       0.85      0.80      0.82     11702
   macro avg       0.86      0.80      0.81     11702
weighted avg       0.86      0.80      0.81     11702
 samples avg       0.78      0.80      0.78     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.28021247108809677
Epoch 19 loss is 0.24448510212480346
Epoch 29 loss is 0.25997805330578505
Epoch 39 loss is 0.2997642359869558
Epoch 49 loss is 0.284409333111343
Epoch 59 loss is 0.29594549671485526
Epoch 69 loss is 0.27488304189669654
Epoch 79 loss is 0.3148477242705397
Epoch 89 loss is 0.30100965217864945
Epoch 99 loss is 0.34245337392046377
Epoch 109 loss is 0.3656904773209293
Epoch 119 loss is 0.3708399227898303
Epoch 129 loss is 0.3596192533165072
Epoch 139 loss is 0.321385041998874
Epoch 149 loss is 0.3122428807215139
Epoch 159 loss is 0.33298053660486926
Epoch 169 loss is 0.2991883369131804
Epoch 179 loss is 0.31971790791167104
Epoch 189 loss is 0.299987522555066
Epoch 199 loss is 0.31500240064659873
Train Acc.:  0.7926378533125388
              precision    recall  f1-score   support

           0       0.86      0.89      0.88      1064
           1       0.87      0.85      0.86      1064
           2       0.92      0.93      0.92      1064
           3       0.90      0.89      0.90      1064
           4       0.77      0.46      0.58      1064
           5       0.78      0.91      0.84      1064
           6       0.77      0.76      0.77      1063
           7       0.99      0.99      0.99      1064
           8       1.00      1.00      1.00      1063
           9       0.85      0.85      0.85      1064
          10       0.87      0.75      0.81      1064

   micro avg       0.87      0.84      0.86     11702
   macro avg       0.87      0.84      0.85     11702
weighted avg       0.87      0.84      0.85     11702
 samples avg       0.82      0.84      0.83     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-20-20-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-20-20-11">MLMVN [48-20-20-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-20-20-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">20</span>)</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">20</span>, <span class="dv">11</span>)</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb90-24"><a href="#cb90-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb90-25"><a href="#cb90-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb90-26"><a href="#cb90-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb90-27"><a href="#cb90-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb90-28"><a href="#cb90-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb90-29"><a href="#cb90-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb90-30"><a href="#cb90-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb90-31"><a href="#cb90-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-32"><a href="#cb90-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb90-33"><a href="#cb90-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb90-34"><a href="#cb90-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-35"><a href="#cb90-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb90-36"><a href="#cb90-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb90-37"><a href="#cb90-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-38"><a href="#cb90-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb90-39"><a href="#cb90-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb90-40"><a href="#cb90-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-41"><a href="#cb90-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb90-42"><a href="#cb90-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb90-43"><a href="#cb90-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb90-44"><a href="#cb90-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-45"><a href="#cb90-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb90-46"><a href="#cb90-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb90-47"><a href="#cb90-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb90-48"><a href="#cb90-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-49"><a href="#cb90-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb90-50"><a href="#cb90-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb90-51"><a href="#cb90-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb90-52"><a href="#cb90-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-53"><a href="#cb90-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb90-54"><a href="#cb90-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb90-55"><a href="#cb90-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb90-56"><a href="#cb90-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-57"><a href="#cb90-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb90-58"><a href="#cb90-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb90-59"><a href="#cb90-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb90-60"><a href="#cb90-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb90-61"><a href="#cb90-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb90-62"><a href="#cb90-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-63"><a href="#cb90-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb90-64"><a href="#cb90-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb90-65"><a href="#cb90-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb90-66"><a href="#cb90-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-67"><a href="#cb90-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-68"><a href="#cb90-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb90-69"><a href="#cb90-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb90-70"><a href="#cb90-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb90-71"><a href="#cb90-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb90-72"><a href="#cb90-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb90-73"><a href="#cb90-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-74"><a href="#cb90-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb90-75"><a href="#cb90-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb90-76"><a href="#cb90-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb90-77"><a href="#cb90-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb90-78"><a href="#cb90-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-79"><a href="#cb90-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb90-80"><a href="#cb90-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-81"><a href="#cb90-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb90-82"><a href="#cb90-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb90-83"><a href="#cb90-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb90-84"><a href="#cb90-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb90-85"><a href="#cb90-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb90-86"><a href="#cb90-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-87"><a href="#cb90-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb90-88"><a href="#cb90-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb90-89"><a href="#cb90-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb90-90"><a href="#cb90-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-91"><a href="#cb90-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb90-92"><a href="#cb90-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb90-93"><a href="#cb90-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb90-94"><a href="#cb90-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-95"><a href="#cb90-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb90-96"><a href="#cb90-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb90-97"><a href="#cb90-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb90-98"><a href="#cb90-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb90-99"><a href="#cb90-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb90-100"><a href="#cb90-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-101"><a href="#cb90-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb90-102"><a href="#cb90-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb90-103"><a href="#cb90-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb90-104"><a href="#cb90-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-20-20-11]"</span>,</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-20-20-11]"</span>,</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=1ffe99f9f8014fe59562e6e70ac4e4ac
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1ffe99f9f8014fe59562e6e70ac4e4ac/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-20-20-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb94-21"><a href="#cb94-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb94-22"><a href="#cb94-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb94-23"><a href="#cb94-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb94-24"><a href="#cb94-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb94-25"><a href="#cb94-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb94-26"><a href="#cb94-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb94-27"><a href="#cb94-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb94-28"><a href="#cb94-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb94-29"><a href="#cb94-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb94-30"><a href="#cb94-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-31"><a href="#cb94-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-32"><a href="#cb94-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb94-33"><a href="#cb94-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-34"><a href="#cb94-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb94-35"><a href="#cb94-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb94-36"><a href="#cb94-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-37"><a href="#cb94-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb94-38"><a href="#cb94-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb94-39"><a href="#cb94-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb94-40"><a href="#cb94-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-41"><a href="#cb94-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb94-42"><a href="#cb94-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb94-43"><a href="#cb94-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb94-44"><a href="#cb94-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-45"><a href="#cb94-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb94-46"><a href="#cb94-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb94-47"><a href="#cb94-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb94-48"><a href="#cb94-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-49"><a href="#cb94-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb94-50"><a href="#cb94-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb94-51"><a href="#cb94-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb94-52"><a href="#cb94-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-53"><a href="#cb94-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-54"><a href="#cb94-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb94-55"><a href="#cb94-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb94-56"><a href="#cb94-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb94-57"><a href="#cb94-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb94-58"><a href="#cb94-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb94-59"><a href="#cb94-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-60"><a href="#cb94-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-61"><a href="#cb94-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb94-62"><a href="#cb94-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb94-63"><a href="#cb94-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-64"><a href="#cb94-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-65"><a href="#cb94-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb94-66"><a href="#cb94-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb94-67"><a href="#cb94-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-68"><a href="#cb94-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-69"><a href="#cb94-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb94-70"><a href="#cb94-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb94-71"><a href="#cb94-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-72"><a href="#cb94-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-73"><a href="#cb94-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb94-74"><a href="#cb94-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb94-75"><a href="#cb94-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-76"><a href="#cb94-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-77"><a href="#cb94-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb94-78"><a href="#cb94-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb94-79"><a href="#cb94-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-80"><a href="#cb94-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-81"><a href="#cb94-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb94-82"><a href="#cb94-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb94-83"><a href="#cb94-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-84"><a href="#cb94-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-85"><a href="#cb94-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb94-86"><a href="#cb94-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb94-87"><a href="#cb94-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb94-88"><a href="#cb94-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb94-89"><a href="#cb94-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb94-90"><a href="#cb94-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb94-91"><a href="#cb94-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 16:04:21,821 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.
Epoch 9 loss is 0.30027549864319536
Epoch 19 loss is 0.2913620808041314
Epoch 29 loss is 0.29192267719386994
Epoch 39 loss is 0.29982285425986316
Epoch 49 loss is 0.2991453145584028
Epoch 59 loss is 0.2806252701793078
Epoch 69 loss is 0.29399948094396766
Epoch 79 loss is 0.2848964264304165
Epoch 89 loss is 0.2640974185080014
Epoch 99 loss is 0.2781391938018297
Epoch 109 loss is 0.29882929257076446
Epoch 119 loss is 0.2896457279584684
Epoch 129 loss is 0.30861868284987515
Epoch 139 loss is 0.30446866634588193
Epoch 149 loss is 0.3117817060575804
Epoch 159 loss is 0.2748762158827661
Epoch 169 loss is 0.2728797345000005
Epoch 179 loss is 0.2798542257381176
Epoch 189 loss is 0.282661688657906
Epoch 199 loss is 0.30204731380523975
Train Acc.:  0.7394193176234324
              precision    recall  f1-score   support

           0       0.92      0.84      0.88      1063
           1       0.75      0.74      0.75      1064
           2       0.93      0.75      0.83      1064
           3       0.89      0.88      0.88      1064
           4       0.84      0.62      0.72      1064
           5       0.79      0.83      0.81      1063
           6       0.62      0.66      0.64      1064
           7       0.99      1.00      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.81      0.67      0.73      1064
          10       0.85      0.83      0.84      1064

   micro avg       0.85      0.80      0.83     11702
   macro avg       0.85      0.80      0.82     11702
weighted avg       0.85      0.80      0.82     11702
 samples avg       0.77      0.80      0.78     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3553981755313085
Epoch 19 loss is 0.3506370108940286
Epoch 29 loss is 0.30082422517534047
Epoch 39 loss is 0.30063860033973894
Epoch 49 loss is 0.25131238597560024
Epoch 59 loss is 0.23531957531915973
Epoch 69 loss is 0.2380401779400625
Epoch 79 loss is 0.2335633736232218
Epoch 89 loss is 0.23700447810534878
Epoch 99 loss is 0.21088957250172105
Epoch 109 loss is 0.23098805169700698
Epoch 119 loss is 0.2506731403676874
Epoch 129 loss is 0.24334308375596303
Epoch 139 loss is 0.25569175507335257
Epoch 149 loss is 0.2508974958920864
Epoch 159 loss is 0.23111012958349472
Epoch 169 loss is 0.22934085708672713
Epoch 179 loss is 0.23431740304916213
Epoch 189 loss is 0.2142045345957332
Epoch 199 loss is 0.220009767135271
Train Acc.:  0.7865703847715085
              precision    recall  f1-score   support

           0       0.95      0.93      0.94      1063
           1       0.86      0.81      0.83      1064
           2       0.96      0.91      0.94      1064
           3       0.95      0.84      0.90      1063
           4       0.88      0.50      0.63      1064
           5       0.80      0.93      0.86      1064
           6       0.61      0.88      0.72      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.87      0.80      0.83      1064
          10       0.90      0.81      0.85      1064

   micro avg       0.88      0.85      0.87     11702
   macro avg       0.89      0.85      0.86     11702
weighted avg       0.89      0.85      0.86     11702
 samples avg       0.82      0.85      0.83     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.25758739764160093
Epoch 19 loss is 0.266251132975017
Epoch 29 loss is 0.2533447710924989
Epoch 39 loss is 0.24690942194464932
Epoch 49 loss is 0.231170000858992
Epoch 59 loss is 0.22355611411128506
Epoch 69 loss is 0.21875239712216873
Epoch 79 loss is 0.22903089863599008
Epoch 89 loss is 0.2373035179400735
Epoch 99 loss is 0.23382820577734512
Epoch 109 loss is 0.20544294051074055
Epoch 119 loss is 0.20787382558825876
Epoch 129 loss is 0.20653071272151727
Epoch 139 loss is 0.22629611145400377
Epoch 149 loss is 0.2440747135803093
Epoch 159 loss is 0.22601217778289143
Epoch 169 loss is 0.2584220808089135
Epoch 179 loss is 0.27634828629385144
Epoch 189 loss is 0.2721481887397433
Epoch 199 loss is 0.2826596067736297
Train Acc.:  0.8100070502275301
              precision    recall  f1-score   support

           0       0.85      0.95      0.90      1064
           1       0.86      0.81      0.84      1064
           2       0.90      0.90      0.90      1064
           3       0.96      0.94      0.95      1063
           4       0.82      0.49      0.62      1064
           5       0.85      0.84      0.85      1064
           6       0.86      0.58      0.69      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.80      0.94      0.86      1064
          10       0.90      0.77      0.83      1064

   micro avg       0.89      0.84      0.86     11702
   macro avg       0.89      0.84      0.86     11702
weighted avg       0.89      0.84      0.86     11702
 samples avg       0.82      0.84      0.83     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.28775937804643187
Epoch 19 loss is 0.23007369599769445
Epoch 29 loss is 0.25889260081970894
Epoch 39 loss is 0.25902661986905157
Epoch 49 loss is 0.2292498016389515
Epoch 59 loss is 0.22306006992961439
Epoch 69 loss is 0.1976271504421206
Epoch 79 loss is 0.20343043020192228
Epoch 89 loss is 0.21934100686580518
Epoch 99 loss is 0.22369418242070047
Epoch 109 loss is 0.21545978081993952
Epoch 119 loss is 0.21148563517815785
Epoch 129 loss is 0.21415086358216098
Epoch 139 loss is 0.23069838988169805
Epoch 149 loss is 0.2332487083323042
Epoch 159 loss is 0.2051832755068869
Epoch 169 loss is 0.22123131840723095
Epoch 179 loss is 0.20724936091666757
Epoch 189 loss is 0.1936030315379404
Epoch 199 loss is 0.2227826737601992
Train Acc.:  0.8283376418057128
              precision    recall  f1-score   support

           0       0.89      0.91      0.90      1063
           1       0.82      0.90      0.86      1064
           2       0.95      0.92      0.93      1064
           3       0.92      0.96      0.94      1063
           4       0.81      0.73      0.77      1064
           5       0.90      0.81      0.85      1064
           6       0.89      0.43      0.58      1064
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.78      0.96      0.86      1064
          10       0.82      0.88      0.85      1064

   micro avg       0.89      0.86      0.88     11702
   macro avg       0.89      0.86      0.87     11702
weighted avg       0.89      0.86      0.87     11702
 samples avg       0.85      0.86      0.85     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.16656616157130508
Epoch 19 loss is 0.1656779172497797
Epoch 29 loss is 0.22550141041854313
Epoch 39 loss is 0.20679155828053714
Epoch 49 loss is 0.21150408955747108
Epoch 59 loss is 0.23130154015187063
Epoch 69 loss is 0.2393321476323322
Epoch 79 loss is 0.22455397225477222
Epoch 89 loss is 0.25369244062104573
Epoch 99 loss is 0.2299914278176482
Epoch 109 loss is 0.19713104532598436
Epoch 119 loss is 0.196544944395429
Epoch 129 loss is 0.17342359035545055
Epoch 139 loss is 0.18163042589732023
Epoch 149 loss is 0.20009191348732322
Epoch 159 loss is 0.18232177900944924
Epoch 169 loss is 0.1977332400942652
Epoch 179 loss is 0.1948407359503548
Epoch 189 loss is 0.2035039176614566
Epoch 199 loss is 0.1989608963906182
Train Acc.:  0.8589099920951995
              precision    recall  f1-score   support

           0       0.92      0.96      0.94      1064
           1       0.88      0.77      0.82      1064
           2       0.89      0.95      0.92      1064
           3       0.98      0.96      0.97      1064
           4       0.85      0.80      0.82      1064
           5       0.77      0.93      0.84      1064
           6       0.90      0.78      0.84      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.87      0.89      0.88      1064
          10       0.93      0.83      0.88      1064

   micro avg       0.91      0.90      0.90     11702
   macro avg       0.91      0.90      0.90     11702
weighted avg       0.91      0.90      0.90     11702
 samples avg       0.88      0.90      0.88     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-50-50-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-50-50-11">MLMVN [48-50-50-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-50-50-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">50</span>)</span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">50</span>, <span class="dv">50</span>)</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">50</span>, <span class="dv">11</span>)</span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb107-30"><a href="#cb107-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb107-31"><a href="#cb107-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-32"><a href="#cb107-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb107-33"><a href="#cb107-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb107-34"><a href="#cb107-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-35"><a href="#cb107-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb107-36"><a href="#cb107-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb107-37"><a href="#cb107-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-38"><a href="#cb107-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb107-39"><a href="#cb107-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb107-40"><a href="#cb107-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-41"><a href="#cb107-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb107-42"><a href="#cb107-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb107-43"><a href="#cb107-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb107-44"><a href="#cb107-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-45"><a href="#cb107-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb107-46"><a href="#cb107-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb107-47"><a href="#cb107-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb107-48"><a href="#cb107-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-49"><a href="#cb107-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb107-50"><a href="#cb107-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb107-51"><a href="#cb107-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb107-52"><a href="#cb107-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-53"><a href="#cb107-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb107-54"><a href="#cb107-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb107-55"><a href="#cb107-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb107-56"><a href="#cb107-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-57"><a href="#cb107-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb107-58"><a href="#cb107-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb107-59"><a href="#cb107-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb107-60"><a href="#cb107-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb107-61"><a href="#cb107-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb107-62"><a href="#cb107-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-63"><a href="#cb107-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb107-64"><a href="#cb107-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb107-65"><a href="#cb107-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb107-66"><a href="#cb107-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-67"><a href="#cb107-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-68"><a href="#cb107-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb107-69"><a href="#cb107-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb107-70"><a href="#cb107-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb107-71"><a href="#cb107-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb107-72"><a href="#cb107-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb107-73"><a href="#cb107-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-74"><a href="#cb107-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb107-75"><a href="#cb107-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb107-76"><a href="#cb107-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb107-77"><a href="#cb107-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb107-78"><a href="#cb107-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-79"><a href="#cb107-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb107-80"><a href="#cb107-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-81"><a href="#cb107-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb107-82"><a href="#cb107-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb107-83"><a href="#cb107-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb107-84"><a href="#cb107-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb107-85"><a href="#cb107-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb107-86"><a href="#cb107-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-87"><a href="#cb107-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb107-88"><a href="#cb107-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb107-89"><a href="#cb107-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb107-90"><a href="#cb107-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-91"><a href="#cb107-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb107-92"><a href="#cb107-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb107-93"><a href="#cb107-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb107-94"><a href="#cb107-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-95"><a href="#cb107-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb107-96"><a href="#cb107-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb107-97"><a href="#cb107-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb107-98"><a href="#cb107-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb107-99"><a href="#cb107-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb107-100"><a href="#cb107-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-101"><a href="#cb107-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb107-102"><a href="#cb107-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb107-103"><a href="#cb107-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb107-104"><a href="#cb107-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-50-50-11]"</span>,</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-50-50-11]"</span>,</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=66b39d66d7654fa1ad879e1cd8e1a4ce
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/66b39d66d7654fa1ad879e1cd8e1a4ce/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-50-50-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-10"><a href="#cb111-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb111-11"><a href="#cb111-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb111-12"><a href="#cb111-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb111-13"><a href="#cb111-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb111-14"><a href="#cb111-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb111-15"><a href="#cb111-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-16"><a href="#cb111-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb111-17"><a href="#cb111-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb111-18"><a href="#cb111-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb111-19"><a href="#cb111-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-20"><a href="#cb111-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb111-21"><a href="#cb111-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb111-22"><a href="#cb111-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb111-23"><a href="#cb111-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb111-24"><a href="#cb111-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb111-25"><a href="#cb111-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb111-26"><a href="#cb111-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb111-27"><a href="#cb111-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb111-28"><a href="#cb111-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb111-29"><a href="#cb111-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb111-30"><a href="#cb111-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb111-31"><a href="#cb111-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-32"><a href="#cb111-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb111-33"><a href="#cb111-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-34"><a href="#cb111-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb111-35"><a href="#cb111-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb111-36"><a href="#cb111-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-37"><a href="#cb111-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb111-38"><a href="#cb111-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb111-39"><a href="#cb111-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb111-40"><a href="#cb111-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-41"><a href="#cb111-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb111-42"><a href="#cb111-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb111-43"><a href="#cb111-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb111-44"><a href="#cb111-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-45"><a href="#cb111-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb111-46"><a href="#cb111-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb111-47"><a href="#cb111-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb111-48"><a href="#cb111-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb111-49"><a href="#cb111-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb111-50"><a href="#cb111-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb111-51"><a href="#cb111-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb111-52"><a href="#cb111-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb111-53"><a href="#cb111-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb111-54"><a href="#cb111-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb111-55"><a href="#cb111-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb111-56"><a href="#cb111-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb111-57"><a href="#cb111-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb111-58"><a href="#cb111-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb111-59"><a href="#cb111-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-60"><a href="#cb111-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-61"><a href="#cb111-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb111-62"><a href="#cb111-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb111-63"><a href="#cb111-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-64"><a href="#cb111-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-65"><a href="#cb111-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb111-66"><a href="#cb111-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb111-67"><a href="#cb111-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-68"><a href="#cb111-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-69"><a href="#cb111-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb111-70"><a href="#cb111-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb111-71"><a href="#cb111-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-72"><a href="#cb111-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-73"><a href="#cb111-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb111-74"><a href="#cb111-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb111-75"><a href="#cb111-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-76"><a href="#cb111-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-77"><a href="#cb111-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb111-78"><a href="#cb111-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb111-79"><a href="#cb111-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-80"><a href="#cb111-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-81"><a href="#cb111-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb111-82"><a href="#cb111-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb111-83"><a href="#cb111-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-84"><a href="#cb111-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-85"><a href="#cb111-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb111-86"><a href="#cb111-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb111-87"><a href="#cb111-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb111-88"><a href="#cb111-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb111-89"><a href="#cb111-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb111-90"><a href="#cb111-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb111-91"><a href="#cb111-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 16:23:01,246 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.
Epoch 9 loss is 0.1499046038395931
Epoch 19 loss is 0.1297322766785629
Epoch 29 loss is 0.13019643237124603
Epoch 39 loss is 0.12789428760818733
Epoch 49 loss is 0.11103197831648269
Epoch 59 loss is 0.11020350228682631
Epoch 69 loss is 0.11147977013764811
Epoch 79 loss is 0.10846879683535147
Epoch 89 loss is 0.12197619828830582
Epoch 99 loss is 0.11589422750814764
Epoch 109 loss is 0.12233447725951128
Epoch 119 loss is 0.11728783170849341
Epoch 129 loss is 0.11600955988839287
Epoch 139 loss is 0.10777309333314188
Epoch 149 loss is 0.11286425886149831
Epoch 159 loss is 0.10997786349122186
Epoch 169 loss is 0.10343363161044343
Epoch 179 loss is 0.10918495130351621
Epoch 189 loss is 0.10805207773255567
Epoch 199 loss is 0.1042582369224553
Train Acc.:  0.9155681842459461
              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1063
           1       0.92      0.89      0.90      1064
           2       0.98      0.93      0.96      1064
           3       0.96      0.97      0.97      1064
           4       0.93      0.84      0.88      1064
           5       0.90      0.91      0.91      1063
           6       0.88      0.85      0.87      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.94      0.90      0.92      1064
          10       0.94      0.94      0.94      1064

   micro avg       0.95      0.93      0.94     11702
   macro avg       0.95      0.93      0.94     11702
weighted avg       0.95      0.93      0.94     11702
 samples avg       0.92      0.93      0.92     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.16444180091313235
Epoch 19 loss is 0.13450935301533853
Epoch 29 loss is 0.12395481547756365
Epoch 39 loss is 0.11403245044394655
Epoch 49 loss is 0.10585260064483076
Epoch 59 loss is 0.12085044614478288
Epoch 69 loss is 0.12072065235425643
Epoch 79 loss is 0.12322402368413449
Epoch 89 loss is 0.13057146826320154
Epoch 99 loss is 0.1284516260088808
Epoch 109 loss is 0.1310643245563845
Epoch 119 loss is 0.15000504618170252
Epoch 129 loss is 0.15256414701505086
Epoch 139 loss is 0.13475608804617148
Epoch 149 loss is 0.12786701654120483
Epoch 159 loss is 0.12325916810132656
Epoch 169 loss is 0.12089844310402888
Epoch 179 loss is 0.123651544885496
Epoch 189 loss is 0.12969231060336694
Epoch 199 loss is 0.15106089017416613
Train Acc.:  0.912256713739398
              precision    recall  f1-score   support

           0       0.96      0.95      0.95      1063
           1       0.91      0.86      0.88      1064
           2       0.99      0.96      0.97      1064
           3       0.97      0.95      0.96      1063
           4       0.92      0.89      0.90      1064
           5       0.89      0.92      0.91      1064
           6       0.86      0.86      0.86      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.90      0.90      0.90      1064
          10       0.97      0.94      0.96      1064

   micro avg       0.94      0.93      0.94     11702
   macro avg       0.94      0.93      0.94     11702
weighted avg       0.94      0.93      0.94     11702
 samples avg       0.92      0.93      0.92     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.13140804343389728
Epoch 19 loss is 0.11568335987799334
Epoch 29 loss is 0.10454780419932769
Epoch 39 loss is 0.10599905002509148
Epoch 49 loss is 0.10790101651025631
Epoch 59 loss is 0.10838345710268503
Epoch 69 loss is 0.1039850904931613
Epoch 79 loss is 0.10940135407457686
Epoch 89 loss is 0.09866982557423697
Epoch 99 loss is 0.10053318491512914
Epoch 109 loss is 0.09320339956264681
Epoch 119 loss is 0.10104087870858815
Epoch 129 loss is 0.10796066144525626
Epoch 139 loss is 0.10673830550703585
Epoch 149 loss is 0.10363054506764556
Epoch 159 loss is 0.10652278300728972
Epoch 169 loss is 0.11524796425634735
Epoch 179 loss is 0.1187950656937112
Epoch 189 loss is 0.11531818662457277
Epoch 199 loss is 0.10933889640581886
Train Acc.:  0.916145021043861
              precision    recall  f1-score   support

           0       0.96      0.94      0.95      1064
           1       0.91      0.91      0.91      1064
           2       1.00      0.98      0.99      1064
           3       0.97      0.95      0.96      1063
           4       0.87      0.84      0.86      1064
           5       0.92      0.91      0.91      1064
           6       0.86      0.88      0.87      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.88      0.93      0.90      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.94      0.93      0.94     11702
   macro avg       0.94      0.93      0.94     11702
weighted avg       0.94      0.93      0.94     11702
 samples avg       0.92      0.93      0.93     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.15382963120659784
Epoch 19 loss is 0.10859517485079796
Epoch 29 loss is 0.1039033885767897
Epoch 39 loss is 0.10722984283457825
Epoch 49 loss is 0.09191558609732126
Epoch 59 loss is 0.10290934605597563
Epoch 69 loss is 0.0952635279995302
Epoch 79 loss is 0.0910337109495511
Epoch 89 loss is 0.08792022893121838
Epoch 99 loss is 0.09991178895421955
Epoch 109 loss is 0.10163613127338143
Epoch 119 loss is 0.10929553506790932
Epoch 129 loss is 0.11517537118674706
Epoch 139 loss is 0.10527693350535264
Epoch 149 loss is 0.10425300772978957
Epoch 159 loss is 0.10921202865336493
Epoch 169 loss is 0.1063591963798746
Epoch 179 loss is 0.09805513885860391
Epoch 189 loss is 0.09630969560112046
Epoch 199 loss is 0.11513547873782298
Train Acc.:  0.9290277095306257
              precision    recall  f1-score   support

           0       0.97      0.98      0.97      1063
           1       0.91      0.93      0.92      1064
           2       0.99      0.98      0.99      1064
           3       0.98      0.98      0.98      1063
           4       0.92      0.92      0.92      1064
           5       0.94      0.90      0.92      1064
           6       0.88      0.82      0.85      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.91      0.95      0.93      1064
          10       0.98      0.93      0.95      1064

   micro avg       0.95      0.94      0.95     11702
   macro avg       0.95      0.94      0.95     11702
weighted avg       0.95      0.94      0.95     11702
 samples avg       0.93      0.94      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_10107/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.13468610442686887
Epoch 19 loss is 0.11356436751635222
Epoch 29 loss is 0.10511056101321942
Epoch 39 loss is 0.1061999656253616
Epoch 49 loss is 0.10663321393686137
Epoch 59 loss is 0.10928515764258159
Epoch 69 loss is 0.11259141153569381
Epoch 79 loss is 0.09192517041510463
Epoch 89 loss is 0.09801434421544775
Epoch 99 loss is 0.1031168064577551
Epoch 109 loss is 0.09404308404121978
Epoch 119 loss is 0.09329897373453444
Epoch 129 loss is 0.09098583010381506
Epoch 139 loss is 0.08366240511713254
Epoch 149 loss is 0.07989489898176606
Epoch 159 loss is 0.0764208793328456
Epoch 169 loss is 0.0826377881570616
Epoch 179 loss is 0.08229154744932386
Epoch 189 loss is 0.09376729251789975
Epoch 199 loss is 0.09184004778612281
Train Acc.:  0.9369752387463414
              precision    recall  f1-score   support

           0       0.96      0.97      0.97      1064
           1       0.96      0.89      0.92      1064
           2       0.99      0.98      0.98      1064
           3       0.99      0.98      0.99      1064
           4       0.95      0.88      0.92      1064
           5       0.89      0.96      0.92      1064
           6       0.92      0.88      0.90      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.93      0.95      0.94      1064
          10       0.96      0.96      0.96      1064

   micro avg       0.96      0.95      0.95     11702
   macro avg       0.96      0.95      0.95     11702
weighted avg       0.96      0.95      0.95     11702
 samples avg       0.94      0.95      0.94     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-100-100-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-100-100-11">MLMVN [48-100-100-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-100-100-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">100</span>)</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">100</span>, <span class="dv">11</span>)</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb124-15"><a href="#cb124-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb124-16"><a href="#cb124-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb124-17"><a href="#cb124-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb124-18"><a href="#cb124-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb124-19"><a href="#cb124-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb124-20"><a href="#cb124-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb124-21"><a href="#cb124-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb124-22"><a href="#cb124-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-23"><a href="#cb124-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb124-24"><a href="#cb124-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb124-25"><a href="#cb124-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb124-26"><a href="#cb124-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb124-27"><a href="#cb124-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb124-28"><a href="#cb124-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb124-29"><a href="#cb124-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb124-30"><a href="#cb124-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb124-31"><a href="#cb124-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-32"><a href="#cb124-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb124-33"><a href="#cb124-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb124-34"><a href="#cb124-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-35"><a href="#cb124-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb124-36"><a href="#cb124-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb124-37"><a href="#cb124-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-38"><a href="#cb124-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb124-39"><a href="#cb124-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb124-40"><a href="#cb124-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-41"><a href="#cb124-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb124-42"><a href="#cb124-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb124-43"><a href="#cb124-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb124-44"><a href="#cb124-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-45"><a href="#cb124-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb124-46"><a href="#cb124-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb124-47"><a href="#cb124-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb124-48"><a href="#cb124-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-49"><a href="#cb124-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb124-50"><a href="#cb124-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb124-51"><a href="#cb124-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb124-52"><a href="#cb124-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-53"><a href="#cb124-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb124-54"><a href="#cb124-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb124-55"><a href="#cb124-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb124-56"><a href="#cb124-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-57"><a href="#cb124-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb124-58"><a href="#cb124-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb124-59"><a href="#cb124-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb124-60"><a href="#cb124-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb124-61"><a href="#cb124-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb124-62"><a href="#cb124-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-63"><a href="#cb124-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb124-64"><a href="#cb124-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb124-65"><a href="#cb124-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb124-66"><a href="#cb124-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-67"><a href="#cb124-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-68"><a href="#cb124-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb124-69"><a href="#cb124-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb124-70"><a href="#cb124-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb124-71"><a href="#cb124-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb124-72"><a href="#cb124-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb124-73"><a href="#cb124-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-74"><a href="#cb124-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb124-75"><a href="#cb124-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb124-76"><a href="#cb124-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb124-77"><a href="#cb124-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb124-78"><a href="#cb124-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-79"><a href="#cb124-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb124-80"><a href="#cb124-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-81"><a href="#cb124-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb124-82"><a href="#cb124-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb124-83"><a href="#cb124-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb124-84"><a href="#cb124-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb124-85"><a href="#cb124-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb124-86"><a href="#cb124-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-87"><a href="#cb124-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb124-88"><a href="#cb124-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb124-89"><a href="#cb124-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb124-90"><a href="#cb124-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-91"><a href="#cb124-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb124-92"><a href="#cb124-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb124-93"><a href="#cb124-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb124-94"><a href="#cb124-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-95"><a href="#cb124-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb124-96"><a href="#cb124-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb124-97"><a href="#cb124-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb124-98"><a href="#cb124-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb124-99"><a href="#cb124-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb124-100"><a href="#cb124-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-101"><a href="#cb124-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb124-102"><a href="#cb124-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb124-103"><a href="#cb124-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb124-104"><a href="#cb124-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-100-100-11]"</span>,</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"clip_angle_value"</span>],</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb125-14"><a href="#cb125-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb125-15"><a href="#cb125-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb125-16"><a href="#cb125-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-100-100-11]"</span>,</span>
<span id="cb125-17"><a href="#cb125-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"clip_angle_value"</span>: clip_angle_value,</span>
<span id="cb125-18"><a href="#cb125-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb125-19"><a href="#cb125-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=6782b5dd458348fd9dde2a4046e3d09b
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6782b5dd458348fd9dde2a4046e3d09b/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-100-100-11]',
 'clip_angle_value': 1000000}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-16"><a href="#cb128-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb128-17"><a href="#cb128-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb128-18"><a href="#cb128-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr, clip_angle_value<span class="op">=</span>clip_angle_value)</span>
<span id="cb128-19"><a href="#cb128-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-20"><a href="#cb128-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb128-21"><a href="#cb128-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb128-22"><a href="#cb128-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb128-23"><a href="#cb128-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb128-24"><a href="#cb128-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb128-25"><a href="#cb128-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb128-26"><a href="#cb128-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb128-27"><a href="#cb128-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb128-28"><a href="#cb128-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb128-29"><a href="#cb128-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb128-30"><a href="#cb128-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb128-31"><a href="#cb128-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-32"><a href="#cb128-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb128-33"><a href="#cb128-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-34"><a href="#cb128-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb128-35"><a href="#cb128-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb128-36"><a href="#cb128-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-37"><a href="#cb128-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb128-38"><a href="#cb128-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb128-39"><a href="#cb128-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb128-40"><a href="#cb128-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-41"><a href="#cb128-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb128-42"><a href="#cb128-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb128-43"><a href="#cb128-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb128-44"><a href="#cb128-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-45"><a href="#cb128-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb128-46"><a href="#cb128-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb128-47"><a href="#cb128-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb128-48"><a href="#cb128-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb128-49"><a href="#cb128-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb128-50"><a href="#cb128-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb128-51"><a href="#cb128-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb128-52"><a href="#cb128-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb128-53"><a href="#cb128-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb128-54"><a href="#cb128-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb128-55"><a href="#cb128-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb128-56"><a href="#cb128-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb128-57"><a href="#cb128-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb128-58"><a href="#cb128-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb128-59"><a href="#cb128-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-60"><a href="#cb128-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-61"><a href="#cb128-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb128-62"><a href="#cb128-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb128-63"><a href="#cb128-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-64"><a href="#cb128-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-65"><a href="#cb128-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb128-66"><a href="#cb128-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb128-67"><a href="#cb128-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-68"><a href="#cb128-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-69"><a href="#cb128-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb128-70"><a href="#cb128-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb128-71"><a href="#cb128-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-72"><a href="#cb128-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-73"><a href="#cb128-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb128-74"><a href="#cb128-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb128-75"><a href="#cb128-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-76"><a href="#cb128-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-77"><a href="#cb128-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb128-78"><a href="#cb128-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb128-79"><a href="#cb128-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-80"><a href="#cb128-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-81"><a href="#cb128-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb128-82"><a href="#cb128-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb128-83"><a href="#cb128-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-84"><a href="#cb128-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-85"><a href="#cb128-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb128-86"><a href="#cb128-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb128-87"><a href="#cb128-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb128-88"><a href="#cb128-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb128-89"><a href="#cb128-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb128-90"><a href="#cb128-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb128-91"><a href="#cb128-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_15152/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 20:10:34,834 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.
Epoch 9 loss is 0.13682720639443957
Epoch 19 loss is 0.10102070636348458
Epoch 29 loss is 0.08727009396955031
Epoch 39 loss is 0.07671863965077798
Epoch 49 loss is 0.06848380096450109
Epoch 59 loss is 0.0635398093390423
Epoch 69 loss is 0.0628619211282882
Epoch 79 loss is 0.06982675314397956
Epoch 89 loss is 0.06102933808512002
Epoch 99 loss is 0.06000732133720239
Epoch 109 loss is 0.060890425387314366
Epoch 119 loss is 0.05301149732799192
Epoch 129 loss is 0.0522579315878903
Epoch 139 loss is 0.05275362570989821
Epoch 149 loss is 0.04787591243938674
Epoch 159 loss is 0.0456566457444692
Epoch 169 loss is 0.04486260011549171
Epoch 179 loss is 0.0447318709239749
Epoch 189 loss is 0.04379179251941935
Epoch 199 loss is 0.04489066304454168
Train Acc.:  0.9654752494285043
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      1063
           1       0.94      0.94      0.94      1064
           2       0.99      0.97      0.98      1064
           3       1.00      0.99      0.99      1064
           4       0.96      0.93      0.94      1064
           5       0.95      0.94      0.95      1063
           6       0.95      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.95      0.94      0.95      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.97      0.96      0.97     11702
   macro avg       0.97      0.96      0.97     11702
weighted avg       0.97      0.96      0.97     11702
 samples avg       0.96      0.96      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_15152/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.14070723384058115
Epoch 19 loss is 0.09901262946414933
Epoch 29 loss is 0.1163871561542634
Epoch 39 loss is 0.0928615501126676
Epoch 49 loss is 0.09462188473736881
Epoch 59 loss is 0.09051752364025904
Epoch 69 loss is 0.08058040002663806
Epoch 79 loss is 0.07118650187737878
Epoch 89 loss is 0.05725321689636519
Epoch 99 loss is 0.05587193962924077
Epoch 109 loss is 0.053161680819638193
Epoch 119 loss is 0.052564399732753064
Epoch 129 loss is 0.04833477461407704
Epoch 139 loss is 0.046382782052641146
Epoch 149 loss is 0.05173067099999342
Epoch 159 loss is 0.04534002150442024
Epoch 169 loss is 0.046009179389923965
Epoch 179 loss is 0.04352159747685905
Epoch 189 loss is 0.041239578438454445
Epoch 199 loss is 0.040097920362887236
Train Acc.:  0.9692994637554212
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      1063
           1       0.94      0.93      0.94      1064
           2       1.00      0.99      0.99      1064
           3       0.99      0.98      0.99      1063
           4       0.96      0.95      0.95      1064
           5       0.96      0.94      0.95      1064
           6       0.96      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.98      0.97      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_15152/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.1371787810722252
Epoch 19 loss is 0.09978662945393381
Epoch 29 loss is 0.08858285073706741
Epoch 39 loss is 0.0944868673915956
Epoch 49 loss is 0.0822929725506483
Epoch 59 loss is 0.06799231767158967
Epoch 69 loss is 0.06335458838516671
Epoch 79 loss is 0.06147011599416563
Epoch 89 loss is 0.06221068190118905
Epoch 99 loss is 0.05993916873581913
Epoch 109 loss is 0.054406843221859624
Epoch 119 loss is 0.045617025331166426
Epoch 129 loss is 0.046012246996210246
Epoch 139 loss is 0.044311349952736896
Epoch 149 loss is 0.04082695991627327
Epoch 159 loss is 0.039679863425516555
Epoch 169 loss is 0.040999900860178046
Epoch 179 loss is 0.04099159740351322
Epoch 189 loss is 0.03939206668245912
Epoch 199 loss is 0.039607182730482776
Train Acc.:  0.9713718033627449
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1064
           1       0.96      0.94      0.95      1064
           2       0.99      0.98      0.99      1064
           3       1.00      0.99      0.99      1063
           4       0.97      0.95      0.96      1064
           5       0.96      0.95      0.95      1064
           6       0.96      0.92      0.94      1063
           7       1.00      1.00      1.00      1064
           8       1.00      0.99      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_15152/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.13303833512235225
Epoch 19 loss is 0.11102950469889049
Epoch 29 loss is 0.1033785956422809
Epoch 39 loss is 0.10035753702045348
Epoch 49 loss is 0.08026176993296501
Epoch 59 loss is 0.08006638138825455
Epoch 69 loss is 0.07315596454862489
Epoch 79 loss is 0.06695363560418462
Epoch 89 loss is 0.07812911516053243
Epoch 99 loss is 0.07286186109129712
Epoch 109 loss is 0.06823154445600114
Epoch 119 loss is 0.0748763280247656
Epoch 129 loss is 0.06759634080194282
Epoch 139 loss is 0.06537460445723998
Epoch 149 loss is 0.07214919456239974
Epoch 159 loss is 0.06770210388031259
Epoch 169 loss is 0.05968848252020946
Epoch 179 loss is 0.0640390788375198
Epoch 189 loss is 0.06062085278669487
Epoch 199 loss is 0.053366963455588425
Train Acc.:  0.9539812421219048
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      1063
           1       0.93      0.89      0.91      1064
           2       0.99      0.98      0.98      1064
           3       0.99      0.99      0.99      1063
           4       0.95      0.93      0.94      1064
           5       0.94      0.93      0.94      1064
           6       0.93      0.93      0.93      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.96      0.96      0.96      1064
          10       0.99      0.96      0.98      1064

   micro avg       0.97      0.96      0.96     11702
   macro avg       0.97      0.96      0.96     11702
weighted avg       0.97      0.96      0.96     11702
 samples avg       0.95      0.96      0.95     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_15152/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.13915089234034914
Epoch 19 loss is 0.097373618826446
Epoch 29 loss is 0.08775521526507787
Epoch 39 loss is 0.07581347006628077
Epoch 49 loss is 0.06707371275141155
Epoch 59 loss is 0.07044893252214587
Epoch 69 loss is 0.06647936146342898
Epoch 79 loss is 0.06290139006666541
Epoch 89 loss is 0.07122418431326744
Epoch 99 loss is 0.06815365861695168
Epoch 109 loss is 0.07112648692035674
Epoch 119 loss is 0.06887879623935313
Epoch 129 loss is 0.065383156901932
Epoch 139 loss is 0.06278148365408386
Epoch 149 loss is 0.05971489880293905
Epoch 159 loss is 0.05826395838084222
Epoch 169 loss is 0.059517928423252736
Epoch 179 loss is 0.05508918404760044
Epoch 189 loss is 0.05508463686628998
Epoch 199 loss is 0.056739337282013
Train Acc.:  0.9532121263913517
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1064
           1       0.93      0.94      0.93      1064
           2       0.99      0.98      0.98      1064
           3       0.99      0.96      0.98      1064
           4       0.96      0.94      0.95      1064
           5       0.93      0.92      0.93      1064
           6       0.94      0.92      0.93      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.94      0.93      0.94      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.97      0.96      0.96     11702
   macro avg       0.97      0.96      0.96     11702
weighted avg       0.97      0.96      0.96     11702
 samples avg       0.95      0.96      0.95     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>