<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="In this example, the main focus is the classification of individual states of a motor.">

<title>mlmvn - Sensorless Drive Diagnosis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="mlmvn - Sensorless Drive Diagnosis">
<meta property="og:description" content="In this example, the main focus is the classification of individual states of a motor.">
<meta property="og:site-name" content="mlmvn">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">mlmvn</span>
    </a>
  </div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Sensorless Drive Diagnosis</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">MLMVN</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../layers.html" class="sidebar-item-text sidebar-link">Layers</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../loss.html" class="sidebar-item-text sidebar-link">Loss</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../optim.html" class="sidebar-item-text sidebar-link">Optimizer</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Examples</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../examples/xor/xor.html" class="sidebar-item-text sidebar-link">XOR</a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Moons</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../examples/moons/moons.html" class="sidebar-item-text sidebar-link">Building a Binary Classifier</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">SDD</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  examples/autass/00_autass.ipynb
  </li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#load-data" id="toc-load-data" class="nav-link active" data-scroll-target="#load-data">Load Data</a></li>
  <li><a href="#config" id="toc-config" class="nav-link" data-scroll-target="#config">Config</a></li>
  <li><a href="#single-layer" id="toc-single-layer" class="nav-link" data-scroll-target="#single-layer">Single Layer</a>
  <ul>
  <li><a href="#mlmvn-48-10-11" id="toc-mlmvn-48-10-11" class="nav-link" data-scroll-target="#mlmvn-48-10-11">MLMVN [48-10-11]</a></li>
  <li><a href="#mlmvn-48-20-11" id="toc-mlmvn-48-20-11" class="nav-link" data-scroll-target="#mlmvn-48-20-11">MLMVN [48-20-11]</a></li>
  <li><a href="#mlmvn-48-50-11" id="toc-mlmvn-48-50-11" class="nav-link" data-scroll-target="#mlmvn-48-50-11">MLMVN [48-50-11]</a></li>
  <li><a href="#mlmvn-48-100-11" id="toc-mlmvn-48-100-11" class="nav-link" data-scroll-target="#mlmvn-48-100-11">MLMVN [48-100-11]</a></li>
  </ul></li>
  <li><a href="#multi-layer" id="toc-multi-layer" class="nav-link" data-scroll-target="#multi-layer">Multi Layer</a>
  <ul>
  <li><a href="#mlmvn-48-10-10-11" id="toc-mlmvn-48-10-10-11" class="nav-link" data-scroll-target="#mlmvn-48-10-10-11">MLMVN [48-10-10-11]</a></li>
  <li><a href="#mlmvn-48-20-20-11" id="toc-mlmvn-48-20-20-11" class="nav-link" data-scroll-target="#mlmvn-48-20-20-11">MLMVN [48-20-20-11]</a></li>
  <li><a href="#mlmvn-48-50-50-11" id="toc-mlmvn-48-50-50-11" class="nav-link" data-scroll-target="#mlmvn-48-50-50-11">MLMVN [48-50-50-11]</a></li>
  <li><a href="#mlmvn-48-100-100-11" id="toc-mlmvn-48-100-100-11" class="nav-link" data-scroll-target="#mlmvn-48-100-100-11">MLMVN [48-100-100-11]</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/antonpf/mlmvn/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Sensorless Drive Diagnosis</h1>
</div>

<div>
  <div class="description">
    In this example, the main focus is the classification of individual states of a motor.
  </div>
</div>


<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="load-data" class="level2">
<h2 class="anchored" data-anchor-id="load-data">Load Data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>train_csv <span class="op">=</span> pd.read_csv(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"data/autass_data2.csv"</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    header<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span>np.double,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(train_csv.values[:, <span class="dv">1</span>:<span class="dv">50</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> train_csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[:, <span class="dv">0</span>:<span class="dv">48</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[:, <span class="dv">48</span>].astype(<span class="bu">int</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>yt <span class="op">=</span> copy.copy(y)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> <span class="dv">21</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">2</span>] <span class="op">=</span> <span class="dv">22</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">3</span>] <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">4</span>] <span class="op">=</span> <span class="dv">26</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">5</span>] <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">6</span>] <span class="op">=</span> <span class="dv">27</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">7</span>] <span class="op">=</span> <span class="dv">29</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">8</span>] <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">9</span>] <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">10</span>] <span class="op">=</span> <span class="dv">28</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>yt <span class="op">-=</span> <span class="dv">20</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> yt</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> yt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="config" class="level2">
<h2 class="anchored" data-anchor-id="config">Config</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">538</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>clip_angle_value <span class="op">=</span> <span class="dv">1000000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="single-layer" class="level2">
<h2 class="anchored" data-anchor-id="single-layer">Single Layer</h2>
<section id="mlmvn-48-10-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-10-11">MLMVN [48-10-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-10-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">10</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-10-11]"</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-10-11]"</span>,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=e06e21b763e247b78c42f9fc54073711
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e06e21b763e247b78c42f9fc54073711/output/log</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Inconsistency detected by ld.so: dl-open.c: 632: _dl_open: Assertion `_dl_debug_initialize (0, args.nsid)-&gt;r_state == RT_CONSISTENT' failed!</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-10-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 21:03:49,366 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.
Epoch 9 loss is 0.24486756715526192
Epoch 19 loss is 0.16472245788019146
Epoch 29 loss is 0.16197286684855702
Epoch 39 loss is 0.15777335760350167
Epoch 49 loss is 0.16362277181734114
Epoch 59 loss is 0.1599623075779962
Epoch 69 loss is 0.15505627729450555
Epoch 79 loss is 0.1516815979779308
Epoch 89 loss is 0.14756507300426192
Epoch 99 loss is 0.14990614337584326
Epoch 109 loss is 0.14464806634677907
Epoch 119 loss is 0.14414441882839768
Epoch 129 loss is 0.133414154667594
Epoch 139 loss is 0.1568121289545272
Epoch 149 loss is 0.13148384624669937
Epoch 159 loss is 0.15520714324892884
Epoch 169 loss is 0.15488150667967812
Epoch 179 loss is 0.16196303491616978
Epoch 189 loss is 0.17048013425110728
Epoch 199 loss is 0.253481823204663
Train Acc.:  0.9061037878949729
              precision    recall  f1-score   support

           0       0.95      0.96      0.95      1063
           1       0.88      0.91      0.90      1064
           2       0.97      0.95      0.96      1064
           3       0.92      0.93      0.93      1064
           4       0.90      0.87      0.88      1064
           5       0.92      0.89      0.90      1063
           6       0.88      0.85      0.86      1064
           7       0.99      0.97      0.98      1064
           8       1.00      0.99      1.00      1064
           9       0.92      0.90      0.91      1064
          10       0.95      0.97      0.96      1064

   micro avg       0.93      0.93      0.93     11702
   macro avg       0.93      0.93      0.93     11702
weighted avg       0.93      0.93      0.93     11702
 samples avg       0.91      0.93      0.92     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.23206087884330276
Epoch 19 loss is 0.223541648173787
Epoch 29 loss is 0.19793911944454762
Epoch 39 loss is 0.20525953919872755
Epoch 49 loss is 0.19172214725700548
Epoch 59 loss is 0.180403134615211
Epoch 69 loss is 0.20454411746859222
Epoch 79 loss is 0.22207294107387418
Epoch 89 loss is 0.1823581614612617
Epoch 99 loss is 0.1984208152000532
Epoch 109 loss is 0.21770198580729927
Epoch 119 loss is 0.26321624687915784
Epoch 129 loss is 0.2366645070320064
Epoch 139 loss is 0.26308797516611215
Epoch 149 loss is 0.2796785803565688
Epoch 159 loss is 0.19642007623052968
Epoch 169 loss is 0.3310147275301174
Epoch 179 loss is 0.2874490546335206
Epoch 189 loss is 0.2595535203540301
Epoch 199 loss is 0.3217917178123802
Train Acc.:  0.8804452325506869
              precision    recall  f1-score   support

           0       0.97      0.91      0.94      1063
           1       0.89      0.87      0.88      1064
           2       0.98      0.96      0.97      1064
           3       0.98      0.95      0.97      1063
           4       0.84      0.71      0.77      1064
           5       0.92      0.86      0.89      1064
           6       0.84      0.83      0.83      1064
           7       0.99      0.99      0.99      1064
           8       1.00      0.99      1.00      1064
           9       0.87      0.90      0.88      1064
          10       0.92      0.93      0.93      1064

   micro avg       0.93      0.90      0.91     11702
   macro avg       0.93      0.90      0.91     11702
weighted avg       0.93      0.90      0.91     11702
 samples avg       0.89      0.90      0.89     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.23712313444477043
Epoch 19 loss is 0.21309815317807554
Epoch 29 loss is 0.2151226041409042
Epoch 39 loss is 0.22794682641424854
Epoch 49 loss is 0.18590490342052546
Epoch 59 loss is 0.17194472537314034
Epoch 69 loss is 0.18656052422193878
Epoch 79 loss is 0.1756717054349332
Epoch 89 loss is 0.19215676606052032
Epoch 99 loss is 0.19421889588535315
Epoch 109 loss is 0.23849424654511947
Epoch 119 loss is 0.21172349809503013
Epoch 129 loss is 0.310951138936457
Epoch 139 loss is 0.40137440459088775
Epoch 149 loss is 0.3257708635272683
Epoch 159 loss is 0.3058673858224232
Epoch 169 loss is 0.3393692446068119
Epoch 179 loss is 0.3696819123265627
Epoch 189 loss is 0.3599098499384412
Epoch 199 loss is 0.4304120007418079
Train Acc.:  0.8721558741213921
              precision    recall  f1-score   support

           0       0.96      0.94      0.95      1064
           1       0.84      0.81      0.82      1064
           2       0.96      0.91      0.93      1064
           3       0.96      0.92      0.94      1063
           4       0.89      0.84      0.86      1064
           5       0.87      0.84      0.86      1064
           6       0.88      0.76      0.81      1063
           7       1.00      0.99      0.99      1064
           8       1.00      0.99      1.00      1064
           9       0.86      0.86      0.86      1064
          10       0.92      0.91      0.91      1064

   micro avg       0.92      0.89      0.90     11702
   macro avg       0.92      0.89      0.90     11702
weighted avg       0.92      0.89      0.90     11702
 samples avg       0.88      0.89      0.88     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.22525142483966729
Epoch 19 loss is 0.21769357020501426
Epoch 29 loss is 0.2577133298481112
Epoch 39 loss is 0.22213481954372305
Epoch 49 loss is 0.23841117493258804
Epoch 59 loss is 0.309014874649911
Epoch 69 loss is 0.35371038376681546
Epoch 79 loss is 0.3699092508438563
Epoch 89 loss is 0.36114378942190123
Epoch 99 loss is 0.3760639842052328
Epoch 109 loss is 0.289880557557834
Epoch 119 loss is 0.3542478901455959
Epoch 129 loss is 0.2415444508855365
Epoch 139 loss is 0.25469037973175684
Epoch 149 loss is 0.23138965174619833
Epoch 159 loss is 0.21245593786661174
Epoch 169 loss is 0.28983951582336215
Epoch 179 loss is 0.2624560468178941
Epoch 189 loss is 0.22517652315304382
Epoch 199 loss is 0.30955816883738974
Train Acc.:  0.830303159783793
              precision    recall  f1-score   support

           0       0.95      0.94      0.95      1063
           1       0.89      0.81      0.85      1064
           2       0.95      0.85      0.90      1064
           3       0.96      0.94      0.95      1063
           4       0.89      0.81      0.85      1064
           5       0.88      0.81      0.85      1064
           6       0.77      0.70      0.73      1064
           7       1.00      0.99      1.00      1064
           8       0.99      0.99      0.99      1064
           9       0.86      0.89      0.87      1064
          10       0.91      0.84      0.87      1064

   micro avg       0.92      0.87      0.89     11702
   macro avg       0.91      0.87      0.89     11702
weighted avg       0.91      0.87      0.89     11702
 samples avg       0.85      0.87      0.86     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2563851438763014
Epoch 19 loss is 0.21581594458135714
Epoch 29 loss is 0.2354525347648366
Epoch 39 loss is 0.19813069646116785
Epoch 49 loss is 0.23640784807386372
Epoch 59 loss is 0.2855586216205039
Epoch 69 loss is 0.31419042978716966
Epoch 79 loss is 0.3396274724821675
Epoch 89 loss is 0.3805541273460192
Epoch 99 loss is 0.2594859077291225
Epoch 109 loss is 0.27091326668934435
Epoch 119 loss is 0.26640856164548926
Epoch 129 loss is 0.2599984885766787
Epoch 139 loss is 0.24273455139343672
Epoch 149 loss is 0.3339575152030492
Epoch 159 loss is 0.2597825247475023
Epoch 169 loss is 0.2548200790331675
Epoch 179 loss is 0.2912208067110613
Epoch 189 loss is 0.29534861061989254
Epoch 199 loss is 0.35203137025199627
Train Acc.:  0.8437840493943214
              precision    recall  f1-score   support

           0       0.94      0.93      0.94      1064
           1       0.88      0.80      0.84      1064
           2       0.92      0.85      0.88      1064
           3       0.93      0.81      0.87      1064
           4       0.89      0.81      0.85      1064
           5       0.85      0.81      0.83      1064
           6       0.84      0.81      0.83      1063
           7       0.99      0.99      0.99      1064
           8       1.00      1.00      1.00      1063
           9       0.86      0.86      0.86      1064
          10       0.94      0.85      0.89      1064

   micro avg       0.91      0.86      0.89     11702
   macro avg       0.91      0.86      0.89     11702
weighted avg       0.91      0.86      0.89     11702
 samples avg       0.85      0.86      0.86     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-20-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-20-11">MLMVN [48-20-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-20-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">20</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">20</span>, <span class="dv">11</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-20-11]"</span>,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-20-11]"</span>,</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=a07a5ceae0cf4b42a7a0a6bcbcf20e9b
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/a07a5ceae0cf4b42a7a0a6bcbcf20e9b/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-20-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 21:16:31,580 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.
Epoch 9 loss is 0.2058236759317573
Epoch 19 loss is 0.1825360529148008
Epoch 29 loss is 0.13331677743733528
Epoch 39 loss is 0.11695840664945023
Epoch 49 loss is 0.11103621118112407
Epoch 59 loss is 0.12555862590235578
Epoch 69 loss is 0.12181158857175331
Epoch 79 loss is 0.09544202089592545
Epoch 89 loss is 0.11178487401744581
Epoch 99 loss is 0.12958967446816022
Epoch 109 loss is 0.11905161904742857
Epoch 119 loss is 0.10873579923307998
Epoch 129 loss is 0.09673697400248864
Epoch 139 loss is 0.09588223402448456
Epoch 149 loss is 0.11162506774666282
Epoch 159 loss is 0.09059592728792225
Epoch 169 loss is 0.09758258176950994
Epoch 179 loss is 0.10536165971500439
Epoch 189 loss is 0.10205764671734051
Epoch 199 loss is 0.14389611907866479
Train Acc.:  0.9354583716110838
              precision    recall  f1-score   support

           0       0.96      0.96      0.96      1063
           1       0.93      0.93      0.93      1064
           2       0.98      0.95      0.96      1064
           3       0.96      0.95      0.96      1064
           4       0.94      0.93      0.94      1064
           5       0.94      0.94      0.94      1063
           6       0.88      0.88      0.88      1064
           7       1.00      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.94      0.92      0.93      1064
          10       0.97      0.95      0.96      1064

   micro avg       0.95      0.94      0.95     11702
   macro avg       0.95      0.94      0.95     11702
weighted avg       0.95      0.94      0.95     11702
 samples avg       0.94      0.94      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2307281778017605
Epoch 19 loss is 0.18615664526599873
Epoch 29 loss is 0.16207854951403944
Epoch 39 loss is 0.15162521963532127
Epoch 49 loss is 0.16081494726357065
Epoch 59 loss is 0.1257449738051931
Epoch 69 loss is 0.10937442608829037
Epoch 79 loss is 0.12329823825151687
Epoch 89 loss is 0.12930987857639764
Epoch 99 loss is 0.12765360621548777
Epoch 109 loss is 0.15596338287786382
Epoch 119 loss is 0.16634696725086548
Epoch 129 loss is 0.1495299076006498
Epoch 139 loss is 0.1502898510399668
Epoch 149 loss is 0.15832158268949942
Epoch 159 loss is 0.17899469216655411
Epoch 169 loss is 0.23058719525799198
Epoch 179 loss is 0.2102188014453179
Epoch 189 loss is 0.23088950030054886
Epoch 199 loss is 0.21150270029799098
Train Acc.:  0.9071933685132566
              precision    recall  f1-score   support

           0       0.97      0.95      0.96      1063
           1       0.91      0.89      0.90      1064
           2       0.98      0.97      0.97      1064
           3       0.94      0.94      0.94      1063
           4       0.91      0.92      0.92      1064
           5       0.93      0.93      0.93      1064
           6       0.90      0.89      0.90      1064
           7       0.99      0.99      0.99      1064
           8       1.00      0.99      1.00      1064
           9       0.92      0.88      0.90      1064
          10       0.92      0.89      0.90      1064

   micro avg       0.94      0.93      0.94     11702
   macro avg       0.94      0.93      0.94     11702
weighted avg       0.94      0.93      0.94     11702
 samples avg       0.92      0.93      0.92     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.21257633823646344
Epoch 19 loss is 0.17318814963579834
Epoch 29 loss is 0.1458617958714956
Epoch 39 loss is 0.14125668374187184
Epoch 49 loss is 0.128772752498416
Epoch 59 loss is 0.1352171863065471
Epoch 69 loss is 0.10681416437897072
Epoch 79 loss is 0.13216534508015265
Epoch 89 loss is 0.11111060725019
Epoch 99 loss is 0.11440525031960108
Epoch 109 loss is 0.1094888755315214
Epoch 119 loss is 0.11533819118199232
Epoch 129 loss is 0.10736030126824322
Epoch 139 loss is 0.10061210309481297
Epoch 149 loss is 0.10068345790438905
Epoch 159 loss is 0.09078661822690025
Epoch 169 loss is 0.09375741469930386
Epoch 179 loss is 0.09420665138442623
Epoch 189 loss is 0.09327934878515685
Epoch 199 loss is 0.09550257254442705
Train Acc.:  0.9378939047578354
              precision    recall  f1-score   support

           0       0.97      0.94      0.96      1064
           1       0.94      0.91      0.92      1064
           2       0.97      0.96      0.97      1064
           3       0.99      0.97      0.98      1063
           4       0.94      0.92      0.93      1064
           5       0.94      0.93      0.93      1064
           6       0.91      0.86      0.88      1063
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.92      0.91      0.92      1064
          10       0.96      0.95      0.96      1064

   micro avg       0.96      0.94      0.95     11702
   macro avg       0.96      0.94      0.95     11702
weighted avg       0.96      0.94      0.95     11702
 samples avg       0.93      0.94      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.20641349230841313
Epoch 19 loss is 0.15738830801178577
Epoch 29 loss is 0.1799417678605428
Epoch 39 loss is 0.13007205544481562
Epoch 49 loss is 0.11737783309073208
Epoch 59 loss is 0.1114466828734566
Epoch 69 loss is 0.11852908050617135
Epoch 79 loss is 0.11542474814310127
Epoch 89 loss is 0.10236658536166438
Epoch 99 loss is 0.12045357416608558
Epoch 109 loss is 0.1051365830820148
Epoch 119 loss is 0.09761249375093389
Epoch 129 loss is 0.09216019224495184
Epoch 139 loss is 0.09421224180251014
Epoch 149 loss is 0.09233837313904565
Epoch 159 loss is 0.09223922889858933
Epoch 169 loss is 0.08829034240388713
Epoch 179 loss is 0.09375694157319493
Epoch 189 loss is 0.08536116014844801
Epoch 199 loss is 0.08500084532665717
Train Acc.:  0.94060717414062
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1063
           1       0.93      0.93      0.93      1064
           2       0.98      0.96      0.97      1064
           3       0.97      0.95      0.96      1063
           4       0.96      0.92      0.94      1064
           5       0.95      0.92      0.93      1064
           6       0.90      0.90      0.90      1064
           7       0.99      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.95      0.92      0.93      1064
          10       0.97      0.95      0.96      1064

   micro avg       0.96      0.95      0.95     11702
   macro avg       0.96      0.95      0.95     11702
weighted avg       0.96      0.95      0.95     11702
 samples avg       0.94      0.95      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.20548555472102373
Epoch 19 loss is 0.15323632791255185
Epoch 29 loss is 0.1295082539268068
Epoch 39 loss is 0.12081646040093838
Epoch 49 loss is 0.10243967732402894
Epoch 59 loss is 0.10126855908153241
Epoch 69 loss is 0.11296332021354512
Epoch 79 loss is 0.09185533808063474
Epoch 89 loss is 0.09732833351838127
Epoch 99 loss is 0.09188653614455912
Epoch 109 loss is 0.10085898797344649
Epoch 119 loss is 0.10443676031698373
Epoch 129 loss is 0.1023311360663922
Epoch 139 loss is 0.10306230769293823
Epoch 149 loss is 0.11354442989486692
Epoch 159 loss is 0.10403630260181586
Epoch 169 loss is 0.09833004415491228
Epoch 179 loss is 0.14203489341950598
Epoch 189 loss is 0.10320350983155654
Epoch 199 loss is 0.11371829480246762
Train Acc.:  0.9411626466126861
              precision    recall  f1-score   support

           0       0.96      0.97      0.96      1064
           1       0.93      0.92      0.93      1064
           2       0.98      0.94      0.96      1064
           3       0.98      0.96      0.97      1064
           4       0.95      0.91      0.93      1064
           5       0.94      0.93      0.93      1064
           6       0.90      0.90      0.90      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.94      0.91      0.93      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.96      0.94      0.95     11702
   macro avg       0.96      0.94      0.95     11702
weighted avg       0.96      0.94      0.95     11702
 samples avg       0.94      0.94      0.94     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-50-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-50-11">MLMVN [48-50-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-50-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">50</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">50</span>, <span class="dv">11</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb40-66"><a href="#cb40-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-67"><a href="#cb40-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb40-68"><a href="#cb40-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb40-69"><a href="#cb40-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb40-70"><a href="#cb40-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb40-71"><a href="#cb40-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-72"><a href="#cb40-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb40-73"><a href="#cb40-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-74"><a href="#cb40-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb40-75"><a href="#cb40-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb40-76"><a href="#cb40-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb40-77"><a href="#cb40-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb40-78"><a href="#cb40-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb40-79"><a href="#cb40-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-80"><a href="#cb40-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb40-81"><a href="#cb40-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb40-82"><a href="#cb40-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb40-83"><a href="#cb40-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-84"><a href="#cb40-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb40-85"><a href="#cb40-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb40-86"><a href="#cb40-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb40-87"><a href="#cb40-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-88"><a href="#cb40-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb40-89"><a href="#cb40-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb40-90"><a href="#cb40-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-91"><a href="#cb40-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb40-92"><a href="#cb40-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb40-93"><a href="#cb40-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-94"><a href="#cb40-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb40-95"><a href="#cb40-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb40-96"><a href="#cb40-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb40-97"><a href="#cb40-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-50-11]"</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-50-11]"</span>,</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=1dd535f82fcf4ad28379c9d07b765068
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1dd535f82fcf4ad28379c9d07b765068/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-50-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-85"><a href="#cb44-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb44-86"><a href="#cb44-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb44-87"><a href="#cb44-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-88"><a href="#cb44-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb44-89"><a href="#cb44-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb44-90"><a href="#cb44-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb44-91"><a href="#cb44-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 21:30:40,926 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.
Epoch 9 loss is 0.24396016163019188
Epoch 19 loss is 0.1408100184642106
Epoch 29 loss is 0.10649778199752649
Epoch 39 loss is 0.0854672719392131
Epoch 49 loss is 0.09677528437761192
Epoch 59 loss is 0.08491636262762711
Epoch 69 loss is 0.07073853121933514
Epoch 79 loss is 0.0623588742935536
Epoch 89 loss is 0.06493405545145113
Epoch 99 loss is 0.06570635803032455
Epoch 109 loss is 0.05703714881850874
Epoch 119 loss is 0.06868247092465077
Epoch 129 loss is 0.05820049851283152
Epoch 139 loss is 0.057654175689656244
Epoch 149 loss is 0.054585099955762065
Epoch 159 loss is 0.07306441308956363
Epoch 169 loss is 0.06311269411213584
Epoch 179 loss is 0.0785263104691008
Epoch 189 loss is 0.08227202587346817
Epoch 199 loss is 0.055809321412454364
Train Acc.:  0.9670775738671566
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1063
           1       0.96      0.94      0.95      1064
           2       0.99      0.99      0.99      1064
           3       0.98      0.98      0.98      1064
           4       0.96      0.92      0.94      1064
           5       0.97      0.95      0.96      1063
           6       0.95      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2661679163999582
Epoch 19 loss is 0.14625262471743808
Epoch 29 loss is 0.12002121900326053
Epoch 39 loss is 0.11372154790877459
Epoch 49 loss is 0.09963077571846711
Epoch 59 loss is 0.08699985949055491
Epoch 69 loss is 0.08908419619519271
Epoch 79 loss is 0.077337234526038
Epoch 89 loss is 0.0786930436278074
Epoch 99 loss is 0.06691134614071381
Epoch 109 loss is 0.07431928775441791
Epoch 119 loss is 0.059303313881029024
Epoch 129 loss is 0.05999768811650241
Epoch 139 loss is 0.05597988277724091
Epoch 149 loss is 0.05467993967785231
Epoch 159 loss is 0.054380003065822906
Epoch 169 loss is 0.055407265453249616
Epoch 179 loss is 0.05802883371582689
Epoch 189 loss is 0.05513524843130078
Epoch 199 loss is 0.04906545557802202
Train Acc.:  0.9664793727433931
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1063
           1       0.96      0.94      0.95      1064
           2       0.99      0.99      0.99      1064
           3       0.99      0.98      0.98      1063
           4       0.94      0.94      0.94      1064
           5       0.97      0.96      0.97      1064
           6       0.97      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.20502371384654208
Epoch 19 loss is 0.1600604545972537
Epoch 29 loss is 0.17638293655801232
Epoch 39 loss is 0.11561468644927743
Epoch 49 loss is 0.09013697049000392
Epoch 59 loss is 0.08748177132875291
Epoch 69 loss is 0.07197819254783605
Epoch 79 loss is 0.0702369190086657
Epoch 89 loss is 0.07234675932731245
Epoch 99 loss is 0.068253828507892
Epoch 109 loss is 0.07305692741178821
Epoch 119 loss is 0.07672600907945662
Epoch 129 loss is 0.07097353389268429
Epoch 139 loss is 0.06347196049334779
Epoch 149 loss is 0.0644738520961493
Epoch 159 loss is 0.05376314275695037
Epoch 169 loss is 0.05644687125414726
Epoch 179 loss is 0.048773048522865105
Epoch 189 loss is 0.04476576239785017
Epoch 199 loss is 0.04786218107136711
Train Acc.:  0.9728032131946076
              precision    recall  f1-score   support

           0       0.99      0.97      0.98      1064
           1       0.96      0.96      0.96      1064
           2       0.99      0.99      0.99      1064
           3       0.98      0.97      0.98      1063
           4       0.97      0.95      0.96      1064
           5       0.96      0.96      0.96      1064
           6       0.96      0.94      0.95      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.95      0.95      0.95      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.22543501013461756
Epoch 19 loss is 0.17068695324254124
Epoch 29 loss is 0.12409391685149763
Epoch 39 loss is 0.11697496548175042
Epoch 49 loss is 0.08876241864971807
Epoch 59 loss is 0.08153087295328582
Epoch 69 loss is 0.07483510813769084
Epoch 79 loss is 0.10710111668977772
Epoch 89 loss is 0.08042401739956408
Epoch 99 loss is 0.07369310521635002
Epoch 109 loss is 0.07451935209149846
Epoch 119 loss is 0.059885478707408256
Epoch 129 loss is 0.0567862421178645
Epoch 139 loss is 0.05093684395615367
Epoch 149 loss is 0.06306265519934738
Epoch 159 loss is 0.05432974760807225
Epoch 169 loss is 0.04892963285147231
Epoch 179 loss is 0.04994840531613505
Epoch 189 loss is 0.051194054698050454
Epoch 199 loss is 0.044596025063632284
Train Acc.:  0.9714572606661397
              precision    recall  f1-score   support

           0       0.99      0.98      0.98      1063
           1       0.96      0.96      0.96      1064
           2       0.99      0.99      0.99      1064
           3       0.98      0.99      0.98      1063
           4       0.96      0.96      0.96      1064
           5       0.96      0.95      0.95      1064
           6       0.95      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.94      0.95      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2782503201838837
Epoch 19 loss is 0.16120408621727206
Epoch 29 loss is 0.12418159289722493
Epoch 39 loss is 0.09063661342210186
Epoch 49 loss is 0.08274663620794244
Epoch 59 loss is 0.07859951940472655
Epoch 69 loss is 0.07256532240445507
Epoch 79 loss is 0.06916262503726474
Epoch 89 loss is 0.0630058410204737
Epoch 99 loss is 0.07026817485377791
Epoch 109 loss is 0.05949501562367503
Epoch 119 loss is 0.053246533281885174
Epoch 129 loss is 0.05572973071008147
Epoch 139 loss is 0.05570382762746428
Epoch 149 loss is 0.08662761110902355
Epoch 159 loss is 0.057920531802435
Epoch 169 loss is 0.06671310002777765
Epoch 179 loss is 0.06361666252378624
Epoch 189 loss is 0.0634835982068112
Epoch 199 loss is 0.05273022961513594
Train Acc.:  0.9670989381930053
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1064
           1       0.95      0.94      0.95      1064
           2       0.99      0.98      0.98      1064
           3       0.99      0.98      0.98      1064
           4       0.96      0.96      0.96      1064
           5       0.95      0.95      0.95      1064
           6       0.94      0.95      0.94      1063
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.98      0.94      0.96      1064
          10       0.98      0.96      0.97      1064

   micro avg       0.97      0.97      0.97     11702
   macro avg       0.97      0.97      0.97     11702
weighted avg       0.97      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-100-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-100-11">MLMVN [48-100-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-100-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">100</span>)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">100</span>, <span class="dv">11</span>)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-34"><a href="#cb57-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb57-35"><a href="#cb57-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb57-36"><a href="#cb57-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb57-37"><a href="#cb57-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-38"><a href="#cb57-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb57-39"><a href="#cb57-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb57-40"><a href="#cb57-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb57-41"><a href="#cb57-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-42"><a href="#cb57-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb57-43"><a href="#cb57-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb57-44"><a href="#cb57-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb57-45"><a href="#cb57-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-46"><a href="#cb57-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb57-47"><a href="#cb57-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb57-48"><a href="#cb57-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb57-49"><a href="#cb57-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-50"><a href="#cb57-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb57-51"><a href="#cb57-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb57-52"><a href="#cb57-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb57-53"><a href="#cb57-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb57-54"><a href="#cb57-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb57-55"><a href="#cb57-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-56"><a href="#cb57-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb57-57"><a href="#cb57-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb57-58"><a href="#cb57-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb57-59"><a href="#cb57-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-60"><a href="#cb57-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-61"><a href="#cb57-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb57-62"><a href="#cb57-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb57-63"><a href="#cb57-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb57-64"><a href="#cb57-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb57-65"><a href="#cb57-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb57-66"><a href="#cb57-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-67"><a href="#cb57-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb57-68"><a href="#cb57-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb57-69"><a href="#cb57-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb57-70"><a href="#cb57-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb57-71"><a href="#cb57-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-72"><a href="#cb57-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb57-73"><a href="#cb57-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-74"><a href="#cb57-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb57-75"><a href="#cb57-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb57-76"><a href="#cb57-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb57-77"><a href="#cb57-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb57-78"><a href="#cb57-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb57-79"><a href="#cb57-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-80"><a href="#cb57-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb57-81"><a href="#cb57-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb57-82"><a href="#cb57-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb57-83"><a href="#cb57-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-84"><a href="#cb57-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb57-85"><a href="#cb57-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb57-86"><a href="#cb57-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb57-87"><a href="#cb57-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-88"><a href="#cb57-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb57-89"><a href="#cb57-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb57-90"><a href="#cb57-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-91"><a href="#cb57-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb57-92"><a href="#cb57-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb57-93"><a href="#cb57-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-94"><a href="#cb57-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb57-95"><a href="#cb57-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb57-96"><a href="#cb57-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb57-97"><a href="#cb57-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-100-11]"</span>,</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-100-11]"</span>,</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=5a39cc009dd34b58bf9f7dc26695a102
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/5a39cc009dd34b58bf9f7dc26695a102/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-100-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb61-29"><a href="#cb61-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb61-30"><a href="#cb61-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-31"><a href="#cb61-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-32"><a href="#cb61-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb61-33"><a href="#cb61-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-34"><a href="#cb61-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb61-35"><a href="#cb61-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb61-36"><a href="#cb61-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-37"><a href="#cb61-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb61-38"><a href="#cb61-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb61-39"><a href="#cb61-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb61-40"><a href="#cb61-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-41"><a href="#cb61-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb61-42"><a href="#cb61-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb61-43"><a href="#cb61-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb61-44"><a href="#cb61-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-45"><a href="#cb61-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb61-46"><a href="#cb61-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb61-47"><a href="#cb61-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb61-48"><a href="#cb61-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-49"><a href="#cb61-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb61-50"><a href="#cb61-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb61-51"><a href="#cb61-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb61-52"><a href="#cb61-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb61-53"><a href="#cb61-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-54"><a href="#cb61-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb61-55"><a href="#cb61-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb61-56"><a href="#cb61-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb61-57"><a href="#cb61-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb61-58"><a href="#cb61-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb61-59"><a href="#cb61-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-60"><a href="#cb61-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-61"><a href="#cb61-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb61-62"><a href="#cb61-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb61-63"><a href="#cb61-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-64"><a href="#cb61-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-65"><a href="#cb61-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb61-66"><a href="#cb61-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb61-67"><a href="#cb61-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-68"><a href="#cb61-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-69"><a href="#cb61-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb61-70"><a href="#cb61-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb61-71"><a href="#cb61-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-72"><a href="#cb61-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-73"><a href="#cb61-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb61-74"><a href="#cb61-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb61-75"><a href="#cb61-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-76"><a href="#cb61-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-77"><a href="#cb61-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb61-78"><a href="#cb61-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb61-79"><a href="#cb61-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-80"><a href="#cb61-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-81"><a href="#cb61-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb61-82"><a href="#cb61-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb61-83"><a href="#cb61-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-84"><a href="#cb61-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-85"><a href="#cb61-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb61-86"><a href="#cb61-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb61-87"><a href="#cb61-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-88"><a href="#cb61-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb61-89"><a href="#cb61-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb61-90"><a href="#cb61-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb61-91"><a href="#cb61-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 21:54:30,788 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.
Epoch 9 loss is 0.2013734097530227
Epoch 19 loss is 0.13194389245179217
Epoch 29 loss is 0.10095062501209577
Epoch 39 loss is 0.09074072104898326
Epoch 49 loss is 0.08300947225827868
Epoch 59 loss is 0.0738162527877322
Epoch 69 loss is 0.07072649592151344
Epoch 79 loss is 0.07138726478663758
Epoch 89 loss is 0.06214147198340652
Epoch 99 loss is 0.07020503698271563
Epoch 109 loss is 0.074644080701454
Epoch 119 loss is 0.06734553316012935
Epoch 129 loss is 0.059474967215059765
Epoch 139 loss is 0.07335145914140831
Epoch 149 loss is 0.062376795077677044
Epoch 159 loss is 0.0526909906751619
Epoch 169 loss is 0.04941692807072795
Epoch 179 loss is 0.04210521010111257
Epoch 189 loss is 0.0447379761754187
Epoch 199 loss is 0.03882946678160469
Train Acc.:  0.97899886769073
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1063
           1       0.95      0.94      0.95      1064
           2       0.99      0.99      0.99      1064
           3       0.99      0.99      0.99      1064
           4       0.97      0.96      0.97      1064
           5       0.95      0.95      0.95      1063
           6       0.96      0.95      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.94      0.96      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.21319071325891403
Epoch 19 loss is 0.1541410855291198
Epoch 29 loss is 0.12536923443070858
Epoch 39 loss is 0.09970140587708567
Epoch 49 loss is 0.08215366375661032
Epoch 59 loss is 0.07893182432329253
Epoch 69 loss is 0.07569625143171067
Epoch 79 loss is 0.07090777313842668
Epoch 89 loss is 0.06514219463825727
Epoch 99 loss is 0.06887248604052763
Epoch 109 loss is 0.057908520270728236
Epoch 119 loss is 0.057421434582441984
Epoch 129 loss is 0.04913560375422588
Epoch 139 loss is 0.04935810215299344
Epoch 149 loss is 0.04237250334663373
Epoch 159 loss is 0.040383543448331234
Epoch 169 loss is 0.036213423181436236
Epoch 179 loss is 0.03839429011389191
Epoch 189 loss is 0.03846825102366323
Epoch 199 loss is 0.037638064394336875
Train Acc.:  0.9823317025231268
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1063
           1       0.96      0.96      0.96      1064
           2       1.00      0.99      1.00      1064
           3       0.98      0.99      0.99      1063
           4       0.96      0.97      0.96      1064
           5       0.97      0.96      0.96      1064
           6       0.96      0.95      0.96      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.98      0.97      0.98     11702
   macro avg       0.98      0.97      0.98     11702
weighted avg       0.98      0.97      0.98     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.20835717627283434
Epoch 19 loss is 0.1410003145381981
Epoch 29 loss is 0.10362160718673499
Epoch 39 loss is 0.09535519170107241
Epoch 49 loss is 0.08623970792143622
Epoch 59 loss is 0.06967393203823516
Epoch 69 loss is 0.07223157768403805
Epoch 79 loss is 0.06292363524435325
Epoch 89 loss is 0.06516889101094622
Epoch 99 loss is 0.05962646936523845
Epoch 109 loss is 0.052085959520220305
Epoch 119 loss is 0.05127448522332268
Epoch 129 loss is 0.04993565878003458
Epoch 139 loss is 0.04217621548518707
Epoch 149 loss is 0.04918457656177097
Epoch 159 loss is 0.03850822431866483
Epoch 169 loss is 0.038279677015932684
Epoch 179 loss is 0.039339300558995774
Epoch 189 loss is 0.03874658731757177
Epoch 199 loss is 0.036737692281627654
Train Acc.:  0.9827589890401008
              precision    recall  f1-score   support

           0       0.99      0.99      0.99      1064
           1       0.95      0.95      0.95      1064
           2       0.99      0.99      0.99      1064
           3       1.00      0.98      0.99      1063
           4       0.97      0.96      0.97      1064
           5       0.97      0.95      0.96      1064
           6       0.96      0.94      0.95      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.97      0.97      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.98      0.97      0.98     11702
   macro avg       0.98      0.97      0.98     11702
weighted avg       0.98      0.97      0.98     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2047739384042335
Epoch 19 loss is 0.1240691449665739
Epoch 29 loss is 0.1250756910889213
Epoch 39 loss is 0.09980943240036731
Epoch 49 loss is 0.07931262188990955
Epoch 59 loss is 0.06880595699322983
Epoch 69 loss is 0.07295263256108461
Epoch 79 loss is 0.0657251223690606
Epoch 89 loss is 0.06288437799940251
Epoch 99 loss is 0.06078154801969977
Epoch 109 loss is 0.05630173979958109
Epoch 119 loss is 0.05059079914071474
Epoch 129 loss is 0.0519624732081652
Epoch 139 loss is 0.04989906966215809
Epoch 149 loss is 0.046105722466105954
Epoch 159 loss is 0.04479487999819714
Epoch 169 loss is 0.05169914293629347
Epoch 179 loss is 0.05468717561123138
Epoch 189 loss is 0.04472661993607276
Epoch 199 loss is 0.04167204375351288
Train Acc.:  0.9793193325784605
              precision    recall  f1-score   support

           0       0.99      0.97      0.98      1063
           1       0.93      0.96      0.94      1064
           2       1.00      0.99      1.00      1064
           3       0.99      0.98      0.99      1063
           4       0.97      0.97      0.97      1064
           5       0.97      0.94      0.95      1064
           6       0.96      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.98      0.97      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.24100019695211888
Epoch 19 loss is 0.15365636763706683
Epoch 29 loss is 0.11363793386653538
Epoch 39 loss is 0.09423439262203624
Epoch 49 loss is 0.08824477344066799
Epoch 59 loss is 0.07361807474799302
Epoch 69 loss is 0.07530689777501173
Epoch 79 loss is 0.06350203232630086
Epoch 89 loss is 0.061400948333146264
Epoch 99 loss is 0.05771039868957064
Epoch 109 loss is 0.057609481716596814
Epoch 119 loss is 0.05653083207399525
Epoch 129 loss is 0.05154005805827653
Epoch 139 loss is 0.05072576182283894
Epoch 149 loss is 0.046395221236291556
Epoch 159 loss is 0.04620623101501136
Epoch 169 loss is 0.042652961928911375
Epoch 179 loss is 0.0574112161346133
Epoch 189 loss is 0.044568967625431236
Epoch 199 loss is 0.03852016601626823
Train Acc.:  0.9786570384771508
              precision    recall  f1-score   support

           0       0.98      0.98      0.98      1064
           1       0.95      0.96      0.95      1064
           2       0.99      0.98      0.99      1064
           3       0.99      0.98      0.99      1064
           4       0.98      0.95      0.96      1064
           5       0.96      0.94      0.95      1064
           6       0.94      0.95      0.95      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.96      0.95      0.96      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.97     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="multi-layer" class="level2">
<h2 class="anchored" data-anchor-id="multi-layer">Multi Layer</h2>
<section id="mlmvn-48-10-10-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-10-10-11">MLMVN [48-10-10-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-10-10-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">10</span>)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb74-27"><a href="#cb74-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb74-28"><a href="#cb74-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-32"><a href="#cb74-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb74-33"><a href="#cb74-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb74-34"><a href="#cb74-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-35"><a href="#cb74-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb74-36"><a href="#cb74-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb74-37"><a href="#cb74-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-38"><a href="#cb74-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb74-39"><a href="#cb74-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb74-40"><a href="#cb74-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-41"><a href="#cb74-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb74-42"><a href="#cb74-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb74-43"><a href="#cb74-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb74-44"><a href="#cb74-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-45"><a href="#cb74-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb74-46"><a href="#cb74-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb74-47"><a href="#cb74-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb74-48"><a href="#cb74-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-49"><a href="#cb74-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb74-50"><a href="#cb74-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb74-51"><a href="#cb74-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb74-52"><a href="#cb74-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-53"><a href="#cb74-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb74-54"><a href="#cb74-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb74-55"><a href="#cb74-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb74-56"><a href="#cb74-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-57"><a href="#cb74-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb74-58"><a href="#cb74-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb74-59"><a href="#cb74-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb74-60"><a href="#cb74-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb74-61"><a href="#cb74-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb74-62"><a href="#cb74-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-63"><a href="#cb74-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb74-64"><a href="#cb74-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb74-65"><a href="#cb74-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb74-66"><a href="#cb74-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-67"><a href="#cb74-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-68"><a href="#cb74-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb74-69"><a href="#cb74-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb74-70"><a href="#cb74-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb74-71"><a href="#cb74-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb74-72"><a href="#cb74-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb74-73"><a href="#cb74-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-74"><a href="#cb74-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb74-75"><a href="#cb74-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb74-76"><a href="#cb74-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb74-77"><a href="#cb74-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb74-78"><a href="#cb74-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-79"><a href="#cb74-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb74-80"><a href="#cb74-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-81"><a href="#cb74-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb74-82"><a href="#cb74-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb74-83"><a href="#cb74-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb74-84"><a href="#cb74-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb74-85"><a href="#cb74-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb74-86"><a href="#cb74-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-87"><a href="#cb74-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb74-88"><a href="#cb74-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb74-89"><a href="#cb74-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb74-90"><a href="#cb74-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-91"><a href="#cb74-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb74-92"><a href="#cb74-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb74-93"><a href="#cb74-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb74-94"><a href="#cb74-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-95"><a href="#cb74-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb74-96"><a href="#cb74-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb74-97"><a href="#cb74-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb74-98"><a href="#cb74-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb74-99"><a href="#cb74-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb74-100"><a href="#cb74-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-101"><a href="#cb74-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb74-102"><a href="#cb74-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb74-103"><a href="#cb74-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb74-104"><a href="#cb74-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-10-10-11]"</span>,</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-10-10-11]"</span>,</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=938ba07947984546b3b8a4b5a774d0d2
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/938ba07947984546b3b8a4b5a774d0d2/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-10-10-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb78-33"><a href="#cb78-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-34"><a href="#cb78-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb78-35"><a href="#cb78-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb78-36"><a href="#cb78-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-37"><a href="#cb78-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb78-38"><a href="#cb78-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb78-39"><a href="#cb78-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb78-40"><a href="#cb78-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-41"><a href="#cb78-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb78-42"><a href="#cb78-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb78-43"><a href="#cb78-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb78-44"><a href="#cb78-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-45"><a href="#cb78-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb78-46"><a href="#cb78-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb78-47"><a href="#cb78-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb78-48"><a href="#cb78-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-49"><a href="#cb78-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb78-50"><a href="#cb78-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb78-51"><a href="#cb78-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb78-52"><a href="#cb78-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-53"><a href="#cb78-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-54"><a href="#cb78-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb78-55"><a href="#cb78-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb78-56"><a href="#cb78-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb78-57"><a href="#cb78-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-58"><a href="#cb78-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-59"><a href="#cb78-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-60"><a href="#cb78-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-61"><a href="#cb78-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb78-62"><a href="#cb78-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb78-63"><a href="#cb78-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-64"><a href="#cb78-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-65"><a href="#cb78-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb78-66"><a href="#cb78-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb78-67"><a href="#cb78-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-68"><a href="#cb78-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-69"><a href="#cb78-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb78-70"><a href="#cb78-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb78-71"><a href="#cb78-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-72"><a href="#cb78-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-73"><a href="#cb78-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb78-74"><a href="#cb78-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb78-75"><a href="#cb78-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-76"><a href="#cb78-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-77"><a href="#cb78-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb78-78"><a href="#cb78-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb78-79"><a href="#cb78-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-80"><a href="#cb78-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-81"><a href="#cb78-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb78-82"><a href="#cb78-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb78-83"><a href="#cb78-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-84"><a href="#cb78-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-85"><a href="#cb78-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb78-86"><a href="#cb78-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb78-87"><a href="#cb78-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-88"><a href="#cb78-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb78-89"><a href="#cb78-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb78-90"><a href="#cb78-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb78-91"><a href="#cb78-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 22:30:29,803 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.
Epoch 9 loss is 0.541454749663616
Epoch 19 loss is 0.8791410713849316
Epoch 29 loss is 0.8839316416940527
Epoch 39 loss is 0.8966399825316692
Epoch 49 loss is 0.9264105355661884
Epoch 59 loss is 0.951371120459427
Epoch 69 loss is 0.9412028121256291
Epoch 79 loss is 0.9732946912045937
Epoch 89 loss is 0.9181100154489218
Epoch 99 loss is 0.9329806316494657
Epoch 109 loss is 0.9328285713006949
Epoch 119 loss is 0.9419897937541761
Epoch 129 loss is 0.9427600691188952
Epoch 139 loss is 0.9091315900275359
Epoch 149 loss is 0.9324776865430842
Epoch 159 loss is 0.9134668719369948
Epoch 169 loss is 0.9403598662976411
Epoch 179 loss is 0.9422566273044115
Epoch 189 loss is 0.9543148096619407
Epoch 199 loss is 0.9549645974775849
Train Acc.:  0.5539342406050377
              precision    recall  f1-score   support

           0       0.77      0.60      0.68      1063
           1       0.65      0.47      0.55      1064
           2       0.94      0.67      0.78      1064
           3       0.89      0.62      0.73      1064
           4       0.32      0.16      0.21      1064
           5       0.73      0.69      0.71      1063
           6       0.82      0.44      0.57      1064
           7       0.91      0.69      0.78      1064
           8       0.98      0.95      0.97      1064
           9       0.68      0.90      0.77      1064
          10       0.76      0.69      0.72      1064

   micro avg       0.78      0.63      0.69     11702
   macro avg       0.77      0.63      0.68     11702
weighted avg       0.77      0.63      0.68     11702
 samples avg       0.59      0.63      0.60     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.34781102538165637
Epoch 19 loss is 0.4926261439071808
Epoch 29 loss is 0.6328775813367924
Epoch 39 loss is 0.7170839574504813
Epoch 49 loss is 0.6934163739177756
Epoch 59 loss is 0.7141162567964217
Epoch 69 loss is 0.7691114368900678
Epoch 79 loss is 0.7562372003321447
Epoch 89 loss is 0.7766109368260417
Epoch 99 loss is 0.7075230256227628
Epoch 109 loss is 0.7393482060014152
Epoch 119 loss is 0.7458410915657656
Epoch 129 loss is 0.7585006377982786
Epoch 139 loss is 0.7711069196763435
Epoch 149 loss is 0.7341288291259812
Epoch 159 loss is 0.7801133649364619
Epoch 169 loss is 0.7263681341448134
Epoch 179 loss is 0.7519011054649812
Epoch 189 loss is 0.7048160484621695
Epoch 199 loss is 0.7475459901191056
Train Acc.:  0.7177986198645502
              precision    recall  f1-score   support

           0       0.89      0.83      0.86      1063
           1       0.83      0.74      0.78      1064
           2       0.94      0.84      0.89      1064
           3       0.82      0.79      0.81      1063
           4       0.68      0.66      0.67      1064
           5       0.83      0.83      0.83      1064
           6       0.70      0.60      0.64      1064
           7       0.99      0.94      0.96      1064
           8       0.99      0.97      0.98      1064
           9       0.83      0.75      0.79      1064
          10       0.76      0.75      0.76      1064

   micro avg       0.84      0.79      0.82     11702
   macro avg       0.84      0.79      0.81     11702
weighted avg       0.84      0.79      0.81     11702
 samples avg       0.75      0.79      0.77     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.608739696487545
Epoch 19 loss is 0.7696193880760408
Epoch 29 loss is 0.803278156062957
Epoch 39 loss is 0.8387883156242925
Epoch 49 loss is 0.7807021280520069
Epoch 59 loss is 0.8088241619223006
Epoch 69 loss is 0.8070463633451938
Epoch 79 loss is 0.7945587323244073
Epoch 89 loss is 0.8392897794392442
Epoch 99 loss is 0.8214029347151293
Epoch 109 loss is 0.8340895850326423
Epoch 119 loss is 0.8219225005001145
Epoch 129 loss is 0.8507321513973675
Epoch 139 loss is 0.8635722140704808
Epoch 149 loss is 0.8446035024910838
Epoch 159 loss is 0.8153800610100258
Epoch 169 loss is 0.860221271499099
Epoch 179 loss is 0.8713979258843838
Epoch 189 loss is 0.8423941483588755
Epoch 199 loss is 0.7982078285674677
Train Acc.:  0.5234687119447946
              precision    recall  f1-score   support

           0       0.82      0.76      0.79      1064
           1       0.73      0.59      0.66      1064
           2       0.59      0.22      0.33      1064
           3       0.75      0.68      0.71      1063
           4       0.55      0.52      0.54      1064
           5       0.49      0.48      0.48      1064
           6       0.70      0.56      0.62      1063
           7       0.88      0.77      0.83      1064
           8       0.99      0.92      0.95      1064
           9       0.76      0.57      0.65      1064
          10       0.85      0.73      0.79      1064

   micro avg       0.74      0.62      0.68     11702
   macro avg       0.74      0.62      0.67     11702
weighted avg       0.74      0.62      0.67     11702
 samples avg       0.57      0.62      0.58     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2881004625544315
Epoch 19 loss is 0.257601894880915
Epoch 29 loss is 0.3070432292199646
Epoch 39 loss is 1.0114987272533784
Epoch 49 loss is 1.0626524147227299
Epoch 59 loss is 0.7220235506940849
Epoch 69 loss is 0.8701604350827594
Epoch 79 loss is 1.2108727994663198
Epoch 89 loss is 1.138239338460986
Epoch 99 loss is 1.4603121925675742
Epoch 109 loss is 1.4954496178854135
Epoch 119 loss is 1.2679945725843622
Epoch 129 loss is 1.0164146082349543
Epoch 139 loss is 1.1297298068650685
Epoch 149 loss is 1.253219169435164
Epoch 159 loss is 1.1736291050368979
Epoch 169 loss is 1.1826341111065124
Epoch 179 loss is 1.4031484346107788
Epoch 189 loss is 1.2205340600894026
Epoch 199 loss is 1.3048065655004846
Train Acc.:  0.7068600850300168
              precision    recall  f1-score   support

           0       0.75      0.86      0.80      1063
           1       0.81      0.79      0.80      1064
           2       0.94      0.85      0.89      1064
           3       0.87      0.84      0.85      1063
           4       0.73      0.48      0.58      1064
           5       0.85      0.77      0.81      1064
           6       0.84      0.65      0.73      1064
           7       0.97      0.97      0.97      1064
           8       0.99      0.95      0.97      1064
           9       0.53      0.97      0.69      1064
          10       0.92      0.76      0.83      1064

   micro avg       0.81      0.81      0.81     11702
   macro avg       0.84      0.81      0.81     11702
weighted avg       0.84      0.81      0.81     11702
 samples avg       0.75      0.81      0.77     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.5949722022582887
Epoch 19 loss is 0.8542236273844039
Epoch 29 loss is 0.8749541100255886
Epoch 39 loss is 0.9226380212308712
Epoch 49 loss is 0.9189451568276324
Epoch 59 loss is 0.9365599144446709
Epoch 69 loss is 0.9302873031362572
Epoch 79 loss is 0.9133083164488084
Epoch 89 loss is 0.867765838931149
Epoch 99 loss is 0.9202292373715069
Epoch 109 loss is 0.8840360283963321
Epoch 119 loss is 0.8900084272652079
Epoch 129 loss is 0.8624389865444407
Epoch 139 loss is 0.86264759032102
Epoch 149 loss is 0.877634036686458
Epoch 159 loss is 0.8911980209206599
Epoch 169 loss is 0.8815296026365007
Epoch 179 loss is 0.8890566057952398
Epoch 189 loss is 0.8732226294946295
Epoch 199 loss is 0.9232258855476119
Train Acc.:  0.5140470442455188
              precision    recall  f1-score   support

           0       0.80      0.66      0.72      1064
           1       0.72      0.55      0.62      1064
           2       0.75      0.53      0.62      1064
           3       0.75      0.55      0.64      1064
           4       0.49      0.36      0.42      1064
           5       0.76      0.72      0.74      1064
           6       0.56      0.38      0.45      1063
           7       0.94      0.90      0.92      1064
           8       0.98      0.97      0.98      1063
           9       0.64      0.47      0.54      1064
          10       0.75      0.50      0.60      1064

   micro avg       0.75      0.60      0.67     11702
   macro avg       0.74      0.60      0.66     11702
weighted avg       0.74      0.60      0.66     11702
 samples avg       0.55      0.60      0.57     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-20-20-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-20-20-11">MLMVN [48-20-20-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-20-20-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">20</span>)</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">20</span>, <span class="dv">11</span>)</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-41"><a href="#cb91-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb91-42"><a href="#cb91-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb91-43"><a href="#cb91-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb91-44"><a href="#cb91-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-45"><a href="#cb91-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb91-46"><a href="#cb91-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb91-47"><a href="#cb91-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb91-48"><a href="#cb91-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-49"><a href="#cb91-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb91-50"><a href="#cb91-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb91-51"><a href="#cb91-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb91-52"><a href="#cb91-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-53"><a href="#cb91-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb91-54"><a href="#cb91-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb91-55"><a href="#cb91-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb91-56"><a href="#cb91-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-57"><a href="#cb91-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb91-58"><a href="#cb91-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb91-59"><a href="#cb91-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb91-60"><a href="#cb91-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb91-61"><a href="#cb91-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb91-62"><a href="#cb91-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-63"><a href="#cb91-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb91-64"><a href="#cb91-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb91-65"><a href="#cb91-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb91-66"><a href="#cb91-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-67"><a href="#cb91-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-68"><a href="#cb91-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb91-69"><a href="#cb91-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb91-70"><a href="#cb91-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb91-71"><a href="#cb91-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb91-72"><a href="#cb91-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb91-73"><a href="#cb91-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-74"><a href="#cb91-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb91-75"><a href="#cb91-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb91-76"><a href="#cb91-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb91-77"><a href="#cb91-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb91-78"><a href="#cb91-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-79"><a href="#cb91-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb91-80"><a href="#cb91-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-81"><a href="#cb91-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb91-82"><a href="#cb91-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb91-83"><a href="#cb91-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb91-84"><a href="#cb91-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb91-85"><a href="#cb91-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb91-86"><a href="#cb91-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-87"><a href="#cb91-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb91-88"><a href="#cb91-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb91-89"><a href="#cb91-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb91-90"><a href="#cb91-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-91"><a href="#cb91-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb91-92"><a href="#cb91-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb91-93"><a href="#cb91-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb91-94"><a href="#cb91-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-95"><a href="#cb91-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb91-96"><a href="#cb91-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb91-97"><a href="#cb91-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb91-98"><a href="#cb91-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb91-99"><a href="#cb91-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb91-100"><a href="#cb91-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-101"><a href="#cb91-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb91-102"><a href="#cb91-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb91-103"><a href="#cb91-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb91-104"><a href="#cb91-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-20-20-11]"</span>,</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-20-20-11]"</span>,</span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=602b39a918d54282b2aaed44b2781349
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/602b39a918d54282b2aaed44b2781349/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-20-20-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb95-25"><a href="#cb95-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb95-26"><a href="#cb95-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb95-27"><a href="#cb95-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb95-28"><a href="#cb95-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb95-29"><a href="#cb95-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb95-30"><a href="#cb95-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb95-31"><a href="#cb95-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-32"><a href="#cb95-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb95-33"><a href="#cb95-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-34"><a href="#cb95-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb95-35"><a href="#cb95-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb95-36"><a href="#cb95-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-37"><a href="#cb95-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb95-38"><a href="#cb95-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb95-39"><a href="#cb95-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb95-40"><a href="#cb95-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-41"><a href="#cb95-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb95-42"><a href="#cb95-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb95-43"><a href="#cb95-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb95-44"><a href="#cb95-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-45"><a href="#cb95-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb95-46"><a href="#cb95-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb95-47"><a href="#cb95-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb95-48"><a href="#cb95-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb95-49"><a href="#cb95-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb95-50"><a href="#cb95-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb95-51"><a href="#cb95-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb95-52"><a href="#cb95-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb95-53"><a href="#cb95-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb95-54"><a href="#cb95-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb95-55"><a href="#cb95-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb95-56"><a href="#cb95-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb95-57"><a href="#cb95-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb95-58"><a href="#cb95-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb95-59"><a href="#cb95-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-60"><a href="#cb95-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-61"><a href="#cb95-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb95-62"><a href="#cb95-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb95-63"><a href="#cb95-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-64"><a href="#cb95-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-65"><a href="#cb95-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb95-66"><a href="#cb95-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb95-67"><a href="#cb95-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-68"><a href="#cb95-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-69"><a href="#cb95-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb95-70"><a href="#cb95-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb95-71"><a href="#cb95-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-72"><a href="#cb95-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-73"><a href="#cb95-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb95-74"><a href="#cb95-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb95-75"><a href="#cb95-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-76"><a href="#cb95-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-77"><a href="#cb95-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb95-78"><a href="#cb95-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb95-79"><a href="#cb95-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-80"><a href="#cb95-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-81"><a href="#cb95-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb95-82"><a href="#cb95-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb95-83"><a href="#cb95-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-84"><a href="#cb95-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-85"><a href="#cb95-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb95-86"><a href="#cb95-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb95-87"><a href="#cb95-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-88"><a href="#cb95-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb95-89"><a href="#cb95-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb95-90"><a href="#cb95-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb95-91"><a href="#cb95-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 22:48:18,328 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.
Epoch 9 loss is 0.37705022557280987
Epoch 19 loss is 0.2740187803883329
Epoch 29 loss is 0.45516830949325443
Epoch 39 loss is 0.6148356752471408
Epoch 49 loss is 0.6222640142110359
Epoch 59 loss is 0.6982017039915205
Epoch 69 loss is 0.6373358815772685
Epoch 79 loss is 0.6311755328631491
Epoch 89 loss is 0.6082573092923741
Epoch 99 loss is 0.603828268133041
Epoch 109 loss is 0.6389025164997437
Epoch 119 loss is 0.6260305680358405
Epoch 129 loss is 0.6324748094532601
Epoch 139 loss is 0.5943924798856283
Epoch 149 loss is 0.6344508674597645
Epoch 159 loss is 0.6141789833696603
Epoch 169 loss is 0.6086970307893493
Epoch 179 loss is 0.5824319841233837
Epoch 189 loss is 0.6191231138277477
Epoch 199 loss is 0.5666659157355745
Train Acc.:  0.730681308351315
              precision    recall  f1-score   support

           0       0.94      0.91      0.92      1063
           1       0.82      0.70      0.76      1064
           2       0.86      0.73      0.79      1064
           3       0.89      0.83      0.86      1064
           4       0.84      0.81      0.82      1064
           5       0.86      0.76      0.81      1063
           6       0.75      0.87      0.81      1064
           7       0.97      0.91      0.94      1064
           8       1.00      0.97      0.99      1064
           9       0.80      0.68      0.73      1064
          10       0.70      0.53      0.60      1064

   micro avg       0.86      0.79      0.82     11702
   macro avg       0.86      0.79      0.82     11702
weighted avg       0.86      0.79      0.82     11702
 samples avg       0.76      0.79      0.77     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.28806597272279794
Epoch 19 loss is 0.22970233591793254
Epoch 29 loss is 0.28585426998250507
Epoch 39 loss is 0.38502680568849784
Epoch 49 loss is 0.7625332656829702
Epoch 59 loss is 0.6732264557756368
Epoch 69 loss is 0.6217295541960539
Epoch 79 loss is 0.6893322196897432
Epoch 89 loss is 0.6622742723169387
Epoch 99 loss is 0.6960203626199706
Epoch 109 loss is 0.6777245937457624
Epoch 119 loss is 0.7176237176692954
Epoch 129 loss is 0.742547024761961
Epoch 139 loss is 0.7291200957471028
Epoch 149 loss is 0.7470137933371792
Epoch 159 loss is 0.7220883556014549
Epoch 169 loss is 0.7159556336891169
Epoch 179 loss is 0.7108887748361287
Epoch 189 loss is 0.7436282315327294
Epoch 199 loss is 0.694954785146677
Train Acc.:  0.7865276561198111
              precision    recall  f1-score   support

           0       0.95      0.94      0.94      1063
           1       0.80      0.74      0.77      1064
           2       0.90      0.76      0.82      1064
           3       0.93      0.90      0.92      1063
           4       0.85      0.81      0.83      1064
           5       0.81      0.72      0.76      1064
           6       0.76      0.70      0.73      1064
           7       0.99      0.93      0.96      1064
           8       1.00      0.98      0.99      1064
           9       0.87      0.88      0.87      1064
          10       0.91      0.89      0.90      1064

   micro avg       0.89      0.84      0.86     11702
   macro avg       0.89      0.84      0.86     11702
weighted avg       0.89      0.84      0.86     11702
 samples avg       0.81      0.84      0.82     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2511134256926335
Epoch 19 loss is 0.22011394441410292
Epoch 29 loss is 0.22056851414206183
Epoch 39 loss is 0.2551644861115903
Epoch 49 loss is 0.47249132940476685
Epoch 59 loss is 0.6799162460754719
Epoch 69 loss is 0.6906088651202789
Epoch 79 loss is 0.7231533868835432
Epoch 89 loss is 0.738398383750411
Epoch 99 loss is 0.7811543296378222
Epoch 109 loss is 0.7535960303277581
Epoch 119 loss is 0.7163676381713118
Epoch 129 loss is 0.7103477423606941
Epoch 139 loss is 0.7078374401246286
Epoch 149 loss is 0.7199227568498184
Epoch 159 loss is 0.7107330170199827
Epoch 169 loss is 0.7056031770853555
Epoch 179 loss is 0.7327502640763232
Epoch 189 loss is 0.7094453790723616
Epoch 199 loss is 0.7120579963814156
Train Acc.:  0.7947956502232572
              precision    recall  f1-score   support

           0       0.94      0.91      0.93      1064
           1       0.85      0.80      0.82      1064
           2       0.87      0.77      0.82      1064
           3       0.93      0.86      0.89      1063
           4       0.84      0.82      0.83      1064
           5       0.85      0.86      0.86      1064
           6       0.82      0.70      0.75      1063
           7       0.96      0.90      0.93      1064
           8       0.99      0.95      0.97      1064
           9       0.86      0.87      0.86      1064
          10       0.90      0.87      0.88      1064

   micro avg       0.89      0.85      0.87     11702
   macro avg       0.89      0.85      0.87     11702
weighted avg       0.89      0.85      0.87     11702
 samples avg       0.82      0.85      0.83     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.28920254058590406
Epoch 19 loss is 0.20422793527357072
Epoch 29 loss is 0.23514562707658523
Epoch 39 loss is 0.30951769319469474
Epoch 49 loss is 0.802884197886128
Epoch 59 loss is 0.629426963749978
Epoch 69 loss is 0.6618189152336303
Epoch 79 loss is 0.639523350080704
Epoch 89 loss is 0.6033113390242072
Epoch 99 loss is 0.6365305132984823
Epoch 109 loss is 0.6149712207622507
Epoch 119 loss is 0.6332012921437696
Epoch 129 loss is 0.6520249413523534
Epoch 139 loss is 0.6450135768078111
Epoch 149 loss is 0.6724773114718959
Epoch 159 loss is 0.637959073743463
Epoch 169 loss is 0.5887977689032348
Epoch 179 loss is 0.6275123627575196
Epoch 189 loss is 0.6157577124370395
Epoch 199 loss is 0.5571599018372312
Train Acc.:  0.7962056957292712
              precision    recall  f1-score   support

           0       0.95      0.89      0.92      1063
           1       0.84      0.81      0.83      1064
           2       0.93      0.86      0.90      1064
           3       0.89      0.87      0.88      1063
           4       0.87      0.77      0.82      1064
           5       0.84      0.81      0.83      1064
           6       0.76      0.61      0.68      1064
           7       0.99      0.98      0.98      1064
           8       1.00      0.99      1.00      1064
           9       0.83      0.77      0.80      1064
          10       0.93      0.91      0.92      1064

   micro avg       0.90      0.84      0.87     11702
   macro avg       0.90      0.84      0.87     11702
weighted avg       0.90      0.84      0.87     11702
 samples avg       0.82      0.84      0.83     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3162340976591865
Epoch 19 loss is 0.23035400757661567
Epoch 29 loss is 0.3025872000844228
Epoch 39 loss is 0.5825874736764202
Epoch 49 loss is 0.5903150798951642
Epoch 59 loss is 0.5290111967616761
Epoch 69 loss is 0.48948755510049063
Epoch 79 loss is 0.5536584861929144
Epoch 89 loss is 0.5714292967837328
Epoch 99 loss is 0.6023988021147385
Epoch 109 loss is 0.6004173386515079
Epoch 119 loss is 0.5901975806362378
Epoch 129 loss is 0.607343459187801
Epoch 139 loss is 0.6290455412143036
Epoch 149 loss is 0.598815193614319
Epoch 159 loss is 0.5928518543280813
Epoch 169 loss is 0.6891633674581874
Epoch 179 loss is 0.6508888532882792
Epoch 189 loss is 0.6662645081833795
Epoch 199 loss is 0.6955834762648595
Train Acc.:  0.7916550943234987
              precision    recall  f1-score   support

           0       0.96      0.92      0.94      1064
           1       0.78      0.69      0.73      1064
           2       0.89      0.81      0.85      1064
           3       0.94      0.87      0.91      1064
           4       0.95      0.89      0.92      1064
           5       0.82      0.78      0.80      1064
           6       0.77      0.78      0.78      1063
           7       0.99      0.98      0.98      1064
           8       1.00      0.98      0.99      1063
           9       0.78      0.75      0.77      1064
          10       0.93      0.82      0.87      1064

   micro avg       0.89      0.84      0.87     11702
   macro avg       0.89      0.84      0.87     11702
weighted avg       0.89      0.84      0.87     11702
 samples avg       0.82      0.84      0.82     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-50-50-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-50-50-11">MLMVN [48-50-50-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-50-50-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">50</span>)</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb108-8"><a href="#cb108-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">50</span>, <span class="dv">50</span>)</span>
<span id="cb108-9"><a href="#cb108-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb108-10"><a href="#cb108-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">50</span>, <span class="dv">11</span>)</span>
<span id="cb108-11"><a href="#cb108-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb108-12"><a href="#cb108-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb108-13"><a href="#cb108-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb108-14"><a href="#cb108-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb108-15"><a href="#cb108-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb108-16"><a href="#cb108-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb108-17"><a href="#cb108-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb108-18"><a href="#cb108-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb108-19"><a href="#cb108-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb108-20"><a href="#cb108-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb108-21"><a href="#cb108-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb108-22"><a href="#cb108-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-23"><a href="#cb108-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb108-24"><a href="#cb108-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb108-25"><a href="#cb108-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb108-26"><a href="#cb108-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb108-27"><a href="#cb108-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb108-28"><a href="#cb108-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb108-29"><a href="#cb108-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb108-30"><a href="#cb108-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb108-31"><a href="#cb108-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-32"><a href="#cb108-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb108-33"><a href="#cb108-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb108-34"><a href="#cb108-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-35"><a href="#cb108-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb108-36"><a href="#cb108-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb108-37"><a href="#cb108-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-38"><a href="#cb108-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb108-39"><a href="#cb108-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb108-40"><a href="#cb108-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-41"><a href="#cb108-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb108-42"><a href="#cb108-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb108-43"><a href="#cb108-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb108-44"><a href="#cb108-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-45"><a href="#cb108-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb108-46"><a href="#cb108-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb108-47"><a href="#cb108-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb108-48"><a href="#cb108-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-49"><a href="#cb108-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb108-50"><a href="#cb108-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb108-51"><a href="#cb108-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb108-52"><a href="#cb108-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-53"><a href="#cb108-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb108-54"><a href="#cb108-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb108-55"><a href="#cb108-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb108-56"><a href="#cb108-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-57"><a href="#cb108-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb108-58"><a href="#cb108-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb108-59"><a href="#cb108-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb108-60"><a href="#cb108-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb108-61"><a href="#cb108-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb108-62"><a href="#cb108-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-63"><a href="#cb108-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb108-64"><a href="#cb108-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb108-65"><a href="#cb108-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb108-66"><a href="#cb108-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-67"><a href="#cb108-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-68"><a href="#cb108-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb108-69"><a href="#cb108-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb108-70"><a href="#cb108-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb108-71"><a href="#cb108-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb108-72"><a href="#cb108-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb108-73"><a href="#cb108-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-74"><a href="#cb108-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb108-75"><a href="#cb108-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb108-76"><a href="#cb108-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb108-77"><a href="#cb108-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb108-78"><a href="#cb108-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-79"><a href="#cb108-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb108-80"><a href="#cb108-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-81"><a href="#cb108-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb108-82"><a href="#cb108-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb108-83"><a href="#cb108-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb108-84"><a href="#cb108-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb108-85"><a href="#cb108-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb108-86"><a href="#cb108-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-87"><a href="#cb108-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb108-88"><a href="#cb108-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb108-89"><a href="#cb108-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb108-90"><a href="#cb108-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-91"><a href="#cb108-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb108-92"><a href="#cb108-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb108-93"><a href="#cb108-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb108-94"><a href="#cb108-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-95"><a href="#cb108-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb108-96"><a href="#cb108-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb108-97"><a href="#cb108-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb108-98"><a href="#cb108-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb108-99"><a href="#cb108-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb108-100"><a href="#cb108-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-101"><a href="#cb108-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb108-102"><a href="#cb108-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb108-103"><a href="#cb108-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb108-104"><a href="#cb108-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-50-50-11]"</span>,</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: <span class="dv">1</span>,</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb109-13"><a href="#cb109-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb109-14"><a href="#cb109-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb109-15"><a href="#cb109-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb109-16"><a href="#cb109-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-50-50-11]"</span>,</span>
<span id="cb109-17"><a href="#cb109-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb109-18"><a href="#cb109-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb109-19"><a href="#cb109-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=e0d8b1347f9249748b5b9f57cb06d03e
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e0d8b1347f9249748b5b9f57cb06d03e/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-50-50-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb112-9"><a href="#cb112-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-10"><a href="#cb112-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb112-11"><a href="#cb112-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb112-12"><a href="#cb112-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb112-13"><a href="#cb112-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb112-14"><a href="#cb112-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb112-15"><a href="#cb112-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-16"><a href="#cb112-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb112-17"><a href="#cb112-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb112-18"><a href="#cb112-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb112-19"><a href="#cb112-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-20"><a href="#cb112-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb112-21"><a href="#cb112-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb112-22"><a href="#cb112-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb112-23"><a href="#cb112-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb112-24"><a href="#cb112-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb112-25"><a href="#cb112-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb112-26"><a href="#cb112-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb112-27"><a href="#cb112-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb112-28"><a href="#cb112-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb112-29"><a href="#cb112-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb112-30"><a href="#cb112-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb112-31"><a href="#cb112-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-32"><a href="#cb112-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb112-33"><a href="#cb112-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-34"><a href="#cb112-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb112-35"><a href="#cb112-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb112-36"><a href="#cb112-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-37"><a href="#cb112-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb112-38"><a href="#cb112-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb112-39"><a href="#cb112-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb112-40"><a href="#cb112-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-41"><a href="#cb112-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb112-42"><a href="#cb112-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb112-43"><a href="#cb112-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb112-44"><a href="#cb112-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-45"><a href="#cb112-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb112-46"><a href="#cb112-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb112-47"><a href="#cb112-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb112-48"><a href="#cb112-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb112-49"><a href="#cb112-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb112-50"><a href="#cb112-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb112-51"><a href="#cb112-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb112-52"><a href="#cb112-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb112-53"><a href="#cb112-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb112-54"><a href="#cb112-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb112-55"><a href="#cb112-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb112-56"><a href="#cb112-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb112-57"><a href="#cb112-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb112-58"><a href="#cb112-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb112-59"><a href="#cb112-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-60"><a href="#cb112-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-61"><a href="#cb112-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb112-62"><a href="#cb112-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb112-63"><a href="#cb112-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-64"><a href="#cb112-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-65"><a href="#cb112-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb112-66"><a href="#cb112-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb112-67"><a href="#cb112-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-68"><a href="#cb112-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-69"><a href="#cb112-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb112-70"><a href="#cb112-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb112-71"><a href="#cb112-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-72"><a href="#cb112-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-73"><a href="#cb112-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb112-74"><a href="#cb112-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb112-75"><a href="#cb112-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-76"><a href="#cb112-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-77"><a href="#cb112-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb112-78"><a href="#cb112-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb112-79"><a href="#cb112-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-80"><a href="#cb112-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-81"><a href="#cb112-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb112-82"><a href="#cb112-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb112-83"><a href="#cb112-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-84"><a href="#cb112-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-85"><a href="#cb112-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb112-86"><a href="#cb112-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb112-87"><a href="#cb112-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb112-88"><a href="#cb112-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb112-89"><a href="#cb112-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb112-90"><a href="#cb112-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb112-91"><a href="#cb112-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-27 23:19:30,934 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.
Epoch 9 loss is 0.28872390806294274
Epoch 19 loss is 0.17694585572096805
Epoch 29 loss is 0.1425961563405807
Epoch 39 loss is 0.12819857234772986
Epoch 49 loss is 0.127805551287352
Epoch 59 loss is 0.11392432030870624
Epoch 69 loss is 0.09727040211071439
Epoch 79 loss is 0.09686726724688864
Epoch 89 loss is 0.09030257402693799
Epoch 99 loss is 0.09027066309985042
Epoch 109 loss is 0.08883324592656906
Epoch 119 loss is 0.08907481171375083
Epoch 129 loss is 0.08302457843559832
Epoch 139 loss is 0.07895430465486417
Epoch 149 loss is 0.07612614811554756
Epoch 159 loss is 0.0712898657302396
Epoch 169 loss is 0.08380639698034706
Epoch 179 loss is 0.07950043128200916
Epoch 189 loss is 0.08736764655981065
Epoch 199 loss is 0.09124728414202767
Train Acc.:  0.9411199179609887
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      1063
           1       0.93      0.91      0.92      1064
           2       0.99      0.98      0.98      1064
           3       0.97      0.97      0.97      1064
           4       0.94      0.92      0.93      1064
           5       0.93      0.94      0.93      1063
           6       0.90      0.91      0.91      1064
           7       1.00      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.96      0.93      0.94      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.96      0.95      0.96     11702
   macro avg       0.96      0.95      0.96     11702
weighted avg       0.96      0.95      0.96     11702
 samples avg       0.94      0.95      0.95     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2861673749427943
Epoch 19 loss is 0.2201654650800628
Epoch 29 loss is 0.1594589809702913
Epoch 39 loss is 0.13540895566103234
Epoch 49 loss is 0.11925421445742292
Epoch 59 loss is 0.11962160650088925
Epoch 69 loss is 0.11841529734011198
Epoch 79 loss is 0.1179027595753925
Epoch 89 loss is 0.1180392008192653
Epoch 99 loss is 0.11441886133599923
Epoch 109 loss is 0.10782047828315887
Epoch 119 loss is 0.10705718301534183
Epoch 129 loss is 0.10427072867703205
Epoch 139 loss is 0.10474216368094652
Epoch 149 loss is 0.10551091888301718
Epoch 159 loss is 0.10684330984217699
Epoch 169 loss is 0.09317834493277641
Epoch 179 loss is 0.0892558164282839
Epoch 189 loss is 0.08739703451707423
Epoch 199 loss is 0.09199028421820245
Train Acc.:  0.9275749353729144
              precision    recall  f1-score   support

           0       0.97      0.96      0.96      1063
           1       0.92      0.88      0.90      1064
           2       0.98      0.98      0.98      1064
           3       0.97      0.95      0.96      1063
           4       0.94      0.92      0.93      1064
           5       0.93      0.95      0.94      1064
           6       0.89      0.89      0.89      1064
           7       0.99      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.95      0.93      0.94      1064
          10       0.96      0.91      0.93      1064

   micro avg       0.96      0.94      0.95     11702
   macro avg       0.96      0.94      0.95     11702
weighted avg       0.96      0.94      0.95     11702
 samples avg       0.93      0.94      0.93     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.27623336473650534
Epoch 19 loss is 0.16289381252737087
Epoch 29 loss is 0.14757027886946592
Epoch 39 loss is 0.1330734318315473
Epoch 49 loss is 0.11679790998549922
Epoch 59 loss is 0.11167467752516336
Epoch 69 loss is 0.11268039833791481
Epoch 79 loss is 0.10041605954925376
Epoch 89 loss is 0.10264455966541974
Epoch 99 loss is 0.10157209891611255
Epoch 109 loss is 0.09768154280892237
Epoch 119 loss is 0.09983357506318141
Epoch 129 loss is 0.09887546788082555
Epoch 139 loss is 0.0888411840117014
Epoch 149 loss is 0.10038184391363476
Epoch 159 loss is 0.08949124574344279
Epoch 169 loss is 0.09784304006405145
Epoch 179 loss is 0.09877557601509657
Epoch 189 loss is 0.10217435454327402
Epoch 199 loss is 0.1057261382783362
Train Acc.:  0.9322750870596278
              precision    recall  f1-score   support

           0       0.97      0.95      0.96      1064
           1       0.91      0.91      0.91      1064
           2       0.98      0.96      0.97      1064
           3       0.97      0.96      0.96      1063
           4       0.91      0.91      0.91      1064
           5       0.93      0.93      0.93      1064
           6       0.91      0.88      0.89      1063
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.95      0.94      0.94      1064
          10       0.98      0.95      0.97      1064

   micro avg       0.95      0.94      0.95     11702
   macro avg       0.95      0.94      0.95     11702
weighted avg       0.95      0.94      0.95     11702
 samples avg       0.93      0.94      0.93     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.28693840838047774
Epoch 19 loss is 0.20104936966743658
Epoch 29 loss is 0.15755177961335037
Epoch 39 loss is 0.1288596346987444
Epoch 49 loss is 0.12720331433512763
Epoch 59 loss is 0.12023218874962663
Epoch 69 loss is 0.11721781830240018
Epoch 79 loss is 0.10656089263296499
Epoch 89 loss is 0.09562316810246828
Epoch 99 loss is 0.09102231637919944
Epoch 109 loss is 0.09233467007446912
Epoch 119 loss is 0.09115643363442122
Epoch 129 loss is 0.08620270800916764
Epoch 139 loss is 0.11083394580512573
Epoch 149 loss is 0.10150878655964381
Epoch 159 loss is 0.10271093352938551
Epoch 169 loss is 0.10444855459263125
Epoch 179 loss is 0.09594796640621853
Epoch 189 loss is 0.09849182497560287
Epoch 199 loss is 0.1036780848914241
Train Acc.:  0.924904394641827
              precision    recall  f1-score   support

           0       0.98      0.95      0.97      1063
           1       0.91      0.88      0.89      1064
           2       0.98      0.96      0.97      1064
           3       0.96      0.97      0.97      1063
           4       0.91      0.90      0.91      1064
           5       0.91      0.91      0.91      1064
           6       0.89      0.89      0.89      1064
           7       1.00      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.94      0.93      0.93      1064
          10       0.97      0.95      0.96      1064

   micro avg       0.95      0.94      0.95     11702
   macro avg       0.95      0.94      0.95     11702
weighted avg       0.95      0.94      0.95     11702
 samples avg       0.93      0.94      0.93     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3034027193057795
Epoch 19 loss is 0.17692890093552524
Epoch 29 loss is 0.13786514238212272
Epoch 39 loss is 0.12331394422598127
Epoch 49 loss is 0.1237987767889628
Epoch 59 loss is 0.11048595570606672
Epoch 69 loss is 0.09570147391418123
Epoch 79 loss is 0.09912258426639196
Epoch 89 loss is 0.09599648621108799
Epoch 99 loss is 0.0873050642910436
Epoch 109 loss is 0.10142020273437256
Epoch 119 loss is 0.1003337232560557
Epoch 129 loss is 0.10045847144167147
Epoch 139 loss is 0.09709811021654463
Epoch 149 loss is 0.0970311573500608
Epoch 159 loss is 0.09226866990186595
Epoch 169 loss is 0.08758580671534028
Epoch 179 loss is 0.08792805539701785
Epoch 189 loss is 0.08792941901672958
Epoch 199 loss is 0.09815684767604878
Train Acc.:  0.9300959258230607
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      1064
           1       0.91      0.90      0.91      1064
           2       0.98      0.97      0.97      1064
           3       0.98      0.95      0.97      1064
           4       0.94      0.93      0.93      1064
           5       0.93      0.93      0.93      1064
           6       0.90      0.88      0.89      1063
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.93      0.89      0.91      1064
          10       0.97      0.95      0.96      1064

   micro avg       0.96      0.94      0.95     11702
   macro avg       0.96      0.94      0.95     11702
weighted avg       0.96      0.94      0.95     11702
 samples avg       0.93      0.94      0.93     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-100-100-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-100-100-11">MLMVN [48-100-100-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-100-100-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">100</span>)</span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb125-8"><a href="#cb125-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb125-9"><a href="#cb125-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb125-10"><a href="#cb125-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">100</span>, <span class="dv">11</span>)</span>
<span id="cb125-11"><a href="#cb125-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb125-12"><a href="#cb125-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb125-13"><a href="#cb125-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb125-14"><a href="#cb125-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb125-15"><a href="#cb125-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb125-16"><a href="#cb125-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb125-17"><a href="#cb125-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb125-18"><a href="#cb125-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb125-19"><a href="#cb125-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb125-20"><a href="#cb125-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb125-21"><a href="#cb125-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb125-22"><a href="#cb125-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-23"><a href="#cb125-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb125-24"><a href="#cb125-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb125-25"><a href="#cb125-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb125-26"><a href="#cb125-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb125-27"><a href="#cb125-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb125-28"><a href="#cb125-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb125-29"><a href="#cb125-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb125-30"><a href="#cb125-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb125-31"><a href="#cb125-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-32"><a href="#cb125-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb125-33"><a href="#cb125-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb125-34"><a href="#cb125-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-35"><a href="#cb125-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb125-36"><a href="#cb125-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb125-37"><a href="#cb125-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-38"><a href="#cb125-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb125-39"><a href="#cb125-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb125-40"><a href="#cb125-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-41"><a href="#cb125-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb125-42"><a href="#cb125-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb125-43"><a href="#cb125-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb125-44"><a href="#cb125-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-45"><a href="#cb125-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb125-46"><a href="#cb125-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb125-47"><a href="#cb125-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb125-48"><a href="#cb125-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-49"><a href="#cb125-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb125-50"><a href="#cb125-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb125-51"><a href="#cb125-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb125-52"><a href="#cb125-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-53"><a href="#cb125-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb125-54"><a href="#cb125-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb125-55"><a href="#cb125-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb125-56"><a href="#cb125-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-57"><a href="#cb125-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb125-58"><a href="#cb125-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb125-59"><a href="#cb125-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb125-60"><a href="#cb125-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb125-61"><a href="#cb125-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb125-62"><a href="#cb125-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-63"><a href="#cb125-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb125-64"><a href="#cb125-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb125-65"><a href="#cb125-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb125-66"><a href="#cb125-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-67"><a href="#cb125-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-68"><a href="#cb125-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb125-69"><a href="#cb125-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb125-70"><a href="#cb125-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb125-71"><a href="#cb125-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb125-72"><a href="#cb125-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb125-73"><a href="#cb125-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-74"><a href="#cb125-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb125-75"><a href="#cb125-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb125-76"><a href="#cb125-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb125-77"><a href="#cb125-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb125-78"><a href="#cb125-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-79"><a href="#cb125-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb125-80"><a href="#cb125-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-81"><a href="#cb125-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb125-82"><a href="#cb125-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb125-83"><a href="#cb125-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb125-84"><a href="#cb125-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb125-85"><a href="#cb125-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb125-86"><a href="#cb125-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-87"><a href="#cb125-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb125-88"><a href="#cb125-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb125-89"><a href="#cb125-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb125-90"><a href="#cb125-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-91"><a href="#cb125-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb125-92"><a href="#cb125-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb125-93"><a href="#cb125-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb125-94"><a href="#cb125-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-95"><a href="#cb125-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb125-96"><a href="#cb125-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb125-97"><a href="#cb125-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb125-98"><a href="#cb125-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb125-99"><a href="#cb125-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb125-100"><a href="#cb125-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-101"><a href="#cb125-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb125-102"><a href="#cb125-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb125-103"><a href="#cb125-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb125-104"><a href="#cb125-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-100-100-11]"</span>,</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>, <span class="st">"adjusted_loss"</span>],</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  capture a dictionary of hyperparameters with config</span></span>
<span id="cb126-9"><a href="#cb126-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb126-10"><a href="#cb126-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb126-11"><a href="#cb126-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb126-12"><a href="#cb126-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb126-13"><a href="#cb126-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb126-14"><a href="#cb126-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb126-15"><a href="#cb126-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb126-16"><a href="#cb126-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-100-100-11]"</span>,</span>
<span id="cb126-17"><a href="#cb126-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"loss"</span>: <span class="st">"ComplexMSE_adjusted_error"</span>,</span>
<span id="cb126-18"><a href="#cb126-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb126-19"><a href="#cb126-19" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=f1a6a6d4401f4f458770c1073780e682
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/f1a6a6d4401f4f458770c1073780e682/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-100-100-11]',
 'loss': 'ComplexMSE_adjusted_error'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb129-4"><a href="#cb129-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb129-5"><a href="#cb129-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb129-6"><a href="#cb129-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb129-7"><a href="#cb129-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb129-8"><a href="#cb129-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb129-9"><a href="#cb129-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-10"><a href="#cb129-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb129-11"><a href="#cb129-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb129-12"><a href="#cb129-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb129-13"><a href="#cb129-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb129-14"><a href="#cb129-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb129-15"><a href="#cb129-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-16"><a href="#cb129-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb129-17"><a href="#cb129-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSE_adjusted_error.<span class="bu">apply</span></span>
<span id="cb129-18"><a href="#cb129-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb129-19"><a href="#cb129-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-20"><a href="#cb129-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb129-21"><a href="#cb129-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb129-22"><a href="#cb129-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb129-23"><a href="#cb129-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb129-24"><a href="#cb129-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb129-25"><a href="#cb129-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb129-26"><a href="#cb129-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb129-27"><a href="#cb129-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb129-28"><a href="#cb129-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb129-29"><a href="#cb129-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb129-30"><a href="#cb129-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb129-31"><a href="#cb129-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-32"><a href="#cb129-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb129-33"><a href="#cb129-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-34"><a href="#cb129-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb129-35"><a href="#cb129-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb129-36"><a href="#cb129-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-37"><a href="#cb129-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb129-38"><a href="#cb129-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb129-39"><a href="#cb129-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb129-40"><a href="#cb129-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-41"><a href="#cb129-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb129-42"><a href="#cb129-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb129-43"><a href="#cb129-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb129-44"><a href="#cb129-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-45"><a href="#cb129-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb129-46"><a href="#cb129-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb129-47"><a href="#cb129-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb129-48"><a href="#cb129-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb129-49"><a href="#cb129-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb129-50"><a href="#cb129-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb129-51"><a href="#cb129-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb129-52"><a href="#cb129-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb129-53"><a href="#cb129-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb129-54"><a href="#cb129-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb129-55"><a href="#cb129-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb129-56"><a href="#cb129-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb129-57"><a href="#cb129-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb129-58"><a href="#cb129-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb129-59"><a href="#cb129-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-60"><a href="#cb129-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-61"><a href="#cb129-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb129-62"><a href="#cb129-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb129-63"><a href="#cb129-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-64"><a href="#cb129-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-65"><a href="#cb129-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb129-66"><a href="#cb129-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb129-67"><a href="#cb129-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-68"><a href="#cb129-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-69"><a href="#cb129-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb129-70"><a href="#cb129-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb129-71"><a href="#cb129-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-72"><a href="#cb129-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-73"><a href="#cb129-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb129-74"><a href="#cb129-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb129-75"><a href="#cb129-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-76"><a href="#cb129-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-77"><a href="#cb129-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb129-78"><a href="#cb129-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb129-79"><a href="#cb129-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-80"><a href="#cb129-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-81"><a href="#cb129-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb129-82"><a href="#cb129-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb129-83"><a href="#cb129-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-84"><a href="#cb129-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-85"><a href="#cb129-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb129-86"><a href="#cb129-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb129-87"><a href="#cb129-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb129-88"><a href="#cb129-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb129-89"><a href="#cb129-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb129-90"><a href="#cb129-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb129-91"><a href="#cb129-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-28 00:04:38,312 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.
Epoch 9 loss is 0.38493101436237465
Epoch 19 loss is 0.1983475963903122
Epoch 29 loss is 0.15240658259145864
Epoch 39 loss is 0.12849809119734149
Epoch 49 loss is 0.11291338514495056
Epoch 59 loss is 0.10350603182093213
Epoch 69 loss is 0.09896006818913682
Epoch 79 loss is 0.09073580641413742
Epoch 89 loss is 0.08624265189956253
Epoch 99 loss is 0.07990884906610474
Epoch 109 loss is 0.07774202154480073
Epoch 119 loss is 0.0778207451859103
Epoch 129 loss is 0.07292320971236593
Epoch 139 loss is 0.08220096647261978
Epoch 149 loss is 0.07259278084390995
Epoch 159 loss is 0.07333743296400583
Epoch 169 loss is 0.06768991486550634
Epoch 179 loss is 0.06486413256485078
Epoch 189 loss is 0.0663716789708822
Epoch 199 loss is 0.06601747100440022
Train Acc.:  0.9551562800435832
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1063
           1       0.93      0.92      0.93      1064
           2       0.99      0.98      0.98      1064
           3       0.99      0.98      0.98      1064
           4       0.95      0.92      0.93      1064
           5       0.94      0.94      0.94      1063
           6       0.92      0.91      0.92      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.94      0.92      0.93      1064
          10       0.96      0.96      0.96      1064

   micro avg       0.96      0.95      0.96     11702
   macro avg       0.96      0.95      0.96     11702
weighted avg       0.96      0.95      0.96     11702
 samples avg       0.94      0.95      0.95     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.4015647773116825
Epoch 19 loss is 0.21324193953061343
Epoch 29 loss is 0.16244134629623966
Epoch 39 loss is 0.13968662879021496
Epoch 49 loss is 0.1220346592068794
Epoch 59 loss is 0.11857350072574051
Epoch 69 loss is 0.10683356304935711
Epoch 79 loss is 0.09876512760527531
Epoch 89 loss is 0.10028214707730271
Epoch 99 loss is 0.09386611501220359
Epoch 109 loss is 0.09131180958378789
Epoch 119 loss is 0.08078602213530815
Epoch 129 loss is 0.07812674516622654
Epoch 139 loss is 0.07368491314841398
Epoch 149 loss is 0.07233171681695082
Epoch 159 loss is 0.07153585909897314
Epoch 169 loss is 0.07045204344853243
Epoch 179 loss is 0.07031483192734578
Epoch 189 loss is 0.06762652182875349
Epoch 199 loss is 0.06935222633418742
Train Acc.:  0.9489606255474609
              precision    recall  f1-score   support

           0       0.98      0.97      0.97      1063
           1       0.94      0.90      0.92      1064
           2       0.98      0.97      0.97      1064
           3       0.98      0.97      0.97      1063
           4       0.95      0.94      0.94      1064
           5       0.94      0.93      0.93      1064
           6       0.91      0.89      0.90      1064
           7       1.00      1.00      1.00      1064
           8       1.00      0.99      1.00      1064
           9       0.96      0.94      0.95      1064
          10       0.98      0.95      0.97      1064

   micro avg       0.96      0.95      0.96     11702
   macro avg       0.96      0.95      0.96     11702
weighted avg       0.96      0.95      0.96     11702
 samples avg       0.94      0.95      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.35830112541567166
Epoch 19 loss is 0.20503370425712947
Epoch 29 loss is 0.1615004405052233
Epoch 39 loss is 0.14478501770353844
Epoch 49 loss is 0.13139543735043546
Epoch 59 loss is 0.12132454251093122
Epoch 69 loss is 0.11744575716216751
Epoch 79 loss is 0.11623840924937114
Epoch 89 loss is 0.11783017747510419
Epoch 99 loss is 0.11593023945022461
Epoch 109 loss is 0.11583587087231882
Epoch 119 loss is 0.11662312303478367
Epoch 129 loss is 0.11691969687131881
Epoch 139 loss is 0.10854572296958728
Epoch 149 loss is 0.10796596240824434
Epoch 159 loss is 0.11262345830304454
Epoch 169 loss is 0.10184479776708627
Epoch 179 loss is 0.09605088202087457
Epoch 189 loss is 0.09299509987805853
Epoch 199 loss is 0.08898415836437445
Train Acc.:  0.9210374516632127
              precision    recall  f1-score   support

           0       0.97      0.95      0.96      1064
           1       0.91      0.89      0.90      1064
           2       0.98      0.95      0.97      1064
           3       0.96      0.94      0.95      1063
           4       0.92      0.88      0.90      1064
           5       0.90      0.90      0.90      1064
           6       0.89      0.88      0.89      1063
           7       1.00      0.99      0.99      1064
           8       1.00      0.99      1.00      1064
           9       0.93      0.88      0.90      1064
          10       0.97      0.95      0.96      1064

   micro avg       0.95      0.93      0.94     11702
   macro avg       0.95      0.93      0.94     11702
weighted avg       0.95      0.93      0.94     11702
 samples avg       0.91      0.93      0.92     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3732331507452995
Epoch 19 loss is 0.2151142782886408
Epoch 29 loss is 0.1693622907950432
Epoch 39 loss is 0.14709019657419897
Epoch 49 loss is 0.13360652890401162
Epoch 59 loss is 0.12738859159159088
Epoch 69 loss is 0.11634673967810212
Epoch 79 loss is 0.10559853905524143
Epoch 89 loss is 0.10185721106569144
Epoch 99 loss is 0.10076346263612189
Epoch 109 loss is 0.0951012106512096
Epoch 119 loss is 0.0863301443776767
Epoch 129 loss is 0.08374395534767522
Epoch 139 loss is 0.08192660695129174
Epoch 149 loss is 0.0808713694226275
Epoch 159 loss is 0.08485596063007796
Epoch 169 loss is 0.07577642509867422
Epoch 179 loss is 0.07399703079870644
Epoch 189 loss is 0.07316694645295278
Epoch 199 loss is 0.07004600393121947
Train Acc.:  0.9438972803213195
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1063
           1       0.93      0.91      0.92      1064
           2       0.98      0.96      0.97      1064
           3       0.97      0.97      0.97      1063
           4       0.94      0.92      0.93      1064
           5       0.93      0.92      0.93      1064
           6       0.90      0.89      0.89      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.93      0.90      0.92      1064
          10       0.96      0.95      0.96      1064

   micro avg       0.96      0.95      0.95     11702
   macro avg       0.96      0.95      0.95     11702
weighted avg       0.96      0.95      0.95     11702
 samples avg       0.93      0.95      0.94     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_25607/161459083.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3860148494765397
Epoch 19 loss is 0.20866529165214284
Epoch 29 loss is 0.16247025892118347
Epoch 39 loss is 0.13564982800723918
Epoch 49 loss is 0.1240512973257284
Epoch 59 loss is 0.113934101224775
Epoch 69 loss is 0.11061596169703837
Epoch 79 loss is 0.10461476570112285
Epoch 89 loss is 0.10298748909019034
Epoch 99 loss is 0.09786684313663066
Epoch 109 loss is 0.0988537740332937
Epoch 119 loss is 0.09090053269267047
Epoch 129 loss is 0.08742530763329877
Epoch 139 loss is 0.0841747421040788
Epoch 149 loss is 0.08610848018045644
Epoch 159 loss is 0.07819941018533352
Epoch 169 loss is 0.07596165720615952
Epoch 179 loss is 0.07426260096075489
Epoch 189 loss is 0.07633927124679218
Epoch 199 loss is 0.07405352525764859
Train Acc.:  0.9421454056017262
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1064
           1       0.91      0.90      0.91      1064
           2       0.97      0.96      0.96      1064
           3       0.97      0.96      0.97      1064
           4       0.94      0.92      0.93      1064
           5       0.93      0.94      0.94      1064
           6       0.91      0.88      0.90      1063
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.93      0.90      0.92      1064
          10       0.98      0.95      0.97      1064

   micro avg       0.96      0.94      0.95     11702
   macro avg       0.96      0.94      0.95     11702
weighted avg       0.96      0.94      0.95     11702
 samples avg       0.93      0.94      0.94     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>