<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="In this example, the main focus is the classification of individual states of a motor.">

<title>mlmvn - Sensorless Drive Diagnosis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="mlmvn - Sensorless Drive Diagnosis">
<meta property="og:description" content="In this example, the main focus is the classification of individual states of a motor.">
<meta property="og:site-name" content="mlmvn">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">mlmvn</span>
    </a>
  </div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Sensorless Drive Diagnosis</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">MLMVN</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../layers.html" class="sidebar-item-text sidebar-link">Layers</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../loss.html" class="sidebar-item-text sidebar-link">Loss</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../optim.html" class="sidebar-item-text sidebar-link">Optimizer</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Examples</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../examples/xor/xor.html" class="sidebar-item-text sidebar-link">XOR</a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">Moons</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../examples/moons/moons.html" class="sidebar-item-text sidebar-link">Building a Binary Classifier</a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">SDD</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  examples/autass/00_autass.ipynb
  </li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#load-data" id="toc-load-data" class="nav-link active" data-scroll-target="#load-data">Load Data</a></li>
  <li><a href="#config" id="toc-config" class="nav-link" data-scroll-target="#config">Config</a></li>
  <li><a href="#single-layer" id="toc-single-layer" class="nav-link" data-scroll-target="#single-layer">Single Layer</a>
  <ul>
  <li><a href="#mlmvn-48-10-11" id="toc-mlmvn-48-10-11" class="nav-link" data-scroll-target="#mlmvn-48-10-11">MLMVN [48-10-11]</a></li>
  <li><a href="#mlmvn-48-20-11" id="toc-mlmvn-48-20-11" class="nav-link" data-scroll-target="#mlmvn-48-20-11">MLMVN [48-20-11]</a></li>
  <li><a href="#mlmvn-48-50-11" id="toc-mlmvn-48-50-11" class="nav-link" data-scroll-target="#mlmvn-48-50-11">MLMVN [48-50-11]</a></li>
  <li><a href="#mlmvn-48-100-11" id="toc-mlmvn-48-100-11" class="nav-link" data-scroll-target="#mlmvn-48-100-11">MLMVN [48-100-11]</a></li>
  </ul></li>
  <li><a href="#multi-layer" id="toc-multi-layer" class="nav-link" data-scroll-target="#multi-layer">Multi Layer</a>
  <ul>
  <li><a href="#mlmvn-48-10-10-11" id="toc-mlmvn-48-10-10-11" class="nav-link" data-scroll-target="#mlmvn-48-10-10-11">MLMVN [48-10-10-11]</a></li>
  <li><a href="#mlmvn-48-20-20-11" id="toc-mlmvn-48-20-20-11" class="nav-link" data-scroll-target="#mlmvn-48-20-20-11">MLMVN [48-20-20-11]</a></li>
  <li><a href="#mlmvn-48-50-50-11" id="toc-mlmvn-48-50-50-11" class="nav-link" data-scroll-target="#mlmvn-48-50-50-11">MLMVN [48-50-50-11]</a></li>
  <li><a href="#mlmvn-48-100-100-11" id="toc-mlmvn-48-100-100-11" class="nav-link" data-scroll-target="#mlmvn-48-100-100-11">MLMVN [48-100-100-11]</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/antonpf/mlmvn/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Sensorless Drive Diagnosis</h1>
</div>

<div>
  <div class="description">
    In this example, the main focus is the classification of individual states of a motor.
  </div>
</div>


<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="load-data" class="level2">
<h2 class="anchored" data-anchor-id="load-data">Load Data</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>train_csv <span class="op">=</span> pd.read_csv(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"data/autass_data2.csv"</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    header<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span>np.double,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(train_csv.values[:, <span class="dv">1</span>:<span class="dv">50</span>])</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> train_csv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[:, <span class="dv">0</span>:<span class="dv">48</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[:, <span class="dv">48</span>].astype(<span class="bu">int</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>yt <span class="op">=</span> copy.copy(y)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">1</span>] <span class="op">=</span> <span class="dv">21</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">2</span>] <span class="op">=</span> <span class="dv">22</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">3</span>] <span class="op">=</span> <span class="dv">23</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">4</span>] <span class="op">=</span> <span class="dv">26</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">5</span>] <span class="op">=</span> <span class="dv">24</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">6</span>] <span class="op">=</span> <span class="dv">27</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">7</span>] <span class="op">=</span> <span class="dv">29</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">8</span>] <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">9</span>] <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>yt[yt <span class="op">==</span> <span class="dv">10</span>] <span class="op">=</span> <span class="dv">28</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>yt <span class="op">-=</span> <span class="dv">20</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> yt</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> yt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="config" class="level2">
<h2 class="anchored" data-anchor-id="config">Config</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">538</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="single-layer" class="level2">
<h2 class="anchored" data-anchor-id="single-layer">Single Layer</h2>
<section id="mlmvn-48-10-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-10-11">MLMVN [48-10-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-10-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">10</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-10-11]"</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-10-11]"</span>,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=4c6da71cfc4c498f8da7e410e130c443
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/4c6da71cfc4c498f8da7e410e130c443/output/log
======&gt; WARNING! Git diff to large to store (3944kb), skipping uncommitted changes &lt;======</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-10-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb9-91"><a href="#cb9-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2798053516728018
Epoch 19 loss is 0.28693026311742914
Epoch 29 loss is 0.2874780130965781
Epoch 39 loss is 0.2984541579734532
Epoch 49 loss is 0.3193280469886697
Epoch 59 loss is 0.33707626521445716
Epoch 69 loss is 0.30040521139203175
Epoch 79 loss is 0.28762346204797534
Epoch 89 loss is 0.31152799377307844
Epoch 99 loss is 0.2576630936111671
Epoch 109 loss is 0.291178549427303
Epoch 119 loss is 0.2490802764713475
Epoch 129 loss is 0.23373190527990692
Epoch 139 loss is 0.28033977577880903
Epoch 149 loss is 0.2837003647392515
Epoch 159 loss is 0.2538141131717079
Epoch 169 loss is 0.3233458041492201
Epoch 179 loss is 0.34501202353519583
Epoch 189 loss is 0.34266169941405217
Epoch 199 loss is 0.3550294292895104
Train Acc.:  0.8201337406798128
              precision    recall  f1-score   support

           0       0.93      0.93      0.93      1063
           1       0.85      0.74      0.79      1064
           2       0.96      0.93      0.94      1064
           3       0.86      0.93      0.90      1064
           4       0.84      0.83      0.83      1064
           5       0.90      0.75      0.82      1063
           6       0.77      0.71      0.74      1064
           7       0.99      0.97      0.98      1064
           8       0.98      1.00      0.99      1064
           9       0.83      0.74      0.78      1064
          10       0.92      0.83      0.87      1064

   micro avg       0.90      0.85      0.87     11702
   macro avg       0.89      0.85      0.87     11702
weighted avg       0.89      0.85      0.87     11702
 samples avg       0.83      0.85      0.84     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3103850582404919
Epoch 19 loss is 0.2418774839044744
Epoch 29 loss is 0.265908605107625
Epoch 39 loss is 0.2943059382354039
Epoch 49 loss is 0.276436161067747
Epoch 59 loss is 0.32590297383744593
Epoch 69 loss is 0.2767146737255772
Epoch 79 loss is 0.287858186740984
Epoch 89 loss is 0.35907828087177
Epoch 99 loss is 0.4186029191773882
Epoch 109 loss is 0.33285225361643794
Epoch 119 loss is 0.3228767984459741
Epoch 129 loss is 0.35609662567173445
Epoch 139 loss is 0.2398215116127861
Epoch 149 loss is 0.37533755882336206
Epoch 159 loss is 0.3230767278001795
Epoch 169 loss is 0.32038495509949566
Epoch 179 loss is 0.2728121593992897
Epoch 189 loss is 0.23113384043537294
Epoch 199 loss is 0.23697167827133853
Train Acc.:  0.817505928600423
              precision    recall  f1-score   support

           0       0.94      0.89      0.91      1063
           1       0.85      0.72      0.78      1064
           2       0.96      0.92      0.94      1064
           3       0.92      0.91      0.92      1063
           4       0.79      0.69      0.74      1064
           5       0.84      0.91      0.87      1064
           6       0.70      0.70      0.70      1064
           7       0.98      0.99      0.98      1064
           8       1.00      0.99      0.99      1064
           9       0.85      0.91      0.88      1064
          10       0.93      0.84      0.88      1064

   micro avg       0.89      0.86      0.87     11702
   macro avg       0.89      0.86      0.87     11702
weighted avg       0.89      0.86      0.87     11702
 samples avg       0.84      0.86      0.84     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.234435066899149
Epoch 19 loss is 0.23500500804411487
Epoch 29 loss is 0.2704866989198225
Epoch 39 loss is 0.22928027331865544
Epoch 49 loss is 0.24259319125743486
Epoch 59 loss is 0.23993151432124912
Epoch 69 loss is 0.2355228313899169
Epoch 79 loss is 0.29352466112392755
Epoch 89 loss is 0.34534633221051547
Epoch 99 loss is 0.28059194333336157
Epoch 109 loss is 0.3367117756837938
Epoch 119 loss is 0.35539835849532864
Epoch 129 loss is 0.29370534618128263
Epoch 139 loss is 0.2897315933896303
Epoch 149 loss is 0.23595774051726134
Epoch 159 loss is 0.3217572035914341
Epoch 169 loss is 0.3612773694004577
Epoch 179 loss is 0.29780505445589145
Epoch 189 loss is 0.3884432736077841
Epoch 199 loss is 0.5057752075140738
Train Acc.:  0.82641485247933
              precision    recall  f1-score   support

           0       0.74      0.98      0.84      1064
           1       0.87      0.69      0.77      1064
           2       0.96      0.92      0.94      1064
           3       0.93      0.93      0.93      1063
           4       0.84      0.36      0.50      1064
           5       0.79      0.94      0.86      1064
           6       0.87      0.67      0.76      1063
           7       0.96      0.99      0.97      1064
           8       1.00      1.00      1.00      1064
           9       0.84      0.92      0.88      1064
          10       0.94      0.83      0.88      1064

   micro avg       0.88      0.84      0.86     11702
   macro avg       0.89      0.84      0.85     11702
weighted avg       0.89      0.84      0.85     11702
 samples avg       0.83      0.84      0.83     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.24826095378107285
Epoch 19 loss is 0.22183934458525567
Epoch 29 loss is 0.2376939787540323
Epoch 39 loss is 0.23677890969663506
Epoch 49 loss is 0.19782310665792108
Epoch 59 loss is 0.18985340247155252
Epoch 69 loss is 0.1916335525972637
Epoch 79 loss is 0.1843041258845089
Epoch 89 loss is 0.20625673516565524
Epoch 99 loss is 0.2462787903644629
Epoch 109 loss is 0.3130027202975933
Epoch 119 loss is 0.30493362345149605
Epoch 129 loss is 0.299187534389669
Epoch 139 loss is 0.2979426610025583
Epoch 149 loss is 0.3395869082665093
Epoch 159 loss is 0.3224462941593307
Epoch 169 loss is 0.3119062898221137
Epoch 179 loss is 0.36908871562997664
Epoch 189 loss is 0.36701326872569157
Epoch 199 loss is 0.2913419812224387
Train Acc.:  0.8227829170850514
              precision    recall  f1-score   support

           0       0.92      0.95      0.94      1063
           1       0.84      0.91      0.87      1064
           2       0.95      0.91      0.93      1064
           3       0.97      0.93      0.95      1063
           4       0.85      0.57      0.68      1064
           5       0.91      0.87      0.89      1064
           6       0.95      0.30      0.45      1064
           7       0.99      1.00      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.77      0.98      0.86      1064
          10       0.89      0.83      0.86      1064

   micro avg       0.91      0.84      0.87     11702
   macro avg       0.91      0.84      0.86     11702
weighted avg       0.91      0.84      0.86     11702
 samples avg       0.83      0.84      0.83     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.23893833129461026
Epoch 19 loss is 0.35352674061561473
Epoch 29 loss is 0.31082288381730555
Epoch 39 loss is 0.28511622276983273
Epoch 49 loss is 0.23868927247854704
Epoch 59 loss is 0.2485950896878176
Epoch 69 loss is 0.41070443044607735
Epoch 79 loss is 0.3084329791560098
Epoch 89 loss is 0.30589607100727806
Epoch 99 loss is 0.3070161200848258
Epoch 109 loss is 0.2976518308637186
Epoch 119 loss is 0.2824108441539807
Epoch 129 loss is 0.3116600160978614
Epoch 139 loss is 0.31757349912205246
Epoch 149 loss is 0.30672799590632993
Epoch 159 loss is 0.40314944724027096
Epoch 169 loss is 0.4096136636855462
Epoch 179 loss is 0.4105623334879551
Epoch 189 loss is 0.44534144032836126
Epoch 199 loss is 0.46514225549527893
Train Acc.:  0.82690623197385
              precision    recall  f1-score   support

           0       0.96      0.93      0.94      1064
           1       0.93      0.48      0.64      1064
           2       0.91      0.91      0.91      1064
           3       0.94      0.94      0.94      1064
           4       0.84      0.63      0.72      1064
           5       0.75      0.94      0.84      1064
           6       0.85      0.73      0.78      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.86      0.95      0.90      1064
          10       0.91      0.84      0.87      1064

   micro avg       0.90      0.85      0.87     11702
   macro avg       0.90      0.85      0.87     11702
weighted avg       0.90      0.85      0.87     11702
 samples avg       0.84      0.85      0.84     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"MLMVN 48-10-11"</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> <span class="st">"results/MLMVN_48-10-11.png"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-20-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-20-11">MLMVN [48-20-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-20-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">20</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">20</span>, <span class="dv">11</span>)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-56"><a href="#cb23-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb23-57"><a href="#cb23-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb23-58"><a href="#cb23-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb23-59"><a href="#cb23-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-60"><a href="#cb23-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-61"><a href="#cb23-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb23-62"><a href="#cb23-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb23-63"><a href="#cb23-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb23-64"><a href="#cb23-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb23-65"><a href="#cb23-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-66"><a href="#cb23-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-67"><a href="#cb23-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb23-68"><a href="#cb23-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb23-69"><a href="#cb23-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb23-70"><a href="#cb23-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb23-71"><a href="#cb23-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-72"><a href="#cb23-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb23-73"><a href="#cb23-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-74"><a href="#cb23-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb23-75"><a href="#cb23-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb23-76"><a href="#cb23-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb23-77"><a href="#cb23-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb23-78"><a href="#cb23-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb23-79"><a href="#cb23-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-80"><a href="#cb23-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb23-81"><a href="#cb23-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb23-82"><a href="#cb23-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb23-83"><a href="#cb23-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-84"><a href="#cb23-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb23-85"><a href="#cb23-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb23-86"><a href="#cb23-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb23-87"><a href="#cb23-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-88"><a href="#cb23-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb23-89"><a href="#cb23-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb23-90"><a href="#cb23-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-91"><a href="#cb23-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb23-92"><a href="#cb23-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb23-93"><a href="#cb23-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-94"><a href="#cb23-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb23-95"><a href="#cb23-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb23-96"><a href="#cb23-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb23-97"><a href="#cb23-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-20-11]"</span>,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-20-11]"</span>,</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=e9f13dacf8d142619b65b8316ca90780
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e9f13dacf8d142619b65b8316ca90780/output/log
======&gt; WARNING! Git diff to large to store (3944kb), skipping uncommitted changes &lt;======</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-20-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-20 10:34:58,198 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.
Epoch 9 loss is 0.16476403157413338
Epoch 19 loss is 0.13633803332426933
Epoch 29 loss is 0.143033463264053
Epoch 39 loss is 0.17849334688199742
Epoch 49 loss is 0.21700776681915424
Epoch 59 loss is 0.2034918992282111
Epoch 69 loss is 0.1712665802112372
Epoch 79 loss is 0.20085534461565652
Epoch 89 loss is 0.2502123033688426
Epoch 99 loss is 0.30439372755517824
Epoch 109 loss is 0.3093228314198623
Epoch 119 loss is 0.27491323067701007
Epoch 129 loss is 0.28011414414391217
Epoch 139 loss is 0.3747490415384792
Epoch 149 loss is 0.27613471357266345
Epoch 159 loss is 0.23981115046878665
Epoch 169 loss is 0.2038788837831156
Epoch 179 loss is 0.18847714759522705
Epoch 189 loss is 0.2114042358851922
Epoch 199 loss is 0.20061396634221004
Train Acc.:  0.8892473348003503
              precision    recall  f1-score   support

           0       0.94      0.90      0.92      1063
           1       0.87      0.86      0.86      1064
           2       0.99      0.94      0.96      1064
           3       0.97      0.97      0.97      1064
           4       0.87      0.76      0.81      1064
           5       0.87      0.91      0.89      1063
           6       0.87      0.85      0.86      1064
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.90      0.93      0.91      1064
          10       0.95      0.92      0.94      1064

   micro avg       0.93      0.91      0.92     11702
   macro avg       0.93      0.91      0.92     11702
weighted avg       0.93      0.91      0.92     11702
 samples avg       0.90      0.91      0.90     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.16358843200535636
Epoch 19 loss is 0.2041445672195862
Epoch 29 loss is 0.24642286260837112
Epoch 39 loss is 0.19917514925837854
Epoch 49 loss is 0.25327855214060685
Epoch 59 loss is 0.21884971568181918
Epoch 69 loss is 0.22624078805323974
Epoch 79 loss is 0.26295991330810126
Epoch 89 loss is 0.22422218505461025
Epoch 99 loss is 0.2339717792959955
Epoch 109 loss is 0.19874802368569566
Epoch 119 loss is 0.23704211230309438
Epoch 129 loss is 0.24084612126041485
Epoch 139 loss is 0.20031062393464874
Epoch 149 loss is 0.18938521056672464
Epoch 159 loss is 0.20100549875098014
Epoch 169 loss is 0.20972725948272172
Epoch 179 loss is 0.20063395868672673
Epoch 189 loss is 0.183422525811785
Epoch 199 loss is 0.1669537319356945
Train Acc.:  0.8925374409810498
              precision    recall  f1-score   support

           0       0.95      0.92      0.93      1063
           1       0.90      0.87      0.88      1064
           2       0.99      0.97      0.98      1064
           3       0.98      0.95      0.96      1063
           4       0.89      0.76      0.82      1064
           5       0.86      0.94      0.90      1064
           6       0.89      0.89      0.89      1064
           7       1.00      0.99      1.00      1064
           8       1.00      0.99      1.00      1064
           9       0.91      0.93      0.92      1064
          10       0.96      0.94      0.95      1064

   micro avg       0.94      0.92      0.93     11702
   macro avg       0.94      0.92      0.93     11702
weighted avg       0.94      0.92      0.93     11702
 samples avg       0.91      0.92      0.91     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.19744827084414995
Epoch 19 loss is 0.20472359266774093
Epoch 29 loss is 0.2544472813758418
Epoch 39 loss is 0.19045720603945143
Epoch 49 loss is 0.20343276143970507
Epoch 59 loss is 0.22297812880413134
Epoch 69 loss is 0.22178893715622944
Epoch 79 loss is 0.2498799044945237
Epoch 89 loss is 0.22153046921128713
Epoch 99 loss is 0.23420279826211265
Epoch 109 loss is 0.22040926507985742
Epoch 119 loss is 0.21762960224465708
Epoch 129 loss is 0.2605731742439022
Epoch 139 loss is 0.2775670812423578
Epoch 149 loss is 0.19603989313909037
Epoch 159 loss is 0.20545349137968197
Epoch 169 loss is 0.20606501781890182
Epoch 179 loss is 0.18431993443444303
Epoch 189 loss is 0.1741126129084759
Epoch 199 loss is 0.1724068336751827
Train Acc.:  0.8685025744012648
              precision    recall  f1-score   support

           0       0.88      0.94      0.91      1064
           1       0.92      0.75      0.82      1064
           2       0.96      0.94      0.95      1064
           3       0.91      0.97      0.94      1063
           4       0.83      0.81      0.82      1064
           5       0.83      0.93      0.88      1064
           6       0.86      0.72      0.79      1063
           7       0.99      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.83      0.94      0.88      1064
          10       0.96      0.78      0.86      1064

   micro avg       0.90      0.89      0.90     11702
   macro avg       0.91      0.89      0.89     11702
weighted avg       0.91      0.89      0.89     11702
 samples avg       0.88      0.89      0.88     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.18983226445452947
Epoch 19 loss is 0.20989331423949678
Epoch 29 loss is 0.20457408459425253
Epoch 39 loss is 0.21686740636756055
Epoch 49 loss is 0.1686385065915988
Epoch 59 loss is 0.15638486085080838
Epoch 69 loss is 0.1741827037910428
Epoch 79 loss is 0.1640841394764191
Epoch 89 loss is 0.17007600609779142
Epoch 99 loss is 0.19284076037560408
Epoch 109 loss is 0.19138844241490113
Epoch 119 loss is 0.20478322070718535
Epoch 129 loss is 0.21176829993640264
Epoch 139 loss is 0.22802495808894305
Epoch 149 loss is 0.2164955049311736
Epoch 159 loss is 0.2058771283183833
Epoch 169 loss is 0.17439026342796043
Epoch 179 loss is 0.19134460117948338
Epoch 189 loss is 0.17625964681553905
Epoch 199 loss is 0.21366730862193598
Train Acc.:  0.8870895378896319
              precision    recall  f1-score   support

           0       0.90      0.94      0.92      1063
           1       0.87      0.89      0.88      1064
           2       0.93      0.97      0.95      1064
           3       0.97      0.98      0.98      1063
           4       0.90      0.74      0.81      1064
           5       0.94      0.86      0.89      1064
           6       0.86      0.76      0.81      1064
           7       1.00      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.83      0.92      0.88      1064
          10       0.93      0.90      0.91      1064

   micro avg       0.92      0.90      0.91     11702
   macro avg       0.92      0.90      0.91     11702
weighted avg       0.92      0.90      0.91     11702
 samples avg       0.89      0.90      0.90     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.17402481484483856
Epoch 19 loss is 0.1944497671217645
Epoch 29 loss is 0.14842624541281338
Epoch 39 loss is 0.15502097794515227
Epoch 49 loss is 0.15759596208770407
Epoch 59 loss is 0.19170838004354154
Epoch 69 loss is 0.1871364349827201
Epoch 79 loss is 0.18722396261041316
Epoch 89 loss is 0.1876739674177174
Epoch 99 loss is 0.2193864859799957
Epoch 109 loss is 0.19671043294002857
Epoch 119 loss is 0.22702499457486658
Epoch 129 loss is 0.21129339398637728
Epoch 139 loss is 0.2182856933389895
Epoch 149 loss is 0.21264683162737122
Epoch 159 loss is 0.20315929596125387
Epoch 169 loss is 0.18449427099611365
Epoch 179 loss is 0.1783611846088383
Epoch 189 loss is 0.18641298905492973
Epoch 199 loss is 0.17783021613349564
Train Acc.:  0.8899737218792061
              precision    recall  f1-score   support

           0       0.91      0.95      0.93      1064
           1       0.88      0.80      0.84      1064
           2       0.96      0.97      0.97      1064
           3       0.96      0.98      0.97      1064
           4       0.93      0.77      0.84      1064
           5       0.85      0.92      0.88      1064
           6       0.90      0.74      0.81      1063
           7       1.00      0.99      0.99      1064
           8       1.00      1.00      1.00      1063
           9       0.88      0.92      0.90      1064
          10       0.95      0.91      0.93      1064

   micro avg       0.93      0.90      0.92     11702
   macro avg       0.93      0.90      0.91     11702
weighted avg       0.93      0.90      0.91     11702
 samples avg       0.90      0.90      0.90     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"MLMVN 48-20-11"</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> <span class="st">"results/MLMVN_48-20-11.png"</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-50-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-50-11">MLMVN [48-50-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-50-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">50</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">50</span>, <span class="dv">11</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb41-54"><a href="#cb41-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb41-55"><a href="#cb41-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-56"><a href="#cb41-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb41-57"><a href="#cb41-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb41-58"><a href="#cb41-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb41-59"><a href="#cb41-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-60"><a href="#cb41-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-61"><a href="#cb41-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb41-62"><a href="#cb41-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb41-63"><a href="#cb41-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb41-64"><a href="#cb41-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb41-65"><a href="#cb41-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb41-66"><a href="#cb41-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-67"><a href="#cb41-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb41-68"><a href="#cb41-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb41-69"><a href="#cb41-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb41-70"><a href="#cb41-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb41-71"><a href="#cb41-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-72"><a href="#cb41-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb41-73"><a href="#cb41-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-74"><a href="#cb41-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb41-75"><a href="#cb41-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb41-76"><a href="#cb41-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb41-77"><a href="#cb41-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb41-78"><a href="#cb41-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb41-79"><a href="#cb41-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-80"><a href="#cb41-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb41-81"><a href="#cb41-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb41-82"><a href="#cb41-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb41-83"><a href="#cb41-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-84"><a href="#cb41-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb41-85"><a href="#cb41-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb41-86"><a href="#cb41-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb41-87"><a href="#cb41-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-88"><a href="#cb41-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb41-89"><a href="#cb41-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb41-90"><a href="#cb41-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb41-91"><a href="#cb41-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb41-92"><a href="#cb41-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb41-93"><a href="#cb41-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-94"><a href="#cb41-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb41-95"><a href="#cb41-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb41-96"><a href="#cb41-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb41-97"><a href="#cb41-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-50-11]"</span>,</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-50-11]"</span>,</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=3560d2ef7d5a41718f94259373e7966c
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/3560d2ef7d5a41718f94259373e7966c/output/log
======&gt; WARNING! Git diff to large to store (3944kb), skipping uncommitted changes &lt;======</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-50-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb45-47"><a href="#cb45-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb45-48"><a href="#cb45-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-49"><a href="#cb45-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb45-50"><a href="#cb45-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb45-51"><a href="#cb45-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb45-52"><a href="#cb45-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb45-53"><a href="#cb45-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-54"><a href="#cb45-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb45-55"><a href="#cb45-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb45-56"><a href="#cb45-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb45-57"><a href="#cb45-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb45-58"><a href="#cb45-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-59"><a href="#cb45-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-60"><a href="#cb45-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-61"><a href="#cb45-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb45-62"><a href="#cb45-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb45-63"><a href="#cb45-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-64"><a href="#cb45-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-65"><a href="#cb45-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb45-66"><a href="#cb45-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb45-67"><a href="#cb45-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-68"><a href="#cb45-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-69"><a href="#cb45-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb45-70"><a href="#cb45-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb45-71"><a href="#cb45-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-72"><a href="#cb45-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-73"><a href="#cb45-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb45-74"><a href="#cb45-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb45-75"><a href="#cb45-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-76"><a href="#cb45-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-77"><a href="#cb45-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb45-78"><a href="#cb45-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb45-79"><a href="#cb45-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-80"><a href="#cb45-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-81"><a href="#cb45-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb45-82"><a href="#cb45-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb45-83"><a href="#cb45-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-84"><a href="#cb45-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-85"><a href="#cb45-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb45-86"><a href="#cb45-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb45-87"><a href="#cb45-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-88"><a href="#cb45-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb45-89"><a href="#cb45-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb45-90"><a href="#cb45-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb45-91"><a href="#cb45-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-20 10:50:33,334 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.
Epoch 9 loss is 0.09829269534206735
Epoch 19 loss is 0.09728068709222959
Epoch 29 loss is 0.07600427848917563
Epoch 39 loss is 0.060900669434152684
Epoch 49 loss is 0.06557708991894179
Epoch 59 loss is 0.0620218198349882
Epoch 69 loss is 0.055180916741809694
Epoch 79 loss is 0.05770168026537519
Epoch 89 loss is 0.06312691609980167
Epoch 99 loss is 0.06258158094408227
Epoch 109 loss is 0.06726164969189814
Epoch 119 loss is 0.053113842805941974
Epoch 129 loss is 0.05192851680713503
Epoch 139 loss is 0.07066245038442918
Epoch 149 loss is 0.0635699498573305
Epoch 159 loss is 0.05401376535508686
Epoch 169 loss is 0.06267989696299203
Epoch 179 loss is 0.08033388429254887
Epoch 189 loss is 0.06335794221812717
Epoch 199 loss is 0.0672287838247473
Train Acc.:  0.9651975131924712
              precision    recall  f1-score   support

           0       0.96      0.97      0.97      1063
           1       0.97      0.94      0.95      1064
           2       0.99      0.98      0.99      1064
           3       0.99      0.99      0.99      1064
           4       0.96      0.92      0.94      1064
           5       0.96      0.96      0.96      1063
           6       0.95      0.94      0.94      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.96      0.96      0.96      1064
          10       0.98      0.97      0.97      1064

   micro avg       0.97      0.97      0.97     11702
   macro avg       0.97      0.97      0.97     11702
weighted avg       0.97      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.0493437117112904
Epoch 19 loss is 0.056332634788183775
Epoch 29 loss is 0.04848900109983579
Epoch 39 loss is 0.052226300307049216
Epoch 49 loss is 0.06732437820930277
Epoch 59 loss is 0.06013609383791088
Epoch 69 loss is 0.06488484996680653
Epoch 79 loss is 0.07402341604512623
Epoch 89 loss is 0.06415823828871538
Epoch 99 loss is 0.07401829229000033
Epoch 109 loss is 0.07514582485574416
Epoch 119 loss is 0.08362585630122568
Epoch 129 loss is 0.08486246263407854
Epoch 139 loss is 0.07425523240837605
Epoch 149 loss is 0.08443371962698622
Epoch 159 loss is 0.09720449556099098
Epoch 169 loss is 0.10067659587005852
Epoch 179 loss is 0.1193294871778844
Epoch 189 loss is 0.10106213514554797
Epoch 199 loss is 0.09858312506276698
Train Acc.:  0.9670134808896105
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1063
           1       0.96      0.96      0.96      1064
           2       1.00      0.99      0.99      1064
           3       0.99      0.98      0.99      1063
           4       0.96      0.95      0.95      1064
           5       0.98      0.97      0.97      1064
           6       0.97      0.94      0.95      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.98      0.97      0.97      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.98      0.97      0.98     11702
   macro avg       0.98      0.97      0.98     11702
weighted avg       0.98      0.97      0.98     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.048120356670683234
Epoch 19 loss is 0.054593459310181086
Epoch 29 loss is 0.05499350411994715
Epoch 39 loss is 0.0559697642856175
Epoch 49 loss is 0.05754442725054534
Epoch 59 loss is 0.061489513417114956
Epoch 69 loss is 0.07086944463691781
Epoch 79 loss is 0.07706030743617291
Epoch 89 loss is 0.07593725247272468
Epoch 99 loss is 0.0732947676897344
Epoch 109 loss is 0.07531350286253687
Epoch 119 loss is 0.0911661580282024
Epoch 129 loss is 0.11443896325586743
Epoch 139 loss is 0.10335307976202589
Epoch 149 loss is 0.10366860029550481
Epoch 159 loss is 0.09899129316663502
Epoch 169 loss is 0.10486127629475518
Epoch 179 loss is 0.11867640283355274
Epoch 189 loss is 0.12511503382191627
Epoch 199 loss is 0.11799590483737314
Train Acc.:  0.9631679022368449
              precision    recall  f1-score   support

           0       0.97      0.97      0.97      1064
           1       0.96      0.94      0.95      1064
           2       0.99      0.98      0.99      1064
           3       0.99      0.98      0.99      1063
           4       0.97      0.96      0.96      1064
           5       0.96      0.95      0.96      1064
           6       0.93      0.92      0.93      1063
           7       1.00      1.00      1.00      1064
           8       1.00      0.99      1.00      1064
           9       0.96      0.95      0.95      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.97      0.97      0.97     11702
   macro avg       0.97      0.97      0.97     11702
weighted avg       0.97      0.97      0.97     11702
 samples avg       0.96      0.97      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.05517547633181337
Epoch 19 loss is 0.055728762308260765
Epoch 29 loss is 0.06259614482605945
Epoch 39 loss is 0.0581895142248359
Epoch 49 loss is 0.06561329385021032
Epoch 59 loss is 0.07032996307126822
Epoch 69 loss is 0.08136375973696794
Epoch 79 loss is 0.11977187777366366
Epoch 89 loss is 0.084233777680467
Epoch 99 loss is 0.09336862232729168
Epoch 109 loss is 0.10234290900702865
Epoch 119 loss is 0.14733771738094092
Epoch 129 loss is 0.13079876845970226
Epoch 139 loss is 0.11238334142687469
Epoch 149 loss is 0.12448816318563458
Epoch 159 loss is 0.11167881321193102
Epoch 169 loss is 0.10511961659141926
Epoch 179 loss is 0.10364262851145377
Epoch 189 loss is 0.10639918512951829
Epoch 199 loss is 0.09649024979717642
Train Acc.:  0.9571431623475122
              precision    recall  f1-score   support

           0       0.97      0.96      0.96      1063
           1       0.95      0.94      0.95      1064
           2       0.99      0.99      0.99      1064
           3       0.99      0.98      0.98      1063
           4       0.96      0.94      0.95      1064
           5       0.95      0.95      0.95      1064
           6       0.93      0.93      0.93      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.96      0.94      0.95      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.97      0.96      0.97     11702
   macro avg       0.97      0.96      0.97     11702
weighted avg       0.97      0.96      0.97     11702
 samples avg       0.96      0.96      0.96     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.0682420963425835
Epoch 19 loss is 0.06717379428891208
Epoch 29 loss is 0.059015318733788666
Epoch 39 loss is 0.06176190170409206
Epoch 49 loss is 0.07237386654932479
Epoch 59 loss is 0.06911019201668694
Epoch 69 loss is 0.06392903645561016
Epoch 79 loss is 0.0758716126534257
Epoch 89 loss is 0.07730068602143042
Epoch 99 loss is 0.09665151482480443
Epoch 109 loss is 0.09222695319517936
Epoch 119 loss is 0.08182469168154197
Epoch 129 loss is 0.08335947978406187
Epoch 139 loss is 0.09159492862302886
Epoch 149 loss is 0.0906091966868739
Epoch 159 loss is 0.0885280304984252
Epoch 169 loss is 0.09157913406478384
Epoch 179 loss is 0.09323288036652193
Epoch 189 loss is 0.08298648299531121
Epoch 199 loss is 0.09408725998949831
Train Acc.:  0.95340440532399
              precision    recall  f1-score   support

           0       0.97      0.96      0.97      1064
           1       0.98      0.87      0.92      1064
           2       1.00      0.99      0.99      1064
           3       1.00      0.98      0.99      1064
           4       0.97      0.92      0.94      1064
           5       0.93      0.97      0.95      1064
           6       0.96      0.88      0.92      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.95      0.95      0.95      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.98      0.95      0.97     11702
   macro avg       0.98      0.95      0.96     11702
weighted avg       0.98      0.95      0.96     11702
 samples avg       0.95      0.95      0.95     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"MLMVN 48-50-11"</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> <span class="st">"results/MLMVN_48-50-11.png"</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-100-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-100-11">MLMVN [48-100-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-100-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">100</span>)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">100</span>, <span class="dv">11</span>)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-42"><a href="#cb59-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb59-43"><a href="#cb59-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb59-44"><a href="#cb59-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb59-45"><a href="#cb59-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-46"><a href="#cb59-46" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb59-47"><a href="#cb59-47" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb59-48"><a href="#cb59-48" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb59-49"><a href="#cb59-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-50"><a href="#cb59-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb59-51"><a href="#cb59-51" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb59-52"><a href="#cb59-52" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb59-53"><a href="#cb59-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb59-54"><a href="#cb59-54" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb59-55"><a href="#cb59-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-56"><a href="#cb59-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb59-57"><a href="#cb59-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb59-58"><a href="#cb59-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb59-59"><a href="#cb59-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-60"><a href="#cb59-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-61"><a href="#cb59-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb59-62"><a href="#cb59-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb59-63"><a href="#cb59-63" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb59-64"><a href="#cb59-64" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb59-65"><a href="#cb59-65" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb59-66"><a href="#cb59-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-67"><a href="#cb59-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb59-68"><a href="#cb59-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb59-69"><a href="#cb59-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb59-70"><a href="#cb59-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb59-71"><a href="#cb59-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-72"><a href="#cb59-72" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb59-73"><a href="#cb59-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-74"><a href="#cb59-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb59-75"><a href="#cb59-75" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb59-76"><a href="#cb59-76" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb59-77"><a href="#cb59-77" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb59-78"><a href="#cb59-78" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb59-79"><a href="#cb59-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-80"><a href="#cb59-80" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb59-81"><a href="#cb59-81" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb59-82"><a href="#cb59-82" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb59-83"><a href="#cb59-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-84"><a href="#cb59-84" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb59-85"><a href="#cb59-85" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb59-86"><a href="#cb59-86" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb59-87"><a href="#cb59-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-88"><a href="#cb59-88" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb59-89"><a href="#cb59-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb59-90"><a href="#cb59-90" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb59-91"><a href="#cb59-91" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb59-92"><a href="#cb59-92" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb59-93"><a href="#cb59-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-94"><a href="#cb59-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb59-95"><a href="#cb59-95" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb59-96"><a href="#cb59-96" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb59-97"><a href="#cb59-97" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-100-11]"</span>,</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-100-11]"</span>,</span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=054b29d2a004437ca390c9553dc79f1d
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/054b29d2a004437ca390c9553dc79f1d/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-100-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb63-39"><a href="#cb63-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb63-40"><a href="#cb63-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-41"><a href="#cb63-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb63-42"><a href="#cb63-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb63-43"><a href="#cb63-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb63-44"><a href="#cb63-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-45"><a href="#cb63-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb63-46"><a href="#cb63-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb63-47"><a href="#cb63-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb63-48"><a href="#cb63-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb63-49"><a href="#cb63-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb63-50"><a href="#cb63-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb63-51"><a href="#cb63-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb63-52"><a href="#cb63-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb63-53"><a href="#cb63-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb63-54"><a href="#cb63-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb63-55"><a href="#cb63-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb63-56"><a href="#cb63-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb63-57"><a href="#cb63-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb63-58"><a href="#cb63-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb63-59"><a href="#cb63-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-60"><a href="#cb63-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-61"><a href="#cb63-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb63-62"><a href="#cb63-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb63-63"><a href="#cb63-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-64"><a href="#cb63-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-65"><a href="#cb63-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb63-66"><a href="#cb63-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb63-67"><a href="#cb63-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-68"><a href="#cb63-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-69"><a href="#cb63-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb63-70"><a href="#cb63-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb63-71"><a href="#cb63-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-72"><a href="#cb63-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-73"><a href="#cb63-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb63-74"><a href="#cb63-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb63-75"><a href="#cb63-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-76"><a href="#cb63-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-77"><a href="#cb63-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb63-78"><a href="#cb63-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb63-79"><a href="#cb63-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-80"><a href="#cb63-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-81"><a href="#cb63-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb63-82"><a href="#cb63-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb63-83"><a href="#cb63-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-84"><a href="#cb63-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-85"><a href="#cb63-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb63-86"><a href="#cb63-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb63-87"><a href="#cb63-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-88"><a href="#cb63-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb63-89"><a href="#cb63-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb63-90"><a href="#cb63-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb63-91"><a href="#cb63-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-29 15:23:48,722 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.
Epoch 9 loss is 0.09289930875531222
Epoch 19 loss is 0.07704454008271648
Epoch 29 loss is 0.05779679460027949
Epoch 39 loss is 0.054986268708860524
Epoch 49 loss is 0.04733543763708861
Epoch 59 loss is 0.042054025328116786
Epoch 69 loss is 0.0412137783292874
Epoch 79 loss is 0.038980292259018084
Epoch 89 loss is 0.04195052638688717
Epoch 99 loss is 0.04400554969643017
Epoch 109 loss is 0.03583445803540681
Epoch 119 loss is 0.031936028648656224
Epoch 129 loss is 0.03988531487611844
Epoch 139 loss is 0.03480796749345809
Epoch 149 loss is 0.02863502808161461
Epoch 159 loss is 0.03285262207237886
Epoch 169 loss is 0.03205184485479631
Epoch 179 loss is 0.030309476964466488
Epoch 189 loss is 0.031384972778109824
Epoch 199 loss is 0.03278481486374329
Train Acc.:  0.9800884483090136
              precision    recall  f1-score   support

           0       0.98      0.99      0.99      1063
           1       0.95      0.95      0.95      1064
           2       1.00      0.99      0.99      1064
           3       0.99      0.99      0.99      1064
           4       0.98      0.95      0.96      1064
           5       0.98      0.96      0.97      1063
           6       0.97      0.94      0.96      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.97      0.97      1064
          10       0.99      0.98      0.98      1064

   micro avg       0.98      0.97      0.98     11702
   macro avg       0.98      0.97      0.98     11702
weighted avg       0.98      0.97      0.98     11702
 samples avg       0.97      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.09651948221217745
Epoch 19 loss is 0.08151637139831774
Epoch 29 loss is 0.06691473724981084
Epoch 39 loss is 0.06491206984382683
Epoch 49 loss is 0.052847569881330464
Epoch 59 loss is 0.06549010836655457
Epoch 69 loss is 0.049922240791715526
Epoch 79 loss is 0.04560603635225597
Epoch 89 loss is 0.04709434644222514
Epoch 99 loss is 0.09155044545100574
Epoch 109 loss is 0.05177482526370294
Epoch 119 loss is 0.045173095736625105
Epoch 129 loss is 0.041602230912906296
Epoch 139 loss is 0.04224502944542365
Epoch 149 loss is 0.03876666575995179
Epoch 159 loss is 0.04185715320046077
Epoch 169 loss is 0.03916762878300975
Epoch 179 loss is 0.04057138104183288
Epoch 189 loss is 0.036145847549370606
Epoch 199 loss is 0.034255525449209644
Train Acc.:  0.9715640822953832
              precision    recall  f1-score   support

           0       0.98      0.97      0.98      1063
           1       0.95      0.94      0.95      1064
           2       1.00      0.99      0.99      1064
           3       0.98      0.97      0.97      1063
           4       0.97      0.96      0.96      1064
           5       0.96      0.96      0.96      1064
           6       0.98      0.95      0.96      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.96      1064
          10       0.99      0.97      0.98      1064

   micro avg       0.98      0.97      0.97     11702
   macro avg       0.98      0.97      0.97     11702
weighted avg       0.98      0.97      0.97     11702
 samples avg       0.96      0.97      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.10197848648032753
Epoch 19 loss is 0.06620522870230666
Epoch 29 loss is 0.060791586014725764
Epoch 39 loss is 0.044058532288178125
Epoch 49 loss is 0.049066377845850113
Epoch 59 loss is 0.041483171210737656
Epoch 69 loss is 0.041699789178138276
Epoch 79 loss is 0.04010221614370895
Epoch 89 loss is 0.04200853150707565
Epoch 99 loss is 0.03748572558613685
Epoch 109 loss is 0.03339820400178514
Epoch 119 loss is 0.03418882536913687
Epoch 129 loss is 0.0396406334017026
Epoch 139 loss is 0.034145767822064424
Epoch 149 loss is 0.031575263631362274
Epoch 159 loss is 0.03257143025928933
Epoch 169 loss is 0.02900946434349241
Epoch 179 loss is 0.02646721908453089
Epoch 189 loss is 0.0283171769401771
Epoch 199 loss is 0.02924505333332722
Train Acc.:  0.982673531736706
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      1064
           1       0.96      0.94      0.95      1064
           2       1.00      0.99      0.99      1064
           3       0.99      0.98      0.99      1063
           4       0.99      0.96      0.98      1064
           5       0.96      0.96      0.96      1064
           6       0.97      0.96      0.96      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.98      0.97      0.97      1064
          10       0.99      0.98      0.99      1064

   micro avg       0.98      0.98      0.98     11702
   macro avg       0.98      0.98      0.98     11702
weighted avg       0.98      0.98      0.98     11702
 samples avg       0.97      0.98      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.0941163479264211
Epoch 19 loss is 0.07211157489541835
Epoch 29 loss is 0.054892876686113645
Epoch 39 loss is 0.04960960956649523
Epoch 49 loss is 0.05018771907397448
Epoch 59 loss is 0.042637427155138234
Epoch 69 loss is 0.04489222136725094
Epoch 79 loss is 0.04308418388475778
Epoch 89 loss is 0.036878224414511725
Epoch 99 loss is 0.040199719304018
Epoch 109 loss is 0.036902204839538835
Epoch 119 loss is 0.0691947571347315
Epoch 129 loss is 0.030887174296426335
Epoch 139 loss is 0.03259241320497548
Epoch 149 loss is 0.028693328512752206
Epoch 159 loss is 0.031290255170440935
Epoch 169 loss is 0.03944383013459841
Epoch 179 loss is 0.028000841225022752
Epoch 189 loss is 0.02779464459963016
Epoch 199 loss is 0.02589195430212821
Train Acc.:  0.9802807272416519
              precision    recall  f1-score   support

           0       0.99      0.98      0.98      1063
           1       0.96      0.96      0.96      1064
           2       1.00      0.99      0.99      1064
           3       0.98      0.99      0.99      1063
           4       0.98      0.97      0.97      1064
           5       0.97      0.95      0.96      1064
           6       0.97      0.96      0.96      1064
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.97      0.96      0.97      1064
          10       0.99      0.98      0.99      1064

   micro avg       0.98      0.98      0.98     11702
   macro avg       0.98      0.98      0.98     11702
weighted avg       0.98      0.98      0.98     11702
 samples avg       0.97      0.98      0.97     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.09346566112894616
Epoch 19 loss is 0.06853978841650973
Epoch 29 loss is 0.06052018883336245
Epoch 39 loss is 0.05080707795295913
Epoch 49 loss is 0.04300807335716183
Epoch 59 loss is 0.04929946212015129
Epoch 69 loss is 0.03806456857102248
Epoch 79 loss is 0.038455738981116754
Epoch 89 loss is 0.03478891861672422
Epoch 99 loss is 0.03865558608219994
Epoch 109 loss is 0.03655600548813174
Epoch 119 loss is 0.03847341857153493
Epoch 129 loss is 0.03406350920763785
Epoch 139 loss is 0.028289964899613603
Epoch 149 loss is 0.03063547244197927
Epoch 159 loss is 0.03108327055907574
Epoch 169 loss is 0.02898037821549324
Epoch 179 loss is 0.033139572848924596
Epoch 189 loss is 0.03204601005634074
Epoch 199 loss is 0.036471306994475566
Train Acc.:  0.9812207575789946
              precision    recall  f1-score   support

           0       0.99      0.98      0.99      1064
           1       0.96      0.95      0.95      1064
           2       0.99      0.99      0.99      1064
           3       1.00      0.99      0.99      1064
           4       0.97      0.96      0.96      1064
           5       0.97      0.95      0.96      1064
           6       0.96      0.97      0.96      1063
           7       1.00      1.00      1.00      1064
           8       1.00      1.00      1.00      1063
           9       0.98      0.96      0.97      1064
          10       0.99      0.98      0.99      1064

   micro avg       0.98      0.98      0.98     11702
   macro avg       0.98      0.98      0.98     11702
weighted avg       0.98      0.98      0.98     11702
 samples avg       0.97      0.98      0.97     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># title = "MLMVN 48-100-11"</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="co"># image_name = "results/MLMVN_48-100-11.png"</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"$\mathbb</span><span class="sc">{C}</span><span class="st">$: [48-100-11] "</span>, list_losses, list_scores, <span class="st">"mlmvn-48-100-11.png"</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="multi-layer" class="level2">
<h2 class="anchored" data-anchor-id="multi-layer">Multi Layer</h2>
<section id="mlmvn-48-10-10-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-10-10-11">MLMVN [48-10-10-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-10-10-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">10</span>)</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">10</span>, <span class="dv">11</span>)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb78-33"><a href="#cb78-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb78-34"><a href="#cb78-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-35"><a href="#cb78-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb78-36"><a href="#cb78-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb78-37"><a href="#cb78-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-38"><a href="#cb78-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb78-39"><a href="#cb78-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb78-40"><a href="#cb78-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-41"><a href="#cb78-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb78-42"><a href="#cb78-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb78-43"><a href="#cb78-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb78-44"><a href="#cb78-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-45"><a href="#cb78-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb78-46"><a href="#cb78-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb78-47"><a href="#cb78-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb78-48"><a href="#cb78-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-49"><a href="#cb78-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb78-50"><a href="#cb78-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb78-51"><a href="#cb78-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb78-52"><a href="#cb78-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-53"><a href="#cb78-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb78-54"><a href="#cb78-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb78-55"><a href="#cb78-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb78-56"><a href="#cb78-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-57"><a href="#cb78-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb78-58"><a href="#cb78-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb78-59"><a href="#cb78-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb78-60"><a href="#cb78-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb78-61"><a href="#cb78-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb78-62"><a href="#cb78-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-63"><a href="#cb78-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb78-64"><a href="#cb78-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb78-65"><a href="#cb78-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb78-66"><a href="#cb78-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-67"><a href="#cb78-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-68"><a href="#cb78-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb78-69"><a href="#cb78-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb78-70"><a href="#cb78-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb78-71"><a href="#cb78-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb78-72"><a href="#cb78-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb78-73"><a href="#cb78-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-74"><a href="#cb78-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb78-75"><a href="#cb78-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb78-76"><a href="#cb78-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb78-77"><a href="#cb78-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb78-78"><a href="#cb78-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-79"><a href="#cb78-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb78-80"><a href="#cb78-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-81"><a href="#cb78-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb78-82"><a href="#cb78-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb78-83"><a href="#cb78-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb78-84"><a href="#cb78-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb78-85"><a href="#cb78-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb78-86"><a href="#cb78-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-87"><a href="#cb78-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb78-88"><a href="#cb78-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb78-89"><a href="#cb78-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb78-90"><a href="#cb78-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-91"><a href="#cb78-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb78-92"><a href="#cb78-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb78-93"><a href="#cb78-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb78-94"><a href="#cb78-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-95"><a href="#cb78-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb78-96"><a href="#cb78-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb78-97"><a href="#cb78-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb78-98"><a href="#cb78-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb78-99"><a href="#cb78-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb78-100"><a href="#cb78-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-101"><a href="#cb78-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb78-102"><a href="#cb78-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb78-103"><a href="#cb78-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb78-104"><a href="#cb78-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-10-10-11]"</span>,</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-10-10-11]"</span>,</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=e9ee3195207b4491b042a3263013a77e
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e9ee3195207b4491b042a3263013a77e/output/log
======&gt; WARNING! Git diff to large to store (3945kb), skipping uncommitted changes &lt;======</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-10-10-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb82-22"><a href="#cb82-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb82-23"><a href="#cb82-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb82-24"><a href="#cb82-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb82-25"><a href="#cb82-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb82-26"><a href="#cb82-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb82-27"><a href="#cb82-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb82-28"><a href="#cb82-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb82-29"><a href="#cb82-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb82-30"><a href="#cb82-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb82-31"><a href="#cb82-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-32"><a href="#cb82-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb82-33"><a href="#cb82-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-34"><a href="#cb82-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb82-35"><a href="#cb82-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb82-36"><a href="#cb82-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-37"><a href="#cb82-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb82-38"><a href="#cb82-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb82-39"><a href="#cb82-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb82-40"><a href="#cb82-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-41"><a href="#cb82-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb82-42"><a href="#cb82-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb82-43"><a href="#cb82-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb82-44"><a href="#cb82-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-45"><a href="#cb82-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb82-46"><a href="#cb82-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb82-47"><a href="#cb82-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb82-48"><a href="#cb82-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb82-49"><a href="#cb82-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb82-50"><a href="#cb82-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb82-51"><a href="#cb82-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb82-52"><a href="#cb82-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb82-53"><a href="#cb82-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb82-54"><a href="#cb82-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb82-55"><a href="#cb82-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb82-56"><a href="#cb82-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb82-57"><a href="#cb82-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb82-58"><a href="#cb82-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb82-59"><a href="#cb82-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-60"><a href="#cb82-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-61"><a href="#cb82-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb82-62"><a href="#cb82-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb82-63"><a href="#cb82-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-64"><a href="#cb82-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-65"><a href="#cb82-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb82-66"><a href="#cb82-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb82-67"><a href="#cb82-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-68"><a href="#cb82-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-69"><a href="#cb82-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb82-70"><a href="#cb82-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb82-71"><a href="#cb82-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-72"><a href="#cb82-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-73"><a href="#cb82-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb82-74"><a href="#cb82-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb82-75"><a href="#cb82-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-76"><a href="#cb82-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-77"><a href="#cb82-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb82-78"><a href="#cb82-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb82-79"><a href="#cb82-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-80"><a href="#cb82-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-81"><a href="#cb82-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb82-82"><a href="#cb82-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb82-83"><a href="#cb82-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-84"><a href="#cb82-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-85"><a href="#cb82-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb82-86"><a href="#cb82-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb82-87"><a href="#cb82-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb82-88"><a href="#cb82-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb82-89"><a href="#cb82-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb82-90"><a href="#cb82-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb82-91"><a href="#cb82-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-20 11:47:55,893 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.
Epoch 9 loss is 0.6826655212453996
Epoch 19 loss is 0.6434948379370677
Epoch 29 loss is 1.682729382474014
Epoch 39 loss is 0.8484829522237753
Epoch 49 loss is 0.9924725228800226
Epoch 59 loss is 1.225485459978534
Epoch 69 loss is 1.2029846038520795
Epoch 79 loss is 1.099226063781701
Epoch 89 loss is 1.0669221436146312
Epoch 99 loss is 1.107601797709253
Epoch 109 loss is 1.154826187850244
Epoch 119 loss is 1.1937538569152097
Epoch 129 loss is 1.326074848158841
Epoch 139 loss is 1.3625349669490228
Epoch 149 loss is 1.416390194581627
Epoch 159 loss is 1.4488717111937812
Epoch 169 loss is 1.4282188851979989
Epoch 179 loss is 1.3428309118204484
Epoch 189 loss is 1.3047422414686045
Epoch 199 loss is 1.2980417729564269
Train Acc.:  0.2878629264853548
              precision    recall  f1-score   support

           0       0.49      0.68      0.57      1063
           1       0.53      0.09      0.15      1064
           2       0.59      0.17      0.26      1064
           3       0.70      0.02      0.03      1064
           4       0.44      0.21      0.28      1064
           5       0.67      0.74      0.70      1063
           6       0.41      0.04      0.07      1064
           7       0.36      0.80      0.50      1064
           8       0.86      0.94      0.90      1064
           9       0.45      0.13      0.20      1064
          10       0.60      0.01      0.01      1064

   micro avg       0.53      0.35      0.42     11702
   macro avg       0.55      0.35      0.33     11702
weighted avg       0.55      0.35      0.33     11702
 samples avg       0.32      0.35      0.33     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.6345554160230358
Epoch 19 loss is 0.6341311608846562
Epoch 29 loss is 0.6275105257557847
Epoch 39 loss is 0.5689082943180627
Epoch 49 loss is 0.6202519709362332
Epoch 59 loss is 0.5899542082300644
Epoch 69 loss is 0.6059699475133334
Epoch 79 loss is 0.6356775981652049
Epoch 89 loss is 0.6037438777306993
Epoch 99 loss is 0.6510460628543737
Epoch 109 loss is 0.6542612130467913
Epoch 119 loss is 0.6918248563732031
Epoch 129 loss is 0.7057371972704083
Epoch 139 loss is 0.6756918212281602
Epoch 149 loss is 0.7112479285104816
Epoch 159 loss is 0.7220244566123118
Epoch 169 loss is 0.6584324293919726
Epoch 179 loss is 0.677293022265238
Epoch 189 loss is 0.732903885474247
Epoch 199 loss is 0.6873554666623318
Train Acc.:  0.19435127224560428
              precision    recall  f1-score   support

           0       0.65      0.86      0.74      1063
           1       0.60      0.11      0.18      1064
           2       1.00      0.00      0.00      1064
           3       0.71      0.07      0.13      1063
           4       0.53      0.38      0.44      1064
           5       0.51      0.43      0.47      1064
           6       0.12      0.99      0.22      1064
           7       0.83      0.63      0.72      1064
           8       0.86      0.95      0.90      1064
           9       0.60      0.20      0.30      1064
          10       0.49      0.41      0.45      1064

   micro avg       0.36      0.46      0.40     11702
   macro avg       0.63      0.46      0.41     11702
weighted avg       0.63      0.46      0.41     11702
 samples avg       0.32      0.46      0.37     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.5564281727099429
Epoch 19 loss is 0.6418290258673861
Epoch 29 loss is 0.5871009681556589
Epoch 39 loss is 0.5831930390706477
Epoch 49 loss is 0.5565574083864082
Epoch 59 loss is 0.564178441950088
Epoch 69 loss is 0.6094082608646254
Epoch 79 loss is 0.6089586009663431
Epoch 89 loss is 0.5983078625392015
Epoch 99 loss is 0.5945027639119206
Epoch 109 loss is 0.6506962952883656
Epoch 119 loss is 0.7257264045269749
Epoch 129 loss is 0.678212085055888
Epoch 139 loss is 0.6776558524296338
Epoch 149 loss is 0.7104895346845802
Epoch 159 loss is 0.6882588813485881
Epoch 169 loss is 0.7179067223460094
Epoch 179 loss is 0.7732332268676302
Epoch 189 loss is 0.6688145327155556
Epoch 199 loss is 0.7576797799143649
Train Acc.:  0.35227636891917874
              precision    recall  f1-score   support

           0       0.69      0.91      0.78      1064
           1       0.63      0.14      0.23      1064
           2       0.54      0.05      0.08      1064
           3       0.73      0.17      0.27      1063
           4       0.60      0.15      0.24      1064
           5       0.37      0.55      0.45      1064
           6       0.58      0.39      0.47      1063
           7       0.83      0.88      0.85      1064
           8       0.96      0.87      0.91      1064
           9       0.40      0.49      0.44      1064
          10       0.52      0.22      0.31      1064

   micro avg       0.61      0.44      0.51     11702
   macro avg       0.62      0.44      0.46     11702
weighted avg       0.62      0.44      0.46     11702
 samples avg       0.39      0.44      0.41     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.5662067563710496
Epoch 19 loss is 0.6213271730355293
Epoch 29 loss is 0.654203641457264
Epoch 39 loss is 0.6337888713462974
Epoch 49 loss is 0.6457664152151374
Epoch 59 loss is 0.6597888498208774
Epoch 69 loss is 0.6427117237528076
Epoch 79 loss is 0.6644485126309694
Epoch 89 loss is 0.6497885471919859
Epoch 99 loss is 0.664900713379054
Epoch 109 loss is 0.7335350312946143
Epoch 119 loss is 0.6744735994675508
Epoch 129 loss is 0.6019557783170132
Epoch 139 loss is 0.6643925186955931
Epoch 149 loss is 0.6978280461548153
Epoch 159 loss is 0.7079139499893651
Epoch 169 loss is 0.7004879680163926
Epoch 179 loss is 0.6955539259637074
Epoch 189 loss is 0.7227160645393367
Epoch 199 loss is 0.7013177086430019
Train Acc.:  0.30412117845621384
              precision    recall  f1-score   support

           0       0.89      0.70      0.78      1063
           1       0.69      0.03      0.05      1064
           2       0.40      0.00      0.01      1064
           3       0.29      0.75      0.41      1063
           4       0.00      0.00      0.00      1064
           5       0.36      0.40      0.38      1064
           6       0.00      0.00      0.00      1064
           7       0.59      0.90      0.71      1064
           8       0.90      0.95      0.92      1064
           9       0.23      0.91      0.37      1064
          10       0.00      0.00      0.00      1064

   micro avg       0.42      0.42      0.42     11702
   macro avg       0.40      0.42      0.33     11702
weighted avg       0.40      0.42      0.33     11702
 samples avg       0.36      0.42      0.38     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.6462013213971773
Epoch 19 loss is 0.6389570948507705
Epoch 29 loss is 0.6302553856908272
Epoch 39 loss is 0.6686538271036292
Epoch 49 loss is 0.6319803249955375
Epoch 59 loss is 0.6704890164284341
Epoch 69 loss is 0.6675763220336224
Epoch 79 loss is 0.6127046239510358
Epoch 89 loss is 0.6306565278537534
Epoch 99 loss is 0.6080596561205306
Epoch 109 loss is 0.6252069135156932
Epoch 119 loss is 0.6703968429372862
Epoch 129 loss is 0.6282122259810444
Epoch 139 loss is 0.6578433907832637
Epoch 149 loss is 0.6300546160158725
Epoch 159 loss is 0.6349844848961109
Epoch 169 loss is 0.660426835785601
Epoch 179 loss is 0.6781952575179441
Epoch 189 loss is 0.7636298837766232
Epoch 199 loss is 0.7090256706323586
Train Acc.:  0.26226846411861476
              precision    recall  f1-score   support

           0       0.83      0.79      0.81      1064
           1       0.00      0.00      0.00      1064
           2       0.40      0.00      0.00      1064
           3       0.00      0.00      0.00      1064
           4       0.44      0.06      0.10      1064
           5       0.32      0.87      0.46      1064
           6       0.00      0.00      0.00      1063
           7       0.77      0.59      0.67      1064
           8       0.89      0.66      0.76      1063
           9       0.35      0.33      0.34      1064
          10       0.33      0.00      0.01      1064

   micro avg       0.52      0.30      0.38     11702
   macro avg       0.39      0.30      0.29     11702
weighted avg       0.39      0.30      0.29     11702
 samples avg       0.28      0.30      0.29     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"MLMVN 48-10-10-11"</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> <span class="st">"results/MLMVN_48-10-10-11.png"</span></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-20-20-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-20-20-11">MLMVN [48-20-20-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-20-20-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">20</span>)</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">20</span>, <span class="dv">11</span>)</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb96-42"><a href="#cb96-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb96-43"><a href="#cb96-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb96-44"><a href="#cb96-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-45"><a href="#cb96-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb96-46"><a href="#cb96-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb96-47"><a href="#cb96-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb96-48"><a href="#cb96-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-49"><a href="#cb96-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb96-50"><a href="#cb96-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb96-51"><a href="#cb96-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb96-52"><a href="#cb96-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-53"><a href="#cb96-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb96-54"><a href="#cb96-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb96-55"><a href="#cb96-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb96-56"><a href="#cb96-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-57"><a href="#cb96-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb96-58"><a href="#cb96-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb96-59"><a href="#cb96-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb96-60"><a href="#cb96-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb96-61"><a href="#cb96-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb96-62"><a href="#cb96-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-63"><a href="#cb96-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb96-64"><a href="#cb96-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb96-65"><a href="#cb96-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb96-66"><a href="#cb96-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-67"><a href="#cb96-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-68"><a href="#cb96-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb96-69"><a href="#cb96-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb96-70"><a href="#cb96-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb96-71"><a href="#cb96-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb96-72"><a href="#cb96-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb96-73"><a href="#cb96-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-74"><a href="#cb96-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb96-75"><a href="#cb96-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb96-76"><a href="#cb96-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb96-77"><a href="#cb96-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb96-78"><a href="#cb96-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-79"><a href="#cb96-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb96-80"><a href="#cb96-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-81"><a href="#cb96-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb96-82"><a href="#cb96-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb96-83"><a href="#cb96-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb96-84"><a href="#cb96-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb96-85"><a href="#cb96-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb96-86"><a href="#cb96-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-87"><a href="#cb96-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb96-88"><a href="#cb96-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb96-89"><a href="#cb96-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb96-90"><a href="#cb96-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-91"><a href="#cb96-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb96-92"><a href="#cb96-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb96-93"><a href="#cb96-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb96-94"><a href="#cb96-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-95"><a href="#cb96-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb96-96"><a href="#cb96-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb96-97"><a href="#cb96-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb96-98"><a href="#cb96-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb96-99"><a href="#cb96-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb96-100"><a href="#cb96-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-101"><a href="#cb96-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb96-102"><a href="#cb96-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb96-103"><a href="#cb96-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb96-104"><a href="#cb96-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-20-20-11]"</span>,</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-20-20-11]"</span>,</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=a3eff1625a2248499f654e9c4acbc249
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/a3eff1625a2248499f654e9c4acbc249/output/log
======&gt; WARNING! Git diff to large to store (3945kb), skipping uncommitted changes &lt;======</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-20-20-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb100-19"><a href="#cb100-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-20"><a href="#cb100-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb100-21"><a href="#cb100-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb100-22"><a href="#cb100-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb100-23"><a href="#cb100-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb100-24"><a href="#cb100-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb100-25"><a href="#cb100-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb100-26"><a href="#cb100-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb100-27"><a href="#cb100-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb100-28"><a href="#cb100-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb100-29"><a href="#cb100-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb100-30"><a href="#cb100-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb100-31"><a href="#cb100-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-32"><a href="#cb100-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb100-33"><a href="#cb100-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-34"><a href="#cb100-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb100-35"><a href="#cb100-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb100-36"><a href="#cb100-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-37"><a href="#cb100-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb100-38"><a href="#cb100-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb100-39"><a href="#cb100-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb100-40"><a href="#cb100-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-41"><a href="#cb100-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb100-42"><a href="#cb100-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb100-43"><a href="#cb100-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb100-44"><a href="#cb100-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-45"><a href="#cb100-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb100-46"><a href="#cb100-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb100-47"><a href="#cb100-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb100-48"><a href="#cb100-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb100-49"><a href="#cb100-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb100-50"><a href="#cb100-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb100-51"><a href="#cb100-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb100-52"><a href="#cb100-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb100-53"><a href="#cb100-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb100-54"><a href="#cb100-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb100-55"><a href="#cb100-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb100-56"><a href="#cb100-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb100-57"><a href="#cb100-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb100-58"><a href="#cb100-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb100-59"><a href="#cb100-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-60"><a href="#cb100-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-61"><a href="#cb100-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb100-62"><a href="#cb100-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb100-63"><a href="#cb100-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-64"><a href="#cb100-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-65"><a href="#cb100-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb100-66"><a href="#cb100-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb100-67"><a href="#cb100-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-68"><a href="#cb100-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-69"><a href="#cb100-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb100-70"><a href="#cb100-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb100-71"><a href="#cb100-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-72"><a href="#cb100-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-73"><a href="#cb100-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb100-74"><a href="#cb100-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb100-75"><a href="#cb100-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-76"><a href="#cb100-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-77"><a href="#cb100-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb100-78"><a href="#cb100-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb100-79"><a href="#cb100-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-80"><a href="#cb100-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-81"><a href="#cb100-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb100-82"><a href="#cb100-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb100-83"><a href="#cb100-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-84"><a href="#cb100-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-85"><a href="#cb100-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb100-86"><a href="#cb100-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb100-87"><a href="#cb100-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb100-88"><a href="#cb100-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb100-89"><a href="#cb100-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb100-90"><a href="#cb100-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb100-91"><a href="#cb100-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-20 12:08:56,112 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.
Epoch 9 loss is 0.45985028386046645
Epoch 19 loss is 0.46694517354238074
Epoch 29 loss is 0.5138218233961965
Epoch 39 loss is 0.489275320767695
Epoch 49 loss is 0.46309303163233473
Epoch 59 loss is 0.5043632157724234
Epoch 69 loss is 0.5141429366918676
Epoch 79 loss is 0.5218683561772139
Epoch 89 loss is 0.5066106861905705
Epoch 99 loss is 0.4761282653118995
Epoch 109 loss is 0.49827079538493996
Epoch 119 loss is 0.48505905692880175
Epoch 129 loss is 0.5079328540396203
Epoch 139 loss is 0.5163228861017134
Epoch 149 loss is 0.5224325401679479
Epoch 159 loss is 0.5032478966871081
Epoch 169 loss is 0.5706603788963629
Epoch 179 loss is 0.5139070999296187
Epoch 189 loss is 0.5066333485208855
Epoch 199 loss is 0.5263176922877412
Train Acc.:  0.6204200226461855
              precision    recall  f1-score   support

           0       0.87      0.80      0.83      1063
           1       0.83      0.71      0.76      1064
           2       0.77      0.75      0.76      1064
           3       0.86      0.74      0.80      1064
           4       0.68      0.54      0.60      1064
           5       0.83      0.71      0.77      1063
           6       0.56      0.65      0.60      1064
           7       0.94      0.90      0.92      1064
           8       0.99      0.94      0.97      1064
           9       0.74      0.65      0.69      1064
          10       0.80      0.55      0.65      1064

   micro avg       0.80      0.72      0.76     11702
   macro avg       0.81      0.72      0.76     11702
weighted avg       0.81      0.72      0.76     11702
 samples avg       0.67      0.72      0.69     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.5688516497621854
Epoch 19 loss is 0.5853694274411174
Epoch 29 loss is 0.5336314268039691
Epoch 39 loss is 0.5259366272492032
Epoch 49 loss is 0.4406968322599601
Epoch 59 loss is 0.4602583709539878
Epoch 69 loss is 0.422908938931125
Epoch 79 loss is 0.38922475710926
Epoch 89 loss is 0.37434202443270037
Epoch 99 loss is 0.41176429432307965
Epoch 109 loss is 0.4118744091051351
Epoch 119 loss is 0.4045098062961893
Epoch 129 loss is 0.3924032459478811
Epoch 139 loss is 0.3417262166598632
Epoch 149 loss is 0.3713361415147345
Epoch 159 loss is 0.41062360966720174
Epoch 169 loss is 0.4235746380622734
Epoch 179 loss is 0.440063917933553
Epoch 189 loss is 0.4052802512719287
Epoch 199 loss is 0.42583715747994355
Train Acc.:  0.5227636891917875
              precision    recall  f1-score   support

           0       0.92      0.67      0.78      1063
           1       0.77      0.70      0.73      1064
           2       0.80      0.54      0.64      1064
           3       0.71      0.54      0.62      1063
           4       0.59      0.45      0.51      1064
           5       0.79      0.72      0.75      1064
           6       0.46      0.79      0.58      1064
           7       0.80      0.69      0.74      1064
           8       0.99      0.96      0.97      1064
           9       0.00      0.00      0.00      1064
          10       0.65      0.49      0.56      1064

   micro avg       0.72      0.60      0.65     11702
   macro avg       0.68      0.60      0.63     11702
weighted avg       0.68      0.60      0.63     11702
 samples avg       0.56      0.60      0.57     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.47377377477567517
Epoch 19 loss is 0.5818123642246609
Epoch 29 loss is 0.5524255425898911
Epoch 39 loss is 0.5331408636790176
Epoch 49 loss is 0.5266223084583397
Epoch 59 loss is 0.5084204272973452
Epoch 69 loss is 0.5221864123225712
Epoch 79 loss is 0.5321368323514395
Epoch 89 loss is 0.514046891010893
Epoch 99 loss is 0.5044625116486476
Epoch 109 loss is 0.511035881480326
Epoch 119 loss is 0.4235200404283216
Epoch 129 loss is 0.4491360122023273
Epoch 139 loss is 0.4383623463237392
Epoch 149 loss is 0.4002990811088816
Epoch 159 loss is 0.4750563005024748
Epoch 169 loss is 0.49327760079269994
Epoch 179 loss is 0.4378667111784329
Epoch 189 loss is 0.4289851294534545
Epoch 199 loss is 0.44141732628824326
Train Acc.:  0.6061486529792552
              precision    recall  f1-score   support

           0       0.74      0.96      0.83      1064
           1       0.83      0.75      0.79      1064
           2       0.86      0.61      0.71      1064
           3       0.72      0.75      0.73      1063
           4       0.93      0.09      0.16      1064
           5       0.84      0.79      0.81      1064
           6       0.90      0.22      0.36      1063
           7       0.79      0.81      0.80      1064
           8       0.98      0.98      0.98      1064
           9       0.67      0.80      0.73      1064
          10       0.84      0.37      0.51      1064

   micro avg       0.80      0.65      0.72     11702
   macro avg       0.83      0.65      0.68     11702
weighted avg       0.83      0.65      0.68     11702
 samples avg       0.62      0.65      0.63     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.5850991051083132
Epoch 19 loss is 0.5139111042576086
Epoch 29 loss is 0.5051983122342294
Epoch 39 loss is 0.4890419324490173
Epoch 49 loss is 0.4791715259888635
Epoch 59 loss is 0.47829466650556235
Epoch 69 loss is 0.5754105911410774
Epoch 79 loss is 0.5353490414692079
Epoch 89 loss is 0.5049187065218432
Epoch 99 loss is 0.4971644238685353
Epoch 109 loss is 0.5064778615772576
Epoch 119 loss is 0.4903276163818602
Epoch 129 loss is 0.5342514048885157
Epoch 139 loss is 0.5082254582992605
Epoch 149 loss is 0.5033399215026231
Epoch 159 loss is 0.5054516411753327
Epoch 169 loss is 0.49747021993098295
Epoch 179 loss is 0.5080253523201269
Epoch 189 loss is 0.5288063903208304
Epoch 199 loss is 0.5138201494084611
Train Acc.:  0.4667250624906531
              precision    recall  f1-score   support

           0       0.87      0.89      0.88      1063
           1       0.72      0.57      0.64      1064
           2       0.92      0.27      0.42      1064
           3       0.79      0.73      0.76      1063
           4       0.72      0.46      0.56      1064
           5       0.89      0.33      0.48      1064
           6       0.00      0.00      0.00      1064
           7       0.96      0.83      0.89      1064
           8       1.00      0.98      0.99      1064
           9       0.28      0.97      0.44      1064
          10       0.72      0.53      0.61      1064

   micro avg       0.65      0.60      0.62     11702
   macro avg       0.72      0.60      0.60     11702
weighted avg       0.71      0.60      0.60     11702
 samples avg       0.53      0.60      0.55     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.43746739830786185
Epoch 19 loss is 0.5063128960291862
Epoch 29 loss is 0.5443279274534242
Epoch 39 loss is 0.5534543733837946
Epoch 49 loss is 0.5142911469330737
Epoch 59 loss is 0.48123787249991384
Epoch 69 loss is 0.4897241118216021
Epoch 79 loss is 0.486641422076389
Epoch 89 loss is 0.4709927596896531
Epoch 99 loss is 0.5305603540484453
Epoch 109 loss is 0.5086426096428968
Epoch 119 loss is 0.5261937886370114
Epoch 129 loss is 0.5249747488613029
Epoch 139 loss is 0.5480216689726441
Epoch 149 loss is 0.5228087067398941
Epoch 159 loss is 0.4817364265776811
Epoch 169 loss is 0.4622485672858562
Epoch 179 loss is 0.49610587677871815
Epoch 189 loss is 0.4783234726880176
Epoch 199 loss is 0.4741022823045306
Train Acc.:  0.5234900762706433
              precision    recall  f1-score   support

           0       0.87      0.88      0.88      1064
           1       1.00      0.01      0.01      1064
           2       0.82      0.16      0.26      1064
           3       0.81      0.67      0.74      1064
           4       0.76      0.28      0.41      1064
           5       0.44      0.96      0.60      1064
           6       0.69      0.06      0.12      1063
           7       0.96      0.84      0.90      1064
           8       1.00      0.96      0.98      1063
           9       0.60      0.75      0.67      1064
          10       0.69      0.70      0.69      1064

   micro avg       0.71      0.57      0.63     11702
   macro avg       0.79      0.57      0.57     11702
weighted avg       0.79      0.57      0.57     11702
 samples avg       0.55      0.57      0.55     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"MLMVN 48-20-20-11"</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> <span class="st">"results/MLMVN_48-20-20-11.png"</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-50-50-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-50-50-11">MLMVN [48-50-50-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-50-50-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">50</span>)</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb114-8"><a href="#cb114-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">50</span>, <span class="dv">50</span>)</span>
<span id="cb114-9"><a href="#cb114-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb114-10"><a href="#cb114-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">50</span>, <span class="dv">11</span>)</span>
<span id="cb114-11"><a href="#cb114-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb114-12"><a href="#cb114-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb114-13"><a href="#cb114-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb114-14"><a href="#cb114-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb114-15"><a href="#cb114-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb114-16"><a href="#cb114-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb114-17"><a href="#cb114-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb114-18"><a href="#cb114-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb114-19"><a href="#cb114-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb114-20"><a href="#cb114-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb114-21"><a href="#cb114-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb114-22"><a href="#cb114-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-23"><a href="#cb114-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb114-24"><a href="#cb114-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb114-25"><a href="#cb114-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb114-26"><a href="#cb114-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb114-27"><a href="#cb114-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb114-28"><a href="#cb114-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb114-29"><a href="#cb114-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb114-30"><a href="#cb114-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb114-31"><a href="#cb114-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-32"><a href="#cb114-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb114-33"><a href="#cb114-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb114-34"><a href="#cb114-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-35"><a href="#cb114-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb114-36"><a href="#cb114-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb114-37"><a href="#cb114-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-38"><a href="#cb114-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb114-39"><a href="#cb114-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb114-40"><a href="#cb114-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-41"><a href="#cb114-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb114-42"><a href="#cb114-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb114-43"><a href="#cb114-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb114-44"><a href="#cb114-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-45"><a href="#cb114-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb114-46"><a href="#cb114-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb114-47"><a href="#cb114-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb114-48"><a href="#cb114-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-49"><a href="#cb114-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb114-50"><a href="#cb114-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb114-51"><a href="#cb114-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb114-52"><a href="#cb114-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-53"><a href="#cb114-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb114-54"><a href="#cb114-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb114-55"><a href="#cb114-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb114-56"><a href="#cb114-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-57"><a href="#cb114-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb114-58"><a href="#cb114-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb114-59"><a href="#cb114-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb114-60"><a href="#cb114-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb114-61"><a href="#cb114-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb114-62"><a href="#cb114-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-63"><a href="#cb114-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb114-64"><a href="#cb114-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb114-65"><a href="#cb114-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb114-66"><a href="#cb114-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-67"><a href="#cb114-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-68"><a href="#cb114-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb114-69"><a href="#cb114-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb114-70"><a href="#cb114-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb114-71"><a href="#cb114-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb114-72"><a href="#cb114-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb114-73"><a href="#cb114-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-74"><a href="#cb114-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb114-75"><a href="#cb114-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb114-76"><a href="#cb114-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb114-77"><a href="#cb114-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb114-78"><a href="#cb114-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-79"><a href="#cb114-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb114-80"><a href="#cb114-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-81"><a href="#cb114-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb114-82"><a href="#cb114-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb114-83"><a href="#cb114-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb114-84"><a href="#cb114-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb114-85"><a href="#cb114-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb114-86"><a href="#cb114-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-87"><a href="#cb114-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb114-88"><a href="#cb114-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb114-89"><a href="#cb114-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb114-90"><a href="#cb114-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-91"><a href="#cb114-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb114-92"><a href="#cb114-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb114-93"><a href="#cb114-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb114-94"><a href="#cb114-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-95"><a href="#cb114-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb114-96"><a href="#cb114-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb114-97"><a href="#cb114-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb114-98"><a href="#cb114-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb114-99"><a href="#cb114-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb114-100"><a href="#cb114-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb114-101"><a href="#cb114-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb114-102"><a href="#cb114-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb114-103"><a href="#cb114-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb114-104"><a href="#cb114-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-50-50-11]"</span>,</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb115-10"><a href="#cb115-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb115-11"><a href="#cb115-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb115-12"><a href="#cb115-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb115-13"><a href="#cb115-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb115-14"><a href="#cb115-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb115-15"><a href="#cb115-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb115-16"><a href="#cb115-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-50-50-11]"</span>,</span>
<span id="cb115-17"><a href="#cb115-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb115-18"><a href="#cb115-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=2e5cbed78f624d038cdcebb083f902fd
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2e5cbed78f624d038cdcebb083f902fd/output/log
======&gt; WARNING! Git diff to large to store (3945kb), skipping uncommitted changes &lt;======</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-50-50-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb118-19"><a href="#cb118-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-20"><a href="#cb118-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb118-21"><a href="#cb118-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb118-22"><a href="#cb118-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb118-23"><a href="#cb118-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb118-24"><a href="#cb118-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb118-25"><a href="#cb118-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb118-26"><a href="#cb118-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb118-27"><a href="#cb118-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb118-28"><a href="#cb118-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb118-29"><a href="#cb118-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb118-30"><a href="#cb118-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb118-31"><a href="#cb118-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-32"><a href="#cb118-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb118-33"><a href="#cb118-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-34"><a href="#cb118-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb118-35"><a href="#cb118-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb118-36"><a href="#cb118-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-37"><a href="#cb118-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb118-38"><a href="#cb118-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb118-39"><a href="#cb118-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb118-40"><a href="#cb118-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-41"><a href="#cb118-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb118-42"><a href="#cb118-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb118-43"><a href="#cb118-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb118-44"><a href="#cb118-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-45"><a href="#cb118-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb118-46"><a href="#cb118-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb118-47"><a href="#cb118-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb118-48"><a href="#cb118-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb118-49"><a href="#cb118-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb118-50"><a href="#cb118-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb118-51"><a href="#cb118-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb118-52"><a href="#cb118-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb118-53"><a href="#cb118-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb118-54"><a href="#cb118-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb118-55"><a href="#cb118-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb118-56"><a href="#cb118-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb118-57"><a href="#cb118-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb118-58"><a href="#cb118-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb118-59"><a href="#cb118-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-60"><a href="#cb118-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-61"><a href="#cb118-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb118-62"><a href="#cb118-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb118-63"><a href="#cb118-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-64"><a href="#cb118-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-65"><a href="#cb118-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb118-66"><a href="#cb118-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb118-67"><a href="#cb118-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-68"><a href="#cb118-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-69"><a href="#cb118-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb118-70"><a href="#cb118-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb118-71"><a href="#cb118-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-72"><a href="#cb118-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-73"><a href="#cb118-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb118-74"><a href="#cb118-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb118-75"><a href="#cb118-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-76"><a href="#cb118-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-77"><a href="#cb118-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb118-78"><a href="#cb118-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb118-79"><a href="#cb118-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-80"><a href="#cb118-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-81"><a href="#cb118-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb118-82"><a href="#cb118-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb118-83"><a href="#cb118-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-84"><a href="#cb118-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-85"><a href="#cb118-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb118-86"><a href="#cb118-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb118-87"><a href="#cb118-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb118-88"><a href="#cb118-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb118-89"><a href="#cb118-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb118-90"><a href="#cb118-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb118-91"><a href="#cb118-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-20 12:31:47,473 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.
Epoch 9 loss is 0.18509080590032817
Epoch 19 loss is 0.29851992221797385
Epoch 29 loss is 0.38003938826666434
Epoch 39 loss is 0.35283929485604354
Epoch 49 loss is 0.36427067316960327
Epoch 59 loss is 0.31956071771618505
Epoch 69 loss is 0.2919854808615466
Epoch 79 loss is 0.26728589118038387
Epoch 89 loss is 0.2505744858820967
Epoch 99 loss is 0.24366257508408273
Epoch 109 loss is 0.2526296646400297
Epoch 119 loss is 0.2543207117758536
Epoch 129 loss is 0.24703290146744958
Epoch 139 loss is 0.24847459500962163
Epoch 149 loss is 0.24224065429443592
Epoch 159 loss is 0.2280505035935983
Epoch 169 loss is 0.22783374165114506
Epoch 179 loss is 0.22949014620485136
Epoch 189 loss is 0.2128073209289787
Epoch 199 loss is 0.21034013310932373
Train Acc.:  0.8228683743884462
              precision    recall  f1-score   support

           0       0.94      0.90      0.92      1063
           1       0.87      0.80      0.83      1064
           2       0.95      0.87      0.91      1064
           3       0.92      0.91      0.91      1064
           4       0.86      0.73      0.79      1064
           5       0.83      0.88      0.86      1063
           6       0.81      0.76      0.78      1064
           7       0.99      0.97      0.98      1064
           8       1.00      0.99      0.99      1064
           9       0.84      0.81      0.82      1064
          10       0.89      0.86      0.88      1064

   micro avg       0.90      0.86      0.88     11702
   macro avg       0.90      0.86      0.88     11702
weighted avg       0.90      0.86      0.88     11702
 samples avg       0.84      0.86      0.85     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2861229127408851
Epoch 19 loss is 0.3707694820754066
Epoch 29 loss is 0.4135629910389318
Epoch 39 loss is 0.4015215646610006
Epoch 49 loss is 0.3934166022651121
Epoch 59 loss is 0.37682299911311523
Epoch 69 loss is 0.32850649921225733
Epoch 79 loss is 0.3117090487146507
Epoch 89 loss is 0.3013186492740078
Epoch 99 loss is 0.2815918980042734
Epoch 109 loss is 0.3025606700072974
Epoch 119 loss is 0.29640244765806867
Epoch 129 loss is 0.2886136706741845
Epoch 139 loss is 0.27816065918712707
Epoch 149 loss is 0.27148853781337934
Epoch 159 loss is 0.26770085066452964
Epoch 169 loss is 0.2697538316258019
Epoch 179 loss is 0.27578019618131644
Epoch 189 loss is 0.26646575962929053
Epoch 199 loss is 0.2633851934702109
Train Acc.:  0.7693934667891554
              precision    recall  f1-score   support

           0       0.92      0.87      0.90      1063
           1       0.84      0.83      0.83      1064
           2       0.92      0.84      0.88      1064
           3       0.89      0.83      0.86      1063
           4       0.76      0.71      0.73      1064
           5       0.84      0.81      0.83      1064
           6       0.73      0.75      0.74      1064
           7       0.99      0.93      0.96      1064
           8       1.00      0.99      0.99      1064
           9       0.78      0.77      0.78      1064
          10       0.86      0.85      0.86      1064

   micro avg       0.87      0.83      0.85     11702
   macro avg       0.87      0.83      0.85     11702
weighted avg       0.87      0.83      0.85     11702
 samples avg       0.80      0.83      0.81     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.2636942418729698
Epoch 19 loss is 0.38913279833920067
Epoch 29 loss is 0.41660029638624246
Epoch 39 loss is 0.35837348257730234
Epoch 49 loss is 0.32704220072461
Epoch 59 loss is 0.3238192812282917
Epoch 69 loss is 0.324375133254903
Epoch 79 loss is 0.283519484376465
Epoch 89 loss is 0.290315689826698
Epoch 99 loss is 0.29113917867086464
Epoch 109 loss is 0.3312690777595432
Epoch 119 loss is 0.28558159575996994
Epoch 129 loss is 0.27814503072941504
Epoch 139 loss is 0.2974450427403483
Epoch 149 loss is 0.27829655088323624
Epoch 159 loss is 0.27569508470039195
Epoch 169 loss is 0.25619125382733604
Epoch 179 loss is 0.25419745505951113
Epoch 189 loss is 0.2643537804062262
Epoch 199 loss is 0.2618419861302584
Train Acc.:  0.788728181682227
              precision    recall  f1-score   support

           0       0.93      0.90      0.91      1064
           1       0.87      0.77      0.82      1064
           2       0.93      0.89      0.91      1064
           3       0.88      0.86      0.87      1063
           4       0.75      0.73      0.74      1064
           5       0.83      0.85      0.84      1064
           6       0.82      0.71      0.76      1063
           7       0.98      0.97      0.97      1064
           8       1.00      0.98      0.99      1064
           9       0.81      0.79      0.80      1064
          10       0.89      0.84      0.86      1064

   micro avg       0.88      0.84      0.86     11702
   macro avg       0.88      0.84      0.86     11702
weighted avg       0.88      0.84      0.86     11702
 samples avg       0.81      0.84      0.82     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.24772645202023053
Epoch 19 loss is 0.35750288399056496
Epoch 29 loss is 0.3701326898119261
Epoch 39 loss is 0.33407303916772696
Epoch 49 loss is 0.3462574832072576
Epoch 59 loss is 0.3339426384355099
Epoch 69 loss is 0.33922922079491125
Epoch 79 loss is 0.3259204646015445
Epoch 89 loss is 0.3115701407476312
Epoch 99 loss is 0.32205575641217443
Epoch 109 loss is 0.29186489277176464
Epoch 119 loss is 0.2695312388966853
Epoch 129 loss is 0.2950951697843554
Epoch 139 loss is 0.27228947845328294
Epoch 149 loss is 0.27533878935326495
Epoch 159 loss is 0.256029731536393
Epoch 169 loss is 0.2520228613461403
Epoch 179 loss is 0.2517227649665734
Epoch 189 loss is 0.23340040093628597
Epoch 199 loss is 0.2333106974808292
Train Acc.:  0.7876386010639435
              precision    recall  f1-score   support

           0       0.91      0.89      0.90      1063
           1       0.82      0.84      0.83      1064
           2       0.91      0.86      0.89      1064
           3       0.93      0.85      0.89      1063
           4       0.83      0.78      0.81      1064
           5       0.84      0.80      0.82      1064
           6       0.87      0.60      0.71      1064
           7       0.99      0.96      0.97      1064
           8       1.00      0.99      0.99      1064
           9       0.84      0.81      0.82      1064
          10       0.88      0.83      0.85      1064

   micro avg       0.89      0.84      0.86     11702
   macro avg       0.89      0.84      0.86     11702
weighted avg       0.89      0.84      0.86     11702
 samples avg       0.81      0.84      0.82     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_932/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.3290784643476184
Epoch 19 loss is 0.3882220039257218
Epoch 29 loss is 0.3997899653239338
Epoch 39 loss is 0.38339673065514107
Epoch 49 loss is 0.3914353149120599
Epoch 59 loss is 0.3413151340879377
Epoch 69 loss is 0.3134185935040384
Epoch 79 loss is 0.2860619695979416
Epoch 89 loss is 0.2603699206305271
Epoch 99 loss is 0.25138523657636386
Epoch 109 loss is 0.23567740645017163
Epoch 119 loss is 0.21597357797692088
Epoch 129 loss is 0.20476071706230148
Epoch 139 loss is 0.20329952377044547
Epoch 149 loss is 0.20430155607077827
Epoch 159 loss is 0.2098854033635211
Epoch 169 loss is 0.2101066364317881
Epoch 179 loss is 0.208051023591462
Epoch 189 loss is 0.20806564248675852
Epoch 199 loss is 0.2095600698320772
Train Acc.:  0.8154976819706454
              precision    recall  f1-score   support

           0       0.91      0.93      0.92      1064
           1       0.92      0.64      0.75      1064
           2       0.92      0.89      0.91      1064
           3       0.95      0.87      0.91      1064
           4       0.86      0.71      0.78      1064
           5       0.74      0.93      0.82      1064
           6       0.84      0.64      0.73      1063
           7       0.99      0.99      0.99      1064
           8       1.00      0.99      1.00      1063
           9       0.84      0.82      0.83      1064
          10       0.92      0.87      0.90      1064

   micro avg       0.90      0.84      0.87     11702
   macro avg       0.90      0.84      0.87     11702
weighted avg       0.90      0.84      0.87     11702
 samples avg       0.82      0.84      0.83     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="st">"MLMVN 48-50-50-11"</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a>image_name <span class="op">=</span> <span class="st">"results/MLMVN_48-50-50-11.png"</span></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-45-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="mlmvn-48-100-100-11" class="level3">
<h3 class="anchored" data-anchor-id="mlmvn-48-100-100-11">MLMVN [48-100-100-11]</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>PATH <span class="op">=</span> <span class="bu">str</span>(Path.cwd() <span class="op">/</span> <span class="st">"models/autass-mlmvn_48-100-100-11.pt"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, categories, periodicity):</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.categories <span class="op">=</span> categories</span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.periodicity <span class="op">=</span> periodicity</span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> FirstLayer(<span class="dv">48</span>, <span class="dv">100</span>)</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act1 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer <span class="op">=</span> HiddenLayer(<span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act2 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> OutputLayer(<span class="dv">100</span>, <span class="dv">11</span>)</span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.phase_act3 <span class="op">=</span> cmplx_phase_activation()</span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hooks</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.first_linear.register_full_backward_hook(</span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.first_layer_backward_hook</span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer_hook_handle <span class="op">=</span> <span class="va">self</span>.hidden_layer.register_full_backward_hook(</span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layer_backward_hook</span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_hook_handle <span class="op">=</span> <span class="va">self</span>.linear_out.register_full_backward_hook(</span>
<span id="cb132-20"><a href="#cb132-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.output_layer_backward_hook</span>
<span id="cb132-21"><a href="#cb132-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb132-22"><a href="#cb132-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-23"><a href="#cb132-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb132-24"><a href="#cb132-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.first_linear(x)</span>
<span id="cb132-25"><a href="#cb132-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act1(x)</span>
<span id="cb132-26"><a href="#cb132-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.hidden_layer(x)</span>
<span id="cb132-27"><a href="#cb132-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act2(x)</span>
<span id="cb132-28"><a href="#cb132-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(x)</span>
<span id="cb132-29"><a href="#cb132-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.phase_act3(x)</span>
<span id="cb132-30"><a href="#cb132-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb132-31"><a href="#cb132-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-32"><a href="#cb132-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> first_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb132-33"><a href="#cb132-33" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"first_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb132-34"><a href="#cb132-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-35"><a href="#cb132-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hidden_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb132-36"><a href="#cb132-36" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"hidden_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb132-37"><a href="#cb132-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-38"><a href="#cb132-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer_backward_hook(<span class="va">self</span>, module, grad_input, grad_output):</span>
<span id="cb132-39"><a href="#cb132-39" aria-hidden="true" tabindex="-1"></a>        fc_hook(<span class="st">"output_layer"</span>, module, grad_input, grad_output)</span>
<span id="cb132-40"><a href="#cb132-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-41"><a href="#cb132-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> angle2class(<span class="va">self</span>, x: torch.tensor) <span class="op">-&gt;</span> torch.tensor:</span>
<span id="cb132-42"><a href="#cb132-42" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> x.angle() <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.pi</span>
<span id="cb132-43"><a href="#cb132-43" aria-hidden="true" tabindex="-1"></a>        angle <span class="op">=</span> torch.remainder(tmp, <span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb132-44"><a href="#cb132-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-45"><a href="#cb132-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This will be the discrete output (the number of sector)</span></span>
<span id="cb132-46"><a href="#cb132-46" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> torch.floor(<span class="va">self</span>.categories <span class="op">*</span> <span class="va">self</span>.periodicity <span class="op">*</span> angle <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi))</span>
<span id="cb132-47"><a href="#cb132-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.remainder(o, <span class="va">self</span>.categories)</span>
<span id="cb132-48"><a href="#cb132-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-49"><a href="#cb132-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb132-50"><a href="#cb132-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb132-51"><a href="#cb132-51" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the prediction task of the network</span></span>
<span id="cb132-52"><a href="#cb132-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-53"><a href="#cb132-53" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb132-54"><a href="#cb132-54" aria-hidden="true" tabindex="-1"></a><span class="co">          x: torch.Tensor</span></span>
<span id="cb132-55"><a href="#cb132-55" aria-hidden="true" tabindex="-1"></a><span class="co">            Input tensor of size ([3])</span></span>
<span id="cb132-56"><a href="#cb132-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-57"><a href="#cb132-57" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb132-58"><a href="#cb132-58" aria-hidden="true" tabindex="-1"></a><span class="co">          Most likely class i.e., Label with the highest score</span></span>
<span id="cb132-59"><a href="#cb132-59" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb132-60"><a href="#cb132-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the networks</span></span>
<span id="cb132-61"><a href="#cb132-61" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.forward(x)</span>
<span id="cb132-62"><a href="#cb132-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-63"><a href="#cb132-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># # Choose the label with the highest score</span></span>
<span id="cb132-64"><a href="#cb132-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.argmax(output, 1)</span></span>
<span id="cb132-65"><a href="#cb132-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.angle2class(output)</span>
<span id="cb132-66"><a href="#cb132-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-67"><a href="#cb132-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-68"><a href="#cb132-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):</span>
<span id="cb132-69"><a href="#cb132-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of losses for visualization</span></span>
<span id="cb132-70"><a href="#cb132-70" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb132-71"><a href="#cb132-71" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb132-72"><a href="#cb132-72" aria-hidden="true" tabindex="-1"></a>    acc_best <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb132-73"><a href="#cb132-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-74"><a href="#cb132-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb132-75"><a href="#cb132-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pass the data through the network and compute the loss</span></span>
<span id="cb132-76"><a href="#cb132-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll use the whole dataset during the training instead of using batches</span></span>
<span id="cb132-77"><a href="#cb132-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in to order to keep the code simple for now.</span></span>
<span id="cb132-78"><a href="#cb132-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-79"><a href="#cb132-79" aria-hidden="true" tabindex="-1"></a>        batch_loss <span class="op">=</span> []</span>
<span id="cb132-80"><a href="#cb132-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-81"><a href="#cb132-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>((X.shape[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb132-82"><a href="#cb132-82" aria-hidden="true" tabindex="-1"></a>            start_j <span class="op">=</span> j <span class="op">*</span> batch_size</span>
<span id="cb132-83"><a href="#cb132-83" aria-hidden="true" tabindex="-1"></a>            end_j <span class="op">=</span> start_j <span class="op">+</span> batch_size</span>
<span id="cb132-84"><a href="#cb132-84" aria-hidden="true" tabindex="-1"></a>            xb <span class="op">=</span> X[start_j:end_j]</span>
<span id="cb132-85"><a href="#cb132-85" aria-hidden="true" tabindex="-1"></a>            yb <span class="op">=</span> y[start_j:end_j]</span>
<span id="cb132-86"><a href="#cb132-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-87"><a href="#cb132-87" aria-hidden="true" tabindex="-1"></a>            y_pred <span class="op">=</span> model(xb)</span>
<span id="cb132-88"><a href="#cb132-88" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(y_pred, yb, categories, periodicity)</span>
<span id="cb132-89"><a href="#cb132-89" aria-hidden="true" tabindex="-1"></a>            batch_loss.append((torch.<span class="bu">abs</span>(loss)).detach().numpy())</span>
<span id="cb132-90"><a href="#cb132-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-91"><a href="#cb132-91" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb132-92"><a href="#cb132-92" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb132-93"><a href="#cb132-93" aria-hidden="true" tabindex="-1"></a>            optimizer.step(inputs<span class="op">=</span>xb, layers<span class="op">=</span><span class="bu">list</span>(model.children()))</span>
<span id="cb132-94"><a href="#cb132-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-95"><a href="#cb132-95" aria-hidden="true" tabindex="-1"></a>        losses.append(<span class="bu">sum</span>(batch_loss) <span class="op">/</span> <span class="bu">len</span>(batch_loss))</span>
<span id="cb132-96"><a href="#cb132-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">9</span>:</span>
<span id="cb132-97"><a href="#cb132-97" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss is </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb132-98"><a href="#cb132-98" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb132-99"><a href="#cb132-99" aria-hidden="true" tabindex="-1"></a>        scores.append(accuracy(y_pred.squeeze(), y))</span>
<span id="cb132-100"><a href="#cb132-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-101"><a href="#cb132-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[<span class="op">-</span><span class="dv">1</span>] <span class="op">&gt;</span> acc_best:</span>
<span id="cb132-102"><a href="#cb132-102" aria-hidden="true" tabindex="-1"></a>            acc_best <span class="op">=</span> scores[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb132-103"><a href="#cb132-103" aria-hidden="true" tabindex="-1"></a>            torch.save(model.state_dict(), PATH)</span>
<span id="cb132-104"><a href="#cb132-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses, scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> Task.init(</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">"mlmvn"</span>,</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>    task_name<span class="op">=</span><span class="st">"SDD-mlmvn-[48-100-100-11]"</span>,</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a>    tags<span class="op">=</span>[<span class="st">"mlmvn"</span>, <span class="st">"SDD"</span>, <span class="st">"multiple_runs"</span>],</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>writer <span class="op">=</span> SummaryWriter()</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true" tabindex="-1"></a><span class="co"># â€ƒcapture a dictionary of hyperparameters with config</span></span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true" tabindex="-1"></a>config_dict <span class="op">=</span> {</span>
<span id="cb133-10"><a href="#cb133-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"learning_rate"</span>: lr,</span>
<span id="cb133-11"><a href="#cb133-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"epochs"</span>: epochs,</span>
<span id="cb133-12"><a href="#cb133-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"batch_size"</span>: batch_size,</span>
<span id="cb133-13"><a href="#cb133-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"optim"</span>: <span class="st">"ECL"</span>,</span>
<span id="cb133-14"><a href="#cb133-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"categories"</span>: categories,</span>
<span id="cb133-15"><a href="#cb133-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"periodicity"</span>: periodicity,</span>
<span id="cb133-16"><a href="#cb133-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"layer"</span>: <span class="st">"[48-100-100-11]"</span>,</span>
<span id="cb133-17"><a href="#cb133-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb133-18"><a href="#cb133-18" aria-hidden="true" tabindex="-1"></a>task.<span class="ex">connect</span>(config_dict)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ClearML Task: created new task id=40045a36abde40619f35b7868e1213d1
ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/40045a36abde40619f35b7868e1213d1/output/log</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>{'learning_rate': 1,
 'epochs': 200,
 'batch_size': 538,
 'optim': 'ECL',
 'categories': 2,
 'periodicity': 1,
 'layer': '[48-100-100-11]'}</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a>sss <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">5</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a>list_losses <span class="op">=</span> []</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a>list_scores <span class="op">=</span> []</span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a>list_acc <span class="op">=</span> []</span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a>list_loss <span class="op">=</span> []</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true" tabindex="-1"></a>list_f1 <span class="op">=</span> []</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true" tabindex="-1"></a>list_precision <span class="op">=</span> []</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true" tabindex="-1"></a>list_recall <span class="op">=</span> []</span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> sss.split(X, y):</span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true" tabindex="-1"></a>    model_dict: <span class="bu">dict</span> <span class="op">=</span> {}</span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true" tabindex="-1"></a>    x_train, x_valid, y_train, y_valid <span class="op">=</span> get_splitted_data_by_index(</span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true" tabindex="-1"></a>        X, y, neuronCats, train_index, test_index</span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(categories<span class="op">=</span>categories, periodicity<span class="op">=</span>periodicity)</span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> ComplexMSELoss.<span class="bu">apply</span></span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> ECL(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-20"><a href="#cb136-20" aria-hidden="true" tabindex="-1"></a>    losses, scores <span class="op">=</span> fit(</span>
<span id="cb136-21"><a href="#cb136-21" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb136-22"><a href="#cb136-22" aria-hidden="true" tabindex="-1"></a>        x_train,</span>
<span id="cb136-23"><a href="#cb136-23" aria-hidden="true" tabindex="-1"></a>        y_train,</span>
<span id="cb136-24"><a href="#cb136-24" aria-hidden="true" tabindex="-1"></a>        epochs<span class="op">=</span>epochs,</span>
<span id="cb136-25"><a href="#cb136-25" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb136-26"><a href="#cb136-26" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb136-27"><a href="#cb136-27" aria-hidden="true" tabindex="-1"></a>        criterion<span class="op">=</span>criterion,</span>
<span id="cb136-28"><a href="#cb136-28" aria-hidden="true" tabindex="-1"></a>        categories<span class="op">=</span>categories,</span>
<span id="cb136-29"><a href="#cb136-29" aria-hidden="true" tabindex="-1"></a>        periodicity<span class="op">=</span>periodicity,</span>
<span id="cb136-30"><a href="#cb136-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb136-31"><a href="#cb136-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-32"><a href="#cb136-32" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(torch.load(PATH))</span>
<span id="cb136-33"><a href="#cb136-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-34"><a href="#cb136-34" aria-hidden="true" tabindex="-1"></a>    list_scores.append(scores)</span>
<span id="cb136-35"><a href="#cb136-35" aria-hidden="true" tabindex="-1"></a>    list_losses.append(losses)</span>
<span id="cb136-36"><a href="#cb136-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-37"><a href="#cb136-37" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_train)</span>
<span id="cb136-38"><a href="#cb136-38" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_train)</span>
<span id="cb136-39"><a href="#cb136-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Train Acc.: "</span>, acc)</span>
<span id="cb136-40"><a href="#cb136-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-41"><a href="#cb136-41" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(x_valid)</span>
<span id="cb136-42"><a href="#cb136-42" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy(y_pred.squeeze(), y_valid)</span>
<span id="cb136-43"><a href="#cb136-43" aria-hidden="true" tabindex="-1"></a>    list_acc.append(acc)</span>
<span id="cb136-44"><a href="#cb136-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-45"><a href="#cb136-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(classification_report(y_valid, y_pred.detach().numpy(), zero_division<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb136-46"><a href="#cb136-46" aria-hidden="true" tabindex="-1"></a>    list_f1.append(</span>
<span id="cb136-47"><a href="#cb136-47" aria-hidden="true" tabindex="-1"></a>        f1_score(y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb136-48"><a href="#cb136-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb136-49"><a href="#cb136-49" aria-hidden="true" tabindex="-1"></a>    list_precision.append(</span>
<span id="cb136-50"><a href="#cb136-50" aria-hidden="true" tabindex="-1"></a>        precision_score(</span>
<span id="cb136-51"><a href="#cb136-51" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb136-52"><a href="#cb136-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb136-53"><a href="#cb136-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb136-54"><a href="#cb136-54" aria-hidden="true" tabindex="-1"></a>    list_recall.append(</span>
<span id="cb136-55"><a href="#cb136-55" aria-hidden="true" tabindex="-1"></a>        recall_score(</span>
<span id="cb136-56"><a href="#cb136-56" aria-hidden="true" tabindex="-1"></a>            y_valid, y_pred.detach().numpy(), average<span class="op">=</span><span class="st">"weighted"</span>, zero_division<span class="op">=</span><span class="dv">0</span></span>
<span id="cb136-57"><a href="#cb136-57" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb136-58"><a href="#cb136-58" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb136-59"><a href="#cb136-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-60"><a href="#cb136-60" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-61"><a href="#cb136-61" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_mean"</span>,</span>
<span id="cb136-62"><a href="#cb136-62" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_acc),</span>
<span id="cb136-63"><a href="#cb136-63" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-64"><a href="#cb136-64" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-65"><a href="#cb136-65" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_accuracy_std"</span>,</span>
<span id="cb136-66"><a href="#cb136-66" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_acc),</span>
<span id="cb136-67"><a href="#cb136-67" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-68"><a href="#cb136-68" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-69"><a href="#cb136-69" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_mean"</span>,</span>
<span id="cb136-70"><a href="#cb136-70" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_f1),</span>
<span id="cb136-71"><a href="#cb136-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-72"><a href="#cb136-72" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-73"><a href="#cb136-73" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_f1_std"</span>,</span>
<span id="cb136-74"><a href="#cb136-74" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_f1),</span>
<span id="cb136-75"><a href="#cb136-75" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-76"><a href="#cb136-76" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-77"><a href="#cb136-77" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_mean"</span>,</span>
<span id="cb136-78"><a href="#cb136-78" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_precision),</span>
<span id="cb136-79"><a href="#cb136-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-80"><a href="#cb136-80" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-81"><a href="#cb136-81" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_precision_std"</span>,</span>
<span id="cb136-82"><a href="#cb136-82" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_precision),</span>
<span id="cb136-83"><a href="#cb136-83" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-84"><a href="#cb136-84" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-85"><a href="#cb136-85" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_mean"</span>,</span>
<span id="cb136-86"><a href="#cb136-86" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.mean(list_recall),</span>
<span id="cb136-87"><a href="#cb136-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb136-88"><a href="#cb136-88" aria-hidden="true" tabindex="-1"></a>Logger.current_logger().report_single_value(</span>
<span id="cb136-89"><a href="#cb136-89" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"val_recall_std"</span>,</span>
<span id="cb136-90"><a href="#cb136-90" aria-hidden="true" tabindex="-1"></a>    value<span class="op">=</span>np.std(list_recall),</span>
<span id="cb136-91"><a href="#cb136-91" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2022-09-29 15:56:48,669 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.
Epoch 9 loss is 0.14245659853440193
Epoch 19 loss is 0.11926958512560476
Epoch 29 loss is 0.10088007783397561
Epoch 39 loss is 0.10160706429771875
Epoch 49 loss is 0.09256600861419079
Epoch 59 loss is 0.10744608628004096
Epoch 69 loss is 0.12774685299619692
Epoch 79 loss is 0.12405285738088577
Epoch 89 loss is 0.12148849057961768
Epoch 99 loss is 0.17548190991717774
Epoch 109 loss is 0.2519070374055556
Epoch 119 loss is 0.29390135104918574
Epoch 129 loss is 0.2631115029064872
Epoch 139 loss is 0.26468762700088244
Epoch 149 loss is 0.2426366788530615
Epoch 159 loss is 0.24249734627332406
Epoch 169 loss is 0.2180138422574741
Epoch 179 loss is 0.1942677829861684
Epoch 189 loss is 0.1656781252246518
Epoch 199 loss is 0.15678583025914528
Train Acc.:  0.9119576131775162
              precision    recall  f1-score   support

           0       0.97      0.95      0.96      1063
           1       0.89      0.91      0.90      1064
           2       0.99      0.96      0.97      1064
           3       0.94      0.95      0.95      1064
           4       0.92      0.90      0.91      1064
           5       0.91      0.90      0.91      1063
           6       0.86      0.86      0.86      1064
           7       1.00      0.99      0.99      1064
           8       1.00      1.00      1.00      1064
           9       0.92      0.86      0.89      1064
          10       0.96      0.93      0.95      1064

   micro avg       0.94      0.93      0.93     11702
   macro avg       0.94      0.93      0.93     11702
weighted avg       0.94      0.93      0.93     11702
 samples avg       0.91      0.93      0.92     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.13182902422312057
Epoch 19 loss is 0.10948094408196307
Epoch 29 loss is 0.11030647533056807
Epoch 39 loss is 0.09747520923272845
Epoch 49 loss is 0.10055963818000929
Epoch 59 loss is 0.12781840344883313
Epoch 69 loss is 0.16007342374600603
Epoch 79 loss is 0.2685125349928008
Epoch 89 loss is 0.2605283290643327
Epoch 99 loss is 0.24953134086762976
Epoch 109 loss is 0.26341047064393114
Epoch 119 loss is 0.23938208227294375
Epoch 129 loss is 0.22167797312644905
Epoch 139 loss is 0.19953444239050008
Epoch 149 loss is 0.20466815381159612
Epoch 159 loss is 0.18944607768172028
Epoch 169 loss is 0.17543960927604327
Epoch 179 loss is 0.18857073919083642
Epoch 189 loss is 0.1775085144108209
Epoch 199 loss is 0.1846037562875263
Train Acc.:  0.9090947935137906
              precision    recall  f1-score   support

           0       0.95      0.96      0.96      1063
           1       0.89      0.88      0.89      1064
           2       0.99      0.96      0.98      1064
           3       0.98      0.97      0.97      1063
           4       0.94      0.89      0.92      1064
           5       0.91      0.87      0.89      1064
           6       0.87      0.85      0.86      1064
           7       1.00      1.00      1.00      1064
           8       1.00      0.99      0.99      1064
           9       0.94      0.88      0.91      1064
          10       0.96      0.91      0.94      1064

   micro avg       0.95      0.92      0.94     11702
   macro avg       0.95      0.92      0.94     11702
weighted avg       0.95      0.92      0.94     11702
 samples avg       0.91      0.92      0.91     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.14836475443880184
Epoch 19 loss is 0.1144531471986026
Epoch 29 loss is 0.11196463483836677
Epoch 39 loss is 0.11829244064335528
Epoch 49 loss is 0.2990447238807741
Epoch 59 loss is 0.3416765423666923
Epoch 69 loss is 0.3760340294767417
Epoch 79 loss is 0.3602369454833449
Epoch 89 loss is 0.3415423835663611
Epoch 99 loss is 0.3017989133006243
Epoch 109 loss is 0.2706555786076341
Epoch 119 loss is 0.28536886623172014
Epoch 129 loss is 0.25318045626204816
Epoch 139 loss is 0.23099538649480147
Epoch 149 loss is 0.20753359049453549
Epoch 159 loss is 0.19738129756691591
Epoch 169 loss is 0.17773918197037844
Epoch 179 loss is 0.20320036291560054
Epoch 189 loss is 0.1805354561882004
Epoch 199 loss is 0.16590106544112354
Train Acc.:  0.889482342384686
              precision    recall  f1-score   support

           0       0.97      0.90      0.94      1064
           1       0.91      0.86      0.88      1064
           2       0.97      0.91      0.94      1064
           3       0.98      0.96      0.97      1063
           4       0.89      0.89      0.89      1064
           5       0.90      0.91      0.90      1064
           6       0.84      0.84      0.84      1063
           7       1.00      0.99      1.00      1064
           8       1.00      1.00      1.00      1064
           9       0.91      0.85      0.88      1064
          10       0.97      0.93      0.95      1064

   micro avg       0.94      0.91      0.93     11702
   macro avg       0.94      0.91      0.93     11702
weighted avg       0.94      0.91      0.93     11702
 samples avg       0.89      0.91      0.90     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.18644038213906955
Epoch 19 loss is 0.12830064648067416
Epoch 29 loss is 0.16682216144940276
Epoch 39 loss is 0.1759418592172155
Epoch 49 loss is 0.2644686708855199
Epoch 59 loss is 0.3123614077032397
Epoch 69 loss is 0.27872443139031733
Epoch 79 loss is 0.29207407259361867
Epoch 89 loss is 0.2838279987711244
Epoch 99 loss is 0.24346773650683004
Epoch 109 loss is 0.21714146376339183
Epoch 119 loss is 0.19293183048183818
Epoch 129 loss is 0.1947981148247875
Epoch 139 loss is 0.17726155223340453
Epoch 149 loss is 0.20668125270168672
Epoch 159 loss is 0.18338413211908833
Epoch 169 loss is 0.19277969415818375
Epoch 179 loss is 0.16932003069773488
Epoch 189 loss is 0.17114803008451324
Epoch 199 loss is 0.16234674302244245
Train Acc.:  0.872796803896853
              precision    recall  f1-score   support

           0       0.96      0.95      0.96      1063
           1       0.87      0.86      0.87      1064
           2       0.97      0.92      0.95      1064
           3       0.95      0.90      0.92      1063
           4       0.94      0.88      0.91      1064
           5       0.88      0.87      0.88      1064
           6       0.89      0.74      0.81      1064
           7       0.99      0.98      0.98      1064
           8       1.00      0.99      1.00      1064
           9       0.91      0.85      0.88      1064
          10       0.95      0.89      0.92      1064

   micro avg       0.94      0.89      0.92     11702
   macro avg       0.94      0.89      0.92     11702
weighted avg       0.94      0.89      0.92     11702
 samples avg       0.88      0.89      0.88     11702
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_1728/726702759.py:46: UserWarning:

To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9 loss is 0.13195299604718233
Epoch 19 loss is 0.09760398376517222
Epoch 29 loss is 0.09333988532269522
Epoch 39 loss is 0.08521124567492029
Epoch 49 loss is 0.09132043426181803
Epoch 59 loss is 0.09214216010663917
Epoch 69 loss is 0.0896809856068434
Epoch 79 loss is 0.09808253427637235
Epoch 89 loss is 0.1009467307297589
Epoch 99 loss is 0.1341353638423057
Epoch 109 loss is 0.2061791644495565
Epoch 119 loss is 0.26550000244909344
Epoch 129 loss is 0.26717282808322534
Epoch 139 loss is 0.246903110038921
Epoch 149 loss is 0.24215059780955484
Epoch 159 loss is 0.2196847823727955
Epoch 169 loss is 0.22964239756409188
Epoch 179 loss is 0.21456982879325057
Epoch 189 loss is 0.2175676527718779
Epoch 199 loss is 0.20209335012773158
Train Acc.:  0.9208665370564232
              precision    recall  f1-score   support

           0       0.97      0.96      0.97      1064
           1       0.93      0.88      0.91      1064
           2       0.98      0.95      0.96      1064
           3       0.98      0.96      0.97      1064
           4       0.92      0.89      0.91      1064
           5       0.91      0.91      0.91      1064
           6       0.89      0.88      0.88      1063
           7       1.00      0.99      1.00      1064
           8       1.00      0.99      1.00      1063
           9       0.91      0.90      0.91      1064
          10       0.97      0.94      0.95      1064

   micro avg       0.95      0.93      0.94     11702
   macro avg       0.95      0.93      0.94     11702
weighted avg       0.95      0.93      0.94     11702
 samples avg       0.92      0.93      0.92     11702
</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="co"># title = "MLMVN 48-100-100-11"</span></span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="co"># image_name = "results/MLMVN_48-100-100-11.png"</span></span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot_loss_acc_list(title, list_losses, list_scores, image_name)</span></span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a>plot_loss_acc_list(</span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"$\mathbb</span><span class="sc">{C}</span><span class="st">$: [48-100-100-11] "</span>,</span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>    list_losses,</span>
<span id="cb147-7"><a href="#cb147-7" aria-hidden="true" tabindex="-1"></a>    list_scores,</span>
<span id="cb147-8"><a href="#cb147-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mlmvn-48-100-100-11.png"</span>,</span>
<span id="cb147-9"><a href="#cb147-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="01_autass_multiple_runs_files/figure-html/cell-51-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>task.mark_completed()</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>task.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>