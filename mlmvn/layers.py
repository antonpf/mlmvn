# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_layers.ipynb.

# %% auto 0
__all__ = ['OutputLayer', 'OutputLayerFB', 'phase_activation', 'cmplx_phase_activation']

# %% ../nbs/00_layers.ipynb 3
import math
import torch
import torch.nn as nn
from torch.autograd import Function

# %% ../nbs/00_layers.ipynb 4
class OutputLayer(nn.Module):
    """ Custom Linear layer but mimics a standard linear layer """
    def __init__(self, size_in, size_out):
        super().__init__()
        self.size_in, self.size_out = size_in, size_out
        # weights = torch.Tensor(size_out, size_in)

        # initialize weights and biases
        weights = torch.randn(
            self.size_in, self.size_out, dtype=torch.cdouble
        ) / math.sqrt(self.size_in)
        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.
        
        bias = torch.unsqueeze(
            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0
        )
        self.bias = nn.Parameter(bias)

    def forward(self, x):
        return OutputLayerFB.apply(x, self.weights, self.bias)

class OutputLayerFB(Function):

    @staticmethod
    def forward(ctx, input, weights, bias):
        ctx.save_for_backward(input, weights, bias)
        w_times_x= torch.mm(input, weights)
        return torch.add(w_times_x, bias)
    
    @staticmethod
    def backward(ctx, grad_output):  
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        outputs = torch.ones(1, grad_output.size(1))
        grad_output = grad_output / (input.size(1) + 1)
        if ctx.needs_input_grad[0]:
            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T
            grad_input = grad_output.mm(cinv)
        if ctx.needs_input_grad[1]:
            x_pinv = torch.linalg.pinv(
                        torch.cat(
                            [torch.ones(1, input.size(0)), input.T[0:]]
                        )
                    ).T
            angle_pinv = x_pinv[1:, :]
            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(outputs))
            grad_weight = grad_weight * (-1)
        if bias is not None and ctx.needs_input_grad[2]:
            angle_pinv = x_pinv[0, :]
            grad_bias = (angle_pinv @ torch.div(grad_output, torch.abs(outputs))).unsqueeze(dim=0)
            grad_bias = grad_bias * (-1)

        return grad_input, grad_weight, grad_bias

# %% ../nbs/00_layers.ipynb 5
class phase_activation(Function):

    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input / torch.abs(input)
    
    @staticmethod
    def backward(ctx, grad_output):  
        return grad_output, None

class cmplx_phase_activation(nn.Module):
    """ Custom Linear layer but mimics a standard linear layer """
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return phase_activation.apply(x)
