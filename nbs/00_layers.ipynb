{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class FirstLayer(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(\n",
    "            weights\n",
    "        )  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = FirstLayerFB.apply(x, self.weights, self.bias)\n",
    "        x.register_hook(self._hook_fn)\n",
    "        return x\n",
    "\n",
    "    def _hook_fn(self, tensor):\n",
    "        self.grad_output = tensor\n",
    "\n",
    "\n",
    "class FirstLayerFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        w_times_x = torch.mm(input, weights)\n",
    "        output = torch.add(w_times_x, bias)\n",
    "        ctx.save_for_backward(input, weights, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        # output = torch.ones(1, grad_output.size(1))\n",
    "        # grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat([torch.ones(1, input.size(0)), input.T[0:]])\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (\n",
    "                angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            ).unsqueeze(dim=0)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class HiddenLayer(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(\n",
    "            weights\n",
    "        )  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = HiddenLayerFB.apply(x, self.weights, self.bias)\n",
    "        x.register_hook(self._hook_fn)\n",
    "        return x\n",
    "\n",
    "    def _hook_fn(self, tensor):\n",
    "        self.grad_output = tensor / (self.size_in + 1)\n",
    "\n",
    "\n",
    "class HiddenLayerFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        w_times_x = torch.mm(input, weights)\n",
    "        output = torch.add(w_times_x, bias)\n",
    "        ctx.save_for_backward(input, weights, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        # output = torch.ones(1, grad_output.size(1))\n",
    "        grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat([torch.ones(1, input.size(0)), input.T[0:]])\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (\n",
    "                angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            ).unsqueeze(dim=0)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class OutputLayer(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(\n",
    "            weights\n",
    "        )  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = OutputLayerFB.apply(x, self.weights, self.bias)\n",
    "        x.register_hook(self._hook_fn)\n",
    "        return x\n",
    "\n",
    "    def _hook_fn(self, tensor):\n",
    "        self.grad_output = tensor / (self.size_in + 1)\n",
    "        # self.grad_output = torch.ones(1, self.size_out)\n",
    "\n",
    "\n",
    "class OutputLayerFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        w_times_x = torch.mm(input, weights)\n",
    "        output = torch.add(w_times_x, bias)\n",
    "        ctx.save_for_backward(input, weights, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        # output = torch.ones(1, grad_output.size(1))\n",
    "        grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat([torch.ones(1, input.size(0)), input.T[0:]])\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (\n",
    "                angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            ).unsqueeze(dim=0)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class phase_activation(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input / torch.abs(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class cmplx_phase_activation(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return phase_activation.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DropoutFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, p):\n",
    "        # ctx.save_for_backward(input)\n",
    "        # return input / torch.abs(input)\n",
    "        binomial = torch.distributions.binomial.Binomial(probs=1 - p)\n",
    "        return input * binomial.sample(input.size()) * (1.0 / (1 - p))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class MyDropout(nn.Module):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super(MyDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\n",
    "                \"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p)\n",
    "            )\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.training:\n",
    "            return DropoutFB.apply(X, self.p)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class FirstLayerCplx(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(\n",
    "            weights\n",
    "        )  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return FirstLayerCplxFB.apply(x, self.weights, self.bias)\n",
    "\n",
    "\n",
    "class FirstLayerCplxFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        w_times_x = torch.mm(input, weights)\n",
    "        output = torch.add(w_times_x, bias)\n",
    "        ctx.save_for_backward(input, weights, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        # output = torch.ones(1, grad_output.size(1))\n",
    "        # grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat([torch.ones(1, input.size(0)), input.T[0:]])\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (\n",
    "                angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            ).unsqueeze(dim=0)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class HiddenLayerCplx(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(\n",
    "            weights\n",
    "        )  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return HiddenLayerCplxFB.apply(x, self.weights, self.bias)\n",
    "\n",
    "\n",
    "class HiddenLayerCplxFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        w_times_x = torch.mm(input, weights)\n",
    "        output = torch.add(w_times_x, bias)\n",
    "        ctx.save_for_backward(input, weights, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        # output = torch.ones(1, grad_output.size(1))\n",
    "        grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat([torch.ones(1, input.size(0)), input.T[0:]])\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (\n",
    "                angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            ).unsqueeze(dim=0)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class OutputLayerCplx(nn.Module):\n",
    "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(\n",
    "            weights\n",
    "        )  # nn.Parameter is a Tensor that's a module parameter.\n",
    "\n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return OutputLayerCplxFB.apply(x, self.weights, self.bias)\n",
    "\n",
    "\n",
    "class OutputLayerCplxFB(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        w_times_x = torch.mm(input, weights)\n",
    "        output = torch.add(w_times_x, bias)\n",
    "        ctx.save_for_backward(input, weights, bias, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias, output = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "\n",
    "        # output = torch.ones(1, grad_output.size(1))\n",
    "        grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat([torch.ones(1, input.size(0)), input.T[0:]])\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (\n",
    "                angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "            ).unsqueeze(dim=0)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
