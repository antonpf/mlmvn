{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OutputLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        \n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return OutputLayerFB.apply(x, self.weights, self.bias)\n",
    "\n",
    "class OutputLayerFB(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias):\n",
    "        ctx.save_for_backward(input, weights, bias)\n",
    "        w_times_x= torch.mm(input, weights)\n",
    "        return torch.add(w_times_x, bias)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  \n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        outputs = torch.ones(1, grad_output.size(1))\n",
    "        grad_output = grad_output / (input.size(1) + 1)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            cinv = (torch.conj(weight) / torch.square(torch.abs(weight))).T\n",
    "            grad_input = grad_output.mm(cinv)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                        torch.cat(\n",
    "                            [torch.ones(1, input.size(0)), input.T[0:]]\n",
    "                        )\n",
    "                    ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(outputs))\n",
    "            grad_weight = grad_weight * (-1)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            grad_bias = (angle_pinv @ torch.div(grad_output, torch.abs(outputs))).unsqueeze(dim=1)\n",
    "            grad_bias = grad_bias * (-1)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class phase_activation(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input / torch.abs(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  \n",
    "        return grad_output, None\n",
    "\n",
    "class cmplx_phase_activation(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return phase_activation.apply(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
