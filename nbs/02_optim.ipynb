{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class ECL(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=required,\n",
    "        momentum=0,\n",
    "        dampening=0,\n",
    "        weight_decay=0,\n",
    "        nesterov=False,\n",
    "        *,\n",
    "        maximize=False,\n",
    "        foreach: Optional[bool] = None\n",
    "    ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            dampening=dampening,\n",
    "            weight_decay=weight_decay,\n",
    "            nesterov=nesterov,\n",
    "            maximize=maximize,\n",
    "            foreach=foreach,\n",
    "        )\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(ECL, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"nesterov\", False)\n",
    "            group.setdefault(\"maximize\", False)\n",
    "            group.setdefault(\"foreach\", None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, inputs, layers, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            has_sparse_grad = False\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "                    if p.grad.is_sparse:\n",
    "                        has_sparse_grad = True\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if \"momentum_buffer\" not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state[\"momentum_buffer\"])\n",
    "\n",
    "            ecl(\n",
    "                params_with_grad,\n",
    "                d_p_list,\n",
    "                inputs,\n",
    "                layers,\n",
    "                momentum_buffer_list,\n",
    "                weight_decay=group[\"weight_decay\"],\n",
    "                momentum=group[\"momentum\"],\n",
    "                lr=group[\"lr\"],\n",
    "                dampening=group[\"dampening\"],\n",
    "                nesterov=group[\"nesterov\"],\n",
    "                maximize=group[\"maximize\"],\n",
    "                has_sparse_grad=has_sparse_grad,\n",
    "                foreach=group[\"foreach\"],\n",
    "            )\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state[\"momentum_buffer\"] = momentum_buffer\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def ecl(\n",
    "    params: List[Tensor],\n",
    "    d_p_list: List[Tensor],\n",
    "    inputs: Tensor,\n",
    "    layers: List,\n",
    "    momentum_buffer_list: List[Optional[Tensor]],\n",
    "    # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "    # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "    has_sparse_grad: bool = None,\n",
    "    foreach: bool = None,\n",
    "    *,\n",
    "    weight_decay: float,\n",
    "    momentum: float,\n",
    "    lr: float,\n",
    "    dampening: float,\n",
    "    nesterov: bool,\n",
    "    maximize: bool\n",
    "):\n",
    "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
    "\n",
    "    See :class:`~torch.optim.SGD` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    if foreach is None:\n",
    "        # Placeholder for more complex foreach logic to be added when value is not set\n",
    "        foreach = False\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError(\"torch.jit.script not supported with foreach optimizers\")\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_sgd\n",
    "    else:\n",
    "        func = _single_tensor_ecl\n",
    "\n",
    "    func(\n",
    "        params,\n",
    "        d_p_list,\n",
    "        inputs,\n",
    "        layers,\n",
    "        momentum_buffer_list,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=momentum,\n",
    "        lr=lr,\n",
    "        dampening=dampening,\n",
    "        nesterov=nesterov,\n",
    "        has_sparse_grad=has_sparse_grad,\n",
    "        maximize=maximize,\n",
    "    )\n",
    "\n",
    "\n",
    "def _single_tensor_ecl(\n",
    "    params: List[Tensor],\n",
    "    d_p_list: List[Tensor],\n",
    "    inputs: Tensor,\n",
    "    layers: List,\n",
    "    momentum_buffer_list: List[Optional[Tensor]],\n",
    "    *,\n",
    "    weight_decay: float,\n",
    "    momentum: float,\n",
    "    lr: float,\n",
    "    dampening: float,\n",
    "    nesterov: bool,\n",
    "    maximize: bool,\n",
    "    has_sparse_grad: bool\n",
    "):\n",
    "\n",
    "    toggle = True\n",
    "    input_layer = []\n",
    "    input_layer.append(inputs)\n",
    "    output = inputs\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        if toggle == True:\n",
    "            x_pinv = torch.linalg.pinv(\n",
    "                torch.cat(\n",
    "                    [torch.ones(1, input_layer[-1].size(0)), input_layer[-1].T[0:]]\n",
    "                )\n",
    "            ).T\n",
    "            angle_pinv = x_pinv[1:, :]\n",
    "            if i == len(params) - 2:\n",
    "                grad_weight = angle_pinv @ torch.div(\n",
    "                    layers[i].grad_output, torch.abs(torch.ones(1, 1))\n",
    "                )\n",
    "            else:\n",
    "                w_times_x = torch.mm(input_layer[-1], param)\n",
    "                output = torch.add(w_times_x, params[i + 1])\n",
    "                grad_weight = angle_pinv @ torch.div(\n",
    "                    layers[i].grad_output, torch.abs(output)\n",
    "                )\n",
    "            d_p = grad_weight\n",
    "        else:\n",
    "            angle_pinv = x_pinv[0, :]\n",
    "            if i == len(params) - 1:\n",
    "                grad_bias = (\n",
    "                    angle_pinv\n",
    "                    @ torch.div(layers[i - 1].grad_output, torch.abs(torch.ones(1, 1)))\n",
    "                ).unsqueeze(dim=0)\n",
    "            else:\n",
    "                grad_bias = (\n",
    "                    angle_pinv @ torch.div(layers[i - 1].grad_output, torch.abs(output))\n",
    "                ).unsqueeze(dim=0)\n",
    "            d_p = grad_bias\n",
    "\n",
    "        #\n",
    "        # grad_weight = angle_pinv @ torch.div(model.first_linear.grad_output, torch.abs(output))\n",
    "        # grad_weight = grad_weight * (-1)\n",
    "\n",
    "        # grad_output = grad_dict['fc1'][\"grad_output\"][0][0]\n",
    "        # grad_weight = angle_pinv @ torch.div(grad_output, torch.abs(output))\n",
    "\n",
    "        # angle_pinv = x_pinv[0, :]\n",
    "        # grad_bias = (angle_pinv @ torch.div(grad_output, torch.abs(output))).unsqueeze(dim=0)\n",
    "        # grad_bias = grad_bias * (-1)\n",
    "\n",
    "        # d_p = d_p_list[i]\n",
    "        if weight_decay != 0:\n",
    "            d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(d_p).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                d_p = d_p.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                d_p = buf\n",
    "\n",
    "        alpha = lr if maximize else -lr\n",
    "        param.add_(d_p)\n",
    "\n",
    "        if toggle == False and not (i == len(params) - 1):\n",
    "            # Update Output with new param\n",
    "            if i == len(params) - 3:\n",
    "                # output = torch.ones(1, param.shape[0])\n",
    "                w_times_x = torch.mm(input_layer[-1], params[i - 1])\n",
    "                output = torch.add(w_times_x, param)\n",
    "                output = output / torch.abs(output)\n",
    "                input_layer.append(output)\n",
    "            else:\n",
    "                w_times_x = torch.mm(input_layer[-1], params[i - 1])\n",
    "                output = torch.add(w_times_x, param)\n",
    "                output = output / torch.abs(output)\n",
    "                input_layer.append(output)\n",
    "        toggle = not toggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class MySGD(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=required,\n",
    "        momentum=0,\n",
    "        dampening=0,\n",
    "        weight_decay=0,\n",
    "        nesterov=False,\n",
    "        *,\n",
    "        maximize=False,\n",
    "        foreach: Optional[bool] = None\n",
    "    ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            dampening=dampening,\n",
    "            weight_decay=weight_decay,\n",
    "            nesterov=nesterov,\n",
    "            maximize=maximize,\n",
    "            foreach=foreach,\n",
    "        )\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(MySGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"nesterov\", False)\n",
    "            group.setdefault(\"maximize\", False)\n",
    "            group.setdefault(\"foreach\", None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            has_sparse_grad = False\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "                    if p.grad.is_sparse:\n",
    "                        has_sparse_grad = True\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if \"momentum_buffer\" not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state[\"momentum_buffer\"])\n",
    "\n",
    "            sgd(\n",
    "                params_with_grad,\n",
    "                d_p_list,\n",
    "                momentum_buffer_list,\n",
    "                weight_decay=group[\"weight_decay\"],\n",
    "                momentum=group[\"momentum\"],\n",
    "                lr=group[\"lr\"],\n",
    "                dampening=group[\"dampening\"],\n",
    "                nesterov=group[\"nesterov\"],\n",
    "                maximize=group[\"maximize\"],\n",
    "                has_sparse_grad=has_sparse_grad,\n",
    "                foreach=group[\"foreach\"],\n",
    "            )\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state[\"momentum_buffer\"] = momentum_buffer\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def sgd(\n",
    "    params: List[Tensor],\n",
    "    d_p_list: List[Tensor],\n",
    "    momentum_buffer_list: List[Optional[Tensor]],\n",
    "    # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "    # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "    has_sparse_grad: bool = None,\n",
    "    foreach: bool = None,\n",
    "    *,\n",
    "    weight_decay: float,\n",
    "    momentum: float,\n",
    "    lr: float,\n",
    "    dampening: float,\n",
    "    nesterov: bool,\n",
    "    maximize: bool\n",
    "):\n",
    "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
    "\n",
    "    See :class:`~torch.optim.SGD` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    if foreach is None:\n",
    "        # Placeholder for more complex foreach logic to be added when value is not set\n",
    "        foreach = False\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError(\"torch.jit.script not supported with foreach optimizers\")\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_sgd\n",
    "    else:\n",
    "        func = _single_tensor_sgd\n",
    "\n",
    "    func(\n",
    "        params,\n",
    "        d_p_list,\n",
    "        momentum_buffer_list,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=momentum,\n",
    "        lr=lr,\n",
    "        dampening=dampening,\n",
    "        nesterov=nesterov,\n",
    "        has_sparse_grad=has_sparse_grad,\n",
    "        maximize=maximize,\n",
    "    )\n",
    "\n",
    "\n",
    "def _single_tensor_sgd(\n",
    "    params: List[Tensor],\n",
    "    d_p_list: List[Tensor],\n",
    "    momentum_buffer_list: List[Optional[Tensor]],\n",
    "    *,\n",
    "    weight_decay: float,\n",
    "    momentum: float,\n",
    "    lr: float,\n",
    "    dampening: float,\n",
    "    nesterov: bool,\n",
    "    maximize: bool,\n",
    "    has_sparse_grad: bool\n",
    "):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        d_p = d_p_list[i]\n",
    "        if weight_decay != 0:\n",
    "            d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(d_p).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                d_p = d_p.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                d_p = buf\n",
    "\n",
    "        alpha = lr if maximize else -lr\n",
    "        param.add_(d_p, alpha=alpha)\n",
    "\n",
    "\n",
    "def _multi_tensor_sgd(\n",
    "    params: List[Tensor],\n",
    "    grads: List[Tensor],\n",
    "    momentum_buffer_list: List[Optional[Tensor]],\n",
    "    *,\n",
    "    weight_decay: float,\n",
    "    momentum: float,\n",
    "    lr: float,\n",
    "    dampening: float,\n",
    "    nesterov: bool,\n",
    "    maximize: bool,\n",
    "    has_sparse_grad: bool\n",
    "):\n",
    "\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    if has_sparse_grad is None:\n",
    "        has_sparse_grad = any([grad.is_sparse for grad in grads])\n",
    "\n",
    "    if weight_decay != 0:\n",
    "        grads = torch._foreach_add(grads, params, alpha=weight_decay)\n",
    "\n",
    "    if momentum != 0:\n",
    "        bufs = []\n",
    "\n",
    "        all_states_with_momentum_buffer = True\n",
    "        for i in range(len(momentum_buffer_list)):\n",
    "            if momentum_buffer_list[i] is None:\n",
    "                all_states_with_momentum_buffer = False\n",
    "                break\n",
    "            else:\n",
    "                bufs.append(momentum_buffer_list[i])\n",
    "\n",
    "        if all_states_with_momentum_buffer:\n",
    "            torch._foreach_mul_(bufs, momentum)\n",
    "            torch._foreach_add_(bufs, grads, alpha=1 - dampening)\n",
    "        else:\n",
    "            bufs = []\n",
    "            for i in range(len(momentum_buffer_list)):\n",
    "                if momentum_buffer_list[i] is None:\n",
    "                    buf = momentum_buffer_list[i] = torch.clone(grads[i]).detach()\n",
    "                else:\n",
    "                    buf = momentum_buffer_list[i]\n",
    "                    buf.mul_(momentum).add_(grads[i], alpha=1 - dampening)\n",
    "\n",
    "                bufs.append(buf)\n",
    "\n",
    "        if nesterov:\n",
    "            torch._foreach_add_(grads, bufs, alpha=momentum)\n",
    "        else:\n",
    "            grads = bufs\n",
    "\n",
    "    alpha = lr if maximize else -lr\n",
    "    if not has_sparse_grad:\n",
    "        torch._foreach_add_(params, grads, alpha=alpha)\n",
    "    else:\n",
    "        # foreach APIs dont support sparse\n",
    "        for i in range(len(params)):\n",
    "            params[i].add_(grads[i], alpha=alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
