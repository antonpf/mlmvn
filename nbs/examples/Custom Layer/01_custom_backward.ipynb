{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard PyTorch MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 953.6526489257812\n",
      "epoch 1, loss 910.0452880859375\n",
      "epoch 2, loss 879.88330078125\n",
      "epoch 3, loss 851.927490234375\n",
      "epoch 4, loss 823.8736572265625\n",
      "epoch 5, loss 793.564697265625\n",
      "epoch 6, loss 756.58740234375\n",
      "epoch 7, loss 703.5147094726562\n",
      "epoch 8, loss 616.2340087890625\n",
      "epoch 9, loss 469.29998779296875\n",
      "epoch 10, loss 259.6634826660156\n",
      "epoch 11, loss 72.42332458496094\n",
      "epoch 12, loss 7.541385650634766\n",
      "epoch 13, loss 1.3356682062149048\n",
      "epoch 14, loss 0.3731410503387451\n",
      "epoch 15, loss 0.15019331872463226\n",
      "epoch 16, loss 0.09882812201976776\n",
      "epoch 17, loss 0.08701667189598083\n",
      "epoch 18, loss 0.08430331945419312\n",
      "epoch 19, loss 0.08368028700351715\n",
      "epoch 20, loss 0.08353688567876816\n",
      "epoch 21, loss 0.08353271335363388\n",
      "epoch 22, loss 0.08352876454591751\n",
      "epoch 23, loss 0.08352525532245636\n",
      "epoch 24, loss 0.08352211862802505\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's generate some fake data\n",
    "torch.manual_seed(42)\n",
    "resid = torch.rand(100)    \n",
    "inputs = torch.tensor([ [ xx ] for xx in range(100)] , dtype=torch.float32)\n",
    "labels = torch.tensor([ (2 + 0.5*yy + resid[yy]) for yy in range(100)], dtype=torch.float32)\n",
    "\n",
    "# Now we define a linear regression model\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.bn = torch.nn.BatchNorm1d(num_features=1)\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, inx):\n",
    "        x = self.bn(inx) # Adding BN to standardize input helps us use a higher learning rate\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "model = linearRegression(1, 1)     \n",
    "\n",
    "# Using the standard mse_loss of PyTorch\n",
    "epochs = 25    \n",
    "mseloss = F.mse_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = mseloss(outputs.view(-1), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()        \n",
    "    print(f'epoch {epoch}, loss {loss}')    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss function, but relying on PyTorch's automatic gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 994.3351440429688\n",
      "epoch 1, loss 955.7775268554688\n",
      "epoch 2, loss 918.1871337890625\n",
      "epoch 3, loss 871.7307739257812\n",
      "epoch 4, loss 801.802978515625\n",
      "epoch 5, loss 680.582275390625\n",
      "epoch 6, loss 472.83929443359375\n",
      "epoch 7, loss 200.48443603515625\n",
      "epoch 8, loss 26.085895538330078\n",
      "epoch 9, loss 1.3690561056137085\n",
      "epoch 10, loss 0.33454182744026184\n",
      "epoch 11, loss 0.1325385421514511\n",
      "epoch 12, loss 0.09308631718158722\n",
      "epoch 13, loss 0.08537135273218155\n",
      "epoch 14, loss 0.0838615894317627\n",
      "epoch 15, loss 0.08356618881225586\n",
      "epoch 16, loss 0.0835084542632103\n",
      "epoch 17, loss 0.08349698781967163\n",
      "epoch 18, loss 0.08349480479955673\n",
      "epoch 19, loss 0.08349443227052689\n",
      "epoch 20, loss 0.08349420875310898\n",
      "epoch 21, loss 0.08349428325891495\n",
      "epoch 22, loss 0.08349429816007614\n",
      "epoch 23, loss 0.0834941565990448\n",
      "epoch 24, loss 0.08349429816007614\n"
     ]
    }
   ],
   "source": [
    "#######################################################3\n",
    "class MyMSELoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):        \n",
    "        tmp = (inputs-targets)**2\n",
    "        loss =  torch.mean(tmp)        \n",
    "        return loss\n",
    "#######################################################3\n",
    "\n",
    "model = linearRegression(1, 1) \n",
    "    \n",
    "mseloss = MyMSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()    \n",
    "    outputs = model(inputs)\n",
    "    loss = mseloss(outputs.view(-1), labels)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'epoch {epoch}, loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss function with my own backward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "#######################################################\n",
    "class MyMSELoss(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, y_pred, y):    \n",
    "        ctx.save_for_backward(y_pred, y)\n",
    "        return ( (y - y_pred)**2 ).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y_pred, y = ctx.saved_tensors\n",
    "        grad_input = 2 * (y_pred - y) / y_pred.shape[0]        \n",
    "        test()\n",
    "        return grad_input, None\n",
    "\n",
    "    def test(ctx):\n",
    "        pass\n",
    "    \n",
    "#######################################################\n",
    "    \n",
    "model = linearRegression(1, 1) \n",
    "mseloss = MyMSELoss.apply\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "outputs = model(inputs)\n",
    "loss = mseloss(outputs.view(-1), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1005.8683, grad_fn=<MyMSELossBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyMSELossBackward' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom Layer/01_custom_backward.ipynb Zelle 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/01_custom_backward.ipynb#ch0000012?line=0'>1</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "\u001b[1;32m/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom Layer/01_custom_backward.ipynb Zelle 9\u001b[0m in \u001b[0;36mMyMSELoss.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/01_custom_backward.ipynb#ch0000012?line=12'>13</a>\u001b[0m y_pred, y \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39msaved_tensors\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/01_custom_backward.ipynb#ch0000012?line=13'>14</a>\u001b[0m grad_input \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (y_pred \u001b[39m-\u001b[39m y) \u001b[39m/\u001b[39m y_pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]        \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/01_custom_backward.ipynb#ch0000012?line=14'>15</a>\u001b[0m ctx\u001b[39m.\u001b[39;49mtest()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/01_custom_backward.ipynb#ch0000012?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m grad_input, \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyMSELossBackward' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.08349424600601196\n",
      "epoch 1, loss 0.08349424600601196\n",
      "epoch 2, loss 0.08349424600601196\n",
      "epoch 3, loss 0.08349424600601196\n",
      "epoch 4, loss 0.08349424600601196\n",
      "epoch 5, loss 0.08349424600601196\n",
      "epoch 6, loss 0.08349424600601196\n",
      "epoch 7, loss 0.08349424600601196\n",
      "epoch 8, loss 0.08349424600601196\n",
      "epoch 9, loss 0.08349424600601196\n",
      "epoch 10, loss 0.08349424600601196\n",
      "epoch 11, loss 0.08349424600601196\n",
      "epoch 12, loss 0.08349424600601196\n",
      "epoch 13, loss 0.08349424600601196\n",
      "epoch 14, loss 0.08349424600601196\n",
      "epoch 15, loss 0.08349424600601196\n",
      "epoch 16, loss 0.08349424600601196\n",
      "epoch 17, loss 0.08349424600601196\n",
      "epoch 18, loss 0.08349424600601196\n",
      "epoch 19, loss 0.08349424600601196\n",
      "epoch 20, loss 0.08349424600601196\n",
      "epoch 21, loss 0.08349424600601196\n",
      "epoch 22, loss 0.08349424600601196\n",
      "epoch 23, loss 0.08349424600601196\n",
      "epoch 24, loss 0.08349424600601196\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = mseloss(outputs.view(-1), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    print(f'epoch {epoch}, loss {loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
