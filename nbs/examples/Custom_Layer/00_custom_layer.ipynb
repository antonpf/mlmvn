{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Demonstrates the easy of integration of a custom layer \"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        # weights = torch.Tensor(size_out, size_in)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        weights = torch.randn(\n",
    "            self.size_in, self.size_out, dtype=torch.cdouble\n",
    "        ) / math.sqrt(self.size_in)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        \n",
    "        bias = torch.unsqueeze(\n",
    "            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0\n",
    "        )\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x= torch.mm(x, self.weights)\n",
    "        return torch.add(w_times_x, self.bias)  # w times x + b\n",
    "        #return x @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = MyLinearLayer(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  #  for repeatable results\n",
    "model = BasicModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7705-0.1467j],\n",
       "        [-1.0894+0.2842j]], dtype=torch.complex128, requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[0., 0.],\n",
    "               [0., 1.],\n",
    "               [1., 0.],\n",
    "               [1., 1.]])\n",
    "\n",
    "y = torch.Tensor([0., 1., 1., 0.]).reshape(x.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward computation thru model: tensor([[ 0.0000+0.0000j],\n",
      "        [-1.0894+0.2842j],\n",
      "        [ 0.7705-0.1467j],\n",
      "        [-0.3189+0.1375j]], dtype=torch.complex128, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = x.type(torch.cdouble)\n",
    "print('Forward computation thru model:', model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward computation thru model: tensor([[ 0.0000+0.0000j],\n",
      "        [-1.0894+0.2842j],\n",
      "        [ 0.7705-0.1467j],\n",
      "        [-0.3189+0.1375j]], dtype=torch.complex128, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Forward computation thru model:', model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class Complex_RMSE(nn.Module):\n",
    "    def __init__(self, categories: int = 2, periodicity: int = 2):\n",
    "        super(Complex_RMSE, self).__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "\n",
    "    def forward(self, inputs, targets): \n",
    "        target_angles = self.class2angle(targets)\n",
    "        predicted_angles = torch.remainder(inputs.angle() + 2 * np.pi, 2 * np.pi)\n",
    "        # z = torch.abs(predicted) * torch.exp(1.j * predicted_angles)\n",
    "        # error calculation\n",
    "        errors = torch.exp(1.0j * target_angles) - torch.exp(1.0j * predicted_angles)\n",
    "        if self.periodicity > 1:\n",
    "            # select smallest error\n",
    "            idx = torch.argmin(torch.abs(errors), dim=1, keepdim=True)\n",
    "            return errors.gather(1, idx).mean()\n",
    "        else:\n",
    "            return errors.mean()\n",
    "\n",
    "    def class2angle(self, actual: torch.tensor) -> torch.tensor:\n",
    "        # Returns a new tensor with an extra dimension\n",
    "        if actual.size().__len__() == 1:\n",
    "            actual = torch.unsqueeze(actual, 1)\n",
    "\n",
    "        # Class to several angles due to periodicity using bisector\n",
    "        return (\n",
    "            (self.categories * torch.arange(self.periodicity) + actual + 0.5)\n",
    "            / (self.categories * self.periodicity)\n",
    "            * 2\n",
    "            * np.pi\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "#######################################################\n",
    "class MyComplexRMSELoss(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, y_pred, y):\n",
    "        target_angles = class2angle(y)\n",
    "        predicted_angles = torch.remainder(inputs.angle() + 2 * np.pi, 2 * np.pi)\n",
    "        ctx.save_for_backward(y_pred, y)\n",
    "        return ( (y - y_pred)**2 ).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y_pred, y = ctx.saved_tensors\n",
    "        grad_input = 2 * (y_pred - y) / y_pred.shape[0]        \n",
    "        return grad_input, None\n",
    "\n",
    "    def class2angle(self, actual: torch.tensor) -> torch.tensor:\n",
    "        # Returns a new tensor with an extra dimension\n",
    "        if actual.size().__len__() == 1:\n",
    "            actual = torch.unsqueeze(actual, 1)\n",
    "\n",
    "        # Class to several angles due to periodicity using bisector\n",
    "        return (\n",
    "            (self.categories * torch.arange(self.periodicity) + actual + 0.5)\n",
    "            / (self.categories * self.periodicity)\n",
    "            * 2\n",
    "            * np.pi\n",
    "        )\n",
    "    \n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "# criterion = MSEC(categories=2, periodicity=2)\n",
    "criterion = Complex_RMSE(categories=2, periodicity=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000+0.0000j],\n",
       "        [-1.0894+0.2842j],\n",
       "        [ 0.7705-0.1467j],\n",
       "        [-0.3189+0.1375j]], dtype=torch.complex128, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0241-0.1153j, dtype=torch.complex128, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 tensor(0.2840, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "199 tensor(0.2838, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "299 tensor(0.2836, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "399 tensor(0.2834, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "499 tensor(0.2833, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "599 tensor(0.2831, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "699 tensor(0.2829, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "799 tensor(0.2827, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "899 tensor(0.2825, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "999 tensor(0.2824, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1099 tensor(0.2822, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1199 tensor(0.2820, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1299 tensor(0.2818, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1399 tensor(0.2817, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1499 tensor(0.2815, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1599 tensor(0.2813, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1699 tensor(0.2811, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1799 tensor(0.2810, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1899 tensor(0.2808, dtype=torch.float64, grad_fn=<PowBackward0>)\n",
      "1999 tensor(0.2806, dtype=torch.float64, grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BasicModel' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom Layer/00_custom_layer.ipynb Zelle 18\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/00_custom_layer.ipynb#ch0000017?line=15'>16</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/00_custom_layer.ipynb#ch0000017?line=16'>17</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/00_custom_layer.ipynb#ch0000017?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mResult: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mstring()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BasicModel' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        # print(t, torch.abs(loss))\n",
    "        print(t, torch.square(\n",
    "                np.pi - torch.abs(torch.abs(loss) - np.pi)\n",
    "            ))\n",
    "        \n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward computation thru model: tensor([[ 1.2722, -0.6224]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Demonstrates the easy of integration of a custom layer \"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class MyLinearLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, size_in, size_out):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in)\n",
    "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
    "        bias = torch.Tensor(size_out)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "\n",
    "        # initialize weights and biases\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x= torch.mm(x, self.weights.t())\n",
    "        return torch.add(w_times_x, self.bias)  # w times x + b\n",
    "\n",
    "\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 128, 3)\n",
    "        # self.linear = nn.Linear(256, 2)\n",
    "        self.linear = MyLinearLayer(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self. conv(x)\n",
    "        x = x.view(-1, 256)\n",
    "        return self.linear(x)\n",
    "\n",
    "torch.manual_seed(0)  #  for repeatable results\n",
    "basic_model = BasicModel()\n",
    "inp = np.array([[[[1,2,3,4],  # batch(=1) x channels(=1) x height x width\n",
    "                  [1,2,3,4],\n",
    "                  [1,2,3,4]]]])\n",
    "x = torch.tensor(inp, dtype=torch.float)\n",
    "print('Forward computation thru model:', basic_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(basic_model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = basic_model(x)\n",
    "# Compute and print loss\n",
    "loss = criterion(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2722, -0.6224]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom Layer/00_custom_layer.ipynb Zelle 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anton/Documents/Dev/Python/mlmvn/nbs/examples/Custom%20Layer/00_custom_layer.ipynb#ch0000026?line=0'>1</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    163\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    165\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 166\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
