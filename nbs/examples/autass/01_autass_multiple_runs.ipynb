{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensorless Drive Diagnosis\n",
    "\n",
    "> In this example, the main focus is the classification of individual states of a motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlmvn.layers import FirstLayer, HiddenLayer, OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss\n",
    "from mlmvn.optim import MySGD, ECL\n",
    "from pathlib import Path\n",
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# --- helper functions ---\n",
    "def reverse_one_hot(x, neuronCats):\n",
    "    a = np.zeros(len(x))\n",
    "    x = torch.detach(x)\n",
    "    for i in range(len(x)):\n",
    "        a[i] = torch.max(x[i]) - 1 + np.argmax(x[i]) * neuronCats\n",
    "    return a\n",
    "\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    out = out.type(torch.double)\n",
    "    yb = yb.type(torch.double)\n",
    "    x = 0\n",
    "    for i in range(len(out)):\n",
    "        x += torch.equal(out[i], yb[i])\n",
    "    return x / len(out)\n",
    "\n",
    "\n",
    "def prepare_data(x_train, x_valid, y_train, y_valid, neuronCats):\n",
    "    # one-hot encoding\n",
    "    numSamples, numFeatures = x_valid.shape\n",
    "    y_valid_int = y_valid\n",
    "    y2 = y_valid + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    numClasses = max(y2)\n",
    "    target_ids = range(numClasses)\n",
    "    no = int(np.ceil(numClasses / neuronCats))  # number of output neurons\n",
    "    if no != 1:\n",
    "        y_valid = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_valid[i, k] = c\n",
    "    numSamples, numFeatures = x_train.shape\n",
    "    y_train_int = y_train\n",
    "    y2 = y_train + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    if no != 1:\n",
    "        y_train = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_train[i, k] = c\n",
    "    del y2\n",
    "\n",
    "    # Convert numpy arrays into torch tensors\n",
    "    x_train, y_train, x_valid, y_valid = map(\n",
    "        torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "    )\n",
    "    if y_train.size().__len__() == 1:\n",
    "        y_train = torch.unsqueeze(y_train, 1)\n",
    "        y_valid = torch.unsqueeze(y_valid, 1)\n",
    "\n",
    "    # convert angles to complex numbers on unit-circle\n",
    "    x_train = torch.exp(1.0j * x_train)\n",
    "    x_valid = torch.exp(1.0j * x_valid)\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data(X, y, neuronCats):\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=46806, random_state=42\n",
    "    )\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data_by_index(X, y, neuronCats, train_index, test_index):\n",
    "    x_train, x_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# --- Plots ---\n",
    "def plot_loss(title, losses, scores):\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    fig.suptitle(\"CVNN - Moons\")\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.legend([\"Acc\", \"Loss\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weights(title, ylabel_1, ylabel_2, weights_real, weights_imag):\n",
    "    # y_min = np.min([np.min(weights_real), np.min(weights_imag)])\n",
    "    # y_max = np.max([np.max(weights_real), np.max(weights_imag)])\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(14, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax[0].plot(np.linspace(1, len(weights_real), len(weights_real)), weights_real)\n",
    "    ax[0].set_xlabel(\"Step\")\n",
    "    ax[0].set_ylabel(ylabel_1)\n",
    "    # ax[0].set_title(\"Real Valued Weigts\")\n",
    "    ax[0].set_xlim(0, len(weights_real))\n",
    "    # ax[0].set_ylim(y_min, y_max)\n",
    "\n",
    "    ax[1].plot(np.linspace(1, len(weights_imag), len(weights_imag)), weights_imag)\n",
    "    ax[1].set_xlabel(\"Step\")\n",
    "    ax[1].set_ylabel(ylabel_2)\n",
    "    # ax[1].set_title(\"Imaginary Valued Weights\")\n",
    "    ax[1].set_xlim(0, len(weights_imag))\n",
    "    # ax[1].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_acc_list(title, list_losses, list_scores, image_name):\n",
    "    losses = np.mean(list_losses, axis=0)\n",
    "    scores = np.mean(list_scores, axis=0)\n",
    "\n",
    "    losses_std = np.std(list_losses, axis=0)\n",
    "    scores_std = np.std(list_scores, axis=0)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(losses), len(losses)),\n",
    "        losses + losses_std,\n",
    "        losses - losses_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(scores), len(scores)),\n",
    "        scores + scores_std,\n",
    "        scores - scores_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.legend([\"Loss Mean\", \"Loss Std\", \"Acc. Mean\", \"Acc. Std\"])\n",
    "    plt.grid(visible=True)\n",
    "    fig.savefig(image_name, format=\"png\", dpi=600)\n",
    "\n",
    "    plt.show()\n",
    "    # save\n",
    "    # fig.savefig(image_name + \".svg\", format=\"svg\", dpi=600)\n",
    "\n",
    "\n",
    "# --- Logging ---\n",
    "model_dict: dict = {}\n",
    "\n",
    "\n",
    "def fc_hook(layer_name, module, grad_input, grad_output):\n",
    "    if layer_name in model_dict:\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output\n",
    "    else:\n",
    "        model_dict[layer_name] = {}\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# control variables\n",
    "# number of categories a neuron can distinguish / parameter that determines the number of output neurons\n",
    "neuronCats = 1\n",
    "# number of categories per neuron, i.e. neuronCats (+ 1 for others in case of multiple Outputs)\n",
    "categories = 2\n",
    "# how often a classification sector occurs (1 means no periodicity)\n",
    "periodicity = 1\n",
    "# path to store best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\n",
    "    \"data/autass_data2.csv\",\n",
    "    header=None,\n",
    "    dtype=np.double,\n",
    ")\n",
    "data = np.array(train_csv.values[:, 1:50])\n",
    "del train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0:48]\n",
    "y = data[:, 48].astype(int) - 1\n",
    "\n",
    "yt = copy.copy(y)\n",
    "yt[yt == 0] = 20\n",
    "yt[yt == 1] = 21\n",
    "yt[yt == 2] = 22\n",
    "yt[yt == 3] = 23\n",
    "yt[yt == 4] = 26\n",
    "yt[yt == 5] = 24\n",
    "yt[yt == 6] = 27\n",
    "yt[yt == 7] = 29\n",
    "yt[yt == 8] = 30\n",
    "yt[yt == 9] = 25\n",
    "yt[yt == 10] = 28\n",
    "yt -= 20\n",
    "y = yt\n",
    "del yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 538\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-10-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 10)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(10, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=6bcfc0d4690e42f4a3cbe67fce6889b9\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6bcfc0d4690e42f4a3cbe67fce6889b9/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 5,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-10-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-10-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-10-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_932/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 10:07:23,675 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\n",
      "Train Acc.:  0.7186745572243468\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      1063\n",
      "           1       0.89      0.64      0.75      1064\n",
      "           2       0.90      0.74      0.81      1064\n",
      "           3       0.91      0.81      0.86      1064\n",
      "           4       0.76      0.62      0.68      1064\n",
      "           5       0.72      0.90      0.80      1063\n",
      "           6       0.68      0.73      0.70      1064\n",
      "           7       0.97      0.96      0.97      1064\n",
      "           8       0.99      0.99      0.99      1064\n",
      "           9       0.77      0.77      0.77      1064\n",
      "          10       0.87      0.69      0.77      1064\n",
      "\n",
      "   micro avg       0.84      0.80      0.82     11702\n",
      "   macro avg       0.85      0.80      0.82     11702\n",
      "weighted avg       0.85      0.80      0.82     11702\n",
      " samples avg       0.76      0.80      0.77     11702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_932/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc.:  0.7787296771850364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92      1063\n",
      "           1       0.89      0.59      0.71      1064\n",
      "           2       0.94      0.87      0.90      1064\n",
      "           3       0.92      0.86      0.89      1063\n",
      "           4       0.77      0.76      0.76      1064\n",
      "           5       0.79      0.82      0.81      1064\n",
      "           6       0.72      0.83      0.77      1064\n",
      "           7       0.99      0.96      0.98      1064\n",
      "           8       1.00      0.95      0.97      1064\n",
      "           9       0.86      0.86      0.86      1064\n",
      "          10       0.84      0.78      0.81      1064\n",
      "\n",
      "   micro avg       0.88      0.83      0.85     11702\n",
      "   macro avg       0.88      0.83      0.85     11702\n",
      "weighted avg       0.88      0.83      0.85     11702\n",
      " samples avg       0.80      0.83      0.81     11702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_932/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc.:  0.7650992372935672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      1064\n",
      "           1       0.90      0.58      0.70      1064\n",
      "           2       0.95      0.84      0.89      1064\n",
      "           3       0.90      0.79      0.84      1063\n",
      "           4       0.87      0.59      0.71      1064\n",
      "           5       0.80      0.83      0.82      1064\n",
      "           6       0.78      0.70      0.74      1063\n",
      "           7       0.98      0.93      0.96      1064\n",
      "           8       1.00      0.97      0.98      1064\n",
      "           9       0.82      0.85      0.83      1064\n",
      "          10       0.94      0.72      0.81      1064\n",
      "\n",
      "   micro avg       0.89      0.79      0.84     11702\n",
      "   macro avg       0.89      0.79      0.83     11702\n",
      "weighted avg       0.89      0.79      0.83     11702\n",
      " samples avg       0.78      0.79      0.78     11702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_932/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc.:  0.678146431089367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89      1063\n",
      "           1       0.81      0.18      0.29      1064\n",
      "           2       0.82      0.64      0.72      1064\n",
      "           3       0.94      0.83      0.88      1063\n",
      "           4       0.72      0.59      0.65      1064\n",
      "           5       0.77      0.82      0.79      1064\n",
      "           6       0.94      0.30      0.46      1064\n",
      "           7       0.97      0.99      0.98      1064\n",
      "           8       0.97      0.92      0.95      1064\n",
      "           9       0.68      0.96      0.80      1064\n",
      "          10       0.81      0.70      0.75      1064\n",
      "\n",
      "   micro avg       0.83      0.72      0.77     11702\n",
      "   macro avg       0.84      0.72      0.74     11702\n",
      "weighted avg       0.84      0.72      0.74     11702\n",
      " samples avg       0.69      0.72      0.70     11702\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_932/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc.:  0.6857521310915035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      1064\n",
      "           1       0.90      0.08      0.14      1064\n",
      "           2       0.75      0.85      0.80      1064\n",
      "           3       0.93      0.80      0.86      1064\n",
      "           4       0.77      0.55      0.64      1064\n",
      "           5       0.53      0.92      0.68      1064\n",
      "           6       0.78      0.58      0.67      1063\n",
      "           7       0.98      0.97      0.98      1064\n",
      "           8       0.98      0.85      0.91      1063\n",
      "           9       0.84      0.82      0.83      1064\n",
      "          10       0.74      0.77      0.75      1064\n",
      "\n",
      "   micro avg       0.80      0.73      0.77     11702\n",
      "   macro avg       0.83      0.73      0.74     11702\n",
      "weighted avg       0.83      0.73      0.74     11702\n",
      " samples avg       0.71      0.73      0.72     11702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAFECAYAAADvBkssAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqVUlEQVR4nO3deXwTZeIG8GcmZ+9SjrZAOUTuSwSPynIoUIFVEWTFg7uoWGFFfoq4unKI4snWC4S1UERURITFFZEuWkCQQwRFTuUWCqUcvZsmmfn9kSbkbpImTRqe7+cTm8y8M/PmZcB5+r7zjiDLsgwiIiIiIqIwIga7AkRERERERP7GoENERERERGGHQYeIiIiIiMIOgw4REREREYUdBh0iIiIiIgo7DDpERERERBR2GHSIiIiIiCjsMOgQEREREVHYYdAhIiIiIqKww6BDRBRg2dnZEAQBgiAgNzfXYb0sy7j++ushCAL69u1rs04QBEyaNMnt/vv27QtBEHDddddBlmWH9Zs3b7YcPzs7GwAwdOhQRERE4MqVKy73+/DDD0OlUuH8+fOWugiCgFdffdXld/zpp5/c1tXe//73P8t+CwoKHNavWrUKPXv2REJCAuLj43HzzTdj2bJlHu//v//9L0aPHo3OnTtDpVJBEASXZfV6PWbNmoUWLVpAo9GgXbt2ePfddz0+1p9//okpU6agT58+iI+Pt2nvmtSLiIh8w6BDRFRLYmJikJWV5bB806ZNOHr0KGJiYmq07+PHj+O7775zWLd48WLExsbaLEtPT0dFRQU++eQTp/srLCzE6tWrcddddyExMdFm3auvvopLly75XFezkpISPPLII2jcuLHT9YsXL8bw4cORnJyM5cuX47PPPkOrVq0wevRo/Otf//LoGKtXr8b27dvRoUMHdO3a1W3ZjIwMzJ07F0888QS+/fZbDB06FE8++SReeeUVj471xx9/YPny5VCr1Rg8eLDf6kVERL5h0CEiqiUjRozAqlWrUFRUZLM8KysLqampaNasmc/7btasGW699VYsXrzYZnlxcTFWrlyJESNG2CwfNGgQGjdu7FDe7NNPP0V5eTnS09Ntlvfv3x+lpaV4+eWXfa6r2fTp01GvXj2MHz/e6frFixejefPm+PzzzzFo0CAMHDgQn3zyCdq1a+eyp8Tev//9bxw5cgQrVqzArbfe6rLc/v37kZWVhRkzZuCZZ55B3759MXfuXEyYMAFz5szxKNj17t0bFy5cQE5ODqZOneqXehERke8YdIiIasmDDz4IwBQizAoLC7Fq1SqXF/veGD9+PL788kub4WifffYZAOCBBx6wKatQKDBmzBjs3r0b+/btc9jXkiVLkJycjEGDBtksb9u2LdLT0/H+++/j5MmTPtd1y5YtWLRoET788EMoFAqnZVQqFaKjoyGKV/9XJQgCYmNjodVqPTqO9bburFmzBrIsY9y4cTbLx40bh/Lycqxfv95vx/K2LBER+Yb/0hIR1ZLY2FgMHz7cphfl008/hSiKDj0uvnjggQegUChsglRWVhaGDx/uMHQNMAUjQRAcenUOHDiAnTt3YsyYMU5DyMyZM6FQKPDPf/7Tp3qae4qmTJmCG2+80WW5yZMn4+DBg3j55Zdx4cIFFBQU4M0338Tu3bvx9NNP+3RsV3777Tc0bNgQSUlJNsu7dOliWU9ERHULgw4RUS0aP348du7cif379wMwDc/629/+VqP7c8xiYmJsgtSBAwewY8cOl71F119/PXr37o2PP/4Yer3esty8vavtkpKS8NRTT2H58uX49ddfva7nP//5TxiNRsyaNcttuWHDhuHLL7/EG2+8gUaNGqFhw4Z48cUXsXTpUvztb3/z+rjuXLx4EQkJCQ7Lo6KioFarcfHiRb8ej4iIAo9Bh4ioFvXp0wetWrXC4sWLsW/fPuzatcsvw9bMxo8fj59++gn79u1DVlYWWrVqhd69e7ssn56ejoKCAqxduxYAYDAY8PHHH6NXr15o3bq1y+2mTZuGhIQEPPvss17Vb+fOncjMzMTChQsRERHhtuz69esxcuRIDBs2DN988w1ycnIwYcIEjB07FkuWLLGUMxqNMBgMlpckSV7VyczdzGfmdbIs2xzLYDD4dCwiIgo8Bh0iolokCALGjRuHjz/+GB988AHatGmDXr16+W3/vXv3RuvWrbFw4UIsW7bMMjzNleHDhyMuLs4SHNatW4fz5887TEJgLzY2Fi+88ALWr1+P77//3uP6jR8/HsOGDUOPHj1w5coVXLlyBRUVFQCAoqIiFBcXAzAFivHjx6N3795YvHgxBg4ciP79++Odd97BQw89hMmTJ6O0tBQA0K9fP6hUKsvLl+BYv359p702paWlqKystPT2LF261OZYKpXK62MREVHtYNAhIqplY8eORUFBAT744AOHm9/9Ydy4cViwYAEuXbqEMWPGuC0bERGBBx98EOvXr0deXh4WL16MmJgYj4aGPf7442jZsiWeffZZp8/vcWb//v1YuXIl6tWrZ3m99tprAIBWrVpZQt/58+eRl5eHm2++2WEfN910E0pLS3HixAkAwMKFC7Fr1y7La+bMmR7VxVrnzp1x4cIFnDt3zma5eaKGTp06AQDuvvtum2Pt2rXL62MREVHtUAa7AkRE15omTZrgmWeewaFDh6oNIr4YM2YMduzYgfbt26NJkybVlk9PT8cHH3yAN954A+vWrcPYsWMRGRlZ7XZqtRpz5szBww8/jAYNGnhUN2e9P9nZ2Vi6dCnWrFljqW+9evWg1Wqxfft2h/I//vgjRFFEcnIyANNMcDU1ZMgQvPDCC1i6dKnNcLzs7GxERERg4MCBAEw9P/Xr16/x8YiIKPAYdIiIguDVV1/1uOzRo0fxxRdfOCzv0KEDOnTo4LC8cePGWLNmjcf779GjB7p06YLMzEzIslztsDVrDz74IN5880188803HpXv27evw7Lc3FwAQM+ePS2BSaPRICMjA/PmzcPo0aMxYsQIKBQKrFmzBp988gnS09OdTh5g7+TJk5Zel6NHjwKApS1btGiBHj16AAA6duyI9PR0zJgxAwqFAjfddBM2bNiARYsWYc6cOR4dy3rfx44dAwD89NNPiI6OBmAaJuhtvYiIyHcMOkREIW79+vVOn+MyY8YMn4ZpOZOeno4nn3wSHTp0wC233OLxdoIg4LXXXkNaWppf6mHtjTfeQPv27bFw4UKMHDkSkiShVatWeO+99/Doo496tI/vv//eYXigeVjemDFjbB48On/+fDRp0gTvvvsuzp07hxYtWuDtt9/G5MmTPa6z/ZC/999/H++//z4A2Azv86ZeRETkG0H2dGA1ERERERFRHcHJCIiIiIiIKOww6BARERERUdhh0CEiIiIiorDDoENERERERGGHQYeIiIiIiMIOgw4REREREYUdBh0iIiIiIgo7DDpERERERBR2GHSIiIiIiCjsMOgQEREREVHYYdAhIiIiIqKww6BDRERERERhh0GHiIiIiIjCDoMOERERERGFHZ+Czvz589GyZUtotVp0794dW7ZscVt++fLl6Nq1KyIjI5GcnIxx48bh4sWLPlWYiIiIiIioOl4HnRUrVmDKlCl4/vnnsWfPHvTq1QuDBg3CqVOnnJb/4YcfMHr0aKSnp2P//v1YuXIldu3ahQkTJtS48kRERERERM4IsizL3mxwyy234MYbb8SCBQssy9q3b497770Xc+fOdSj/5ptvYsGCBTh69Khl2bvvvovXX38dp0+f9uiYkiTh7NmziImJgSAI3lSXiIiIiIjCiCzLKC4uRuPGjSGKrvttlN7stLKyErt378b06dNtlqelpWHbtm1Ot7ntttvw/PPPY926dRg0aBDy8/PxxRdf4K9//avL4+h0Ouh0OsvnM2fOoEOHDt5UlYiIiIiIwtjp06fRtGlTl+u9CjoFBQUwGo1ITEy0WZ6YmIhz58453ea2227D8uXLMWLECFRUVMBgMOCee+7Bu+++6/I4c+fOxaxZsxyWf/jhh4iMjPSmykREREREFEbKysowYcIExMTEuC3n1dC1s2fPokmTJti2bRtSU1Mty19++WUsW7YMhw4dctjmwIED6N+/P5566inceeedyMvLwzPPPIObbroJWVlZTo9j36NTVFSElJQUFBQUIDY21tPqBoRer0dOTg4GDBgAlUoV1LqEI7ZvYLF9A4vtG1hs38Bi+wYW2zfw2MaBFUrtW1RUhAYNGqCwsNBtNvCqR6dBgwZQKBQOvTf5+fkOvTxmc+fORc+ePfHMM88AALp06YKoqCj06tULc+bMQXJyssM2Go0GGo3GYblKpQp6w5qFUl3CEds3sNi+gcX2DSy2b2CxfQOL7Rt4bOPACoX29fT4Xs26plar0b17d+Tk5Ngsz8nJwW233eZ0m7KyMoebhBQKBQDTjURERERERET+5vX00lOnTsWHH36IxYsX4+DBg3jqqadw6tQpTJw4EQDw3HPPYfTo0Zbyd999N7788kssWLAAx44dw9atW/H3v/8dN998Mxo3buy/b0JERERERFTFq6FrADBixAhcvHgRs2fPRl5eHjp16oR169ahefPmAIC8vDybZ+qMHTsWxcXFeO+99/B///d/iI+Pxx133IHXXnvNf9+CiIiIiIjIitdBBwAyMjKQkZHhdF12drbDssmTJ2Py5Mm+HIqIiIiIiMhrXg9dIyIiIiIiCnUMOkREREREFHZ8GrpGAPavAVQqQKkBFOqqnxpAqTZ9dlhWVU5ktiQiIiIiCjQGHV9dOgYIkvfbKZRV4UdjF4ic/HRYpgEUVuFKEPz/vYiIiIiIwgCDTm0zGkyvytKa7UcQTKHHEppUtr1HLpc5CVCikqGJiIiIiMIKg05dJcuAodL00hXXbF+C6GbIndouLFkNw7NfptQAosI/34+IiIiIqAYYdAiQJUBfYXrVlKjwbRieeZl5fgzJh2GBRERERERVGHTIvyQjIJUD+nLftpdFAO2ArZmAUnQyDK+6AOViHYfmEREREV1TGHQodPnrfibAt2F4zsIV72ciIiIiqhMYdOjaYL6fCSU1248gOh9yZ1lmH6TchCsF//oRERERBQqvtIi8IUuAQWd61ZT9/Uw+TzWu4fOZiIiIiOww6BAFS03vZ7Jmfj6TzTA862BUNQxPUJnKnz8AqJSmHiqbl+BkmbMyimrWc3gfERERBReDDlE4MN/PhGruZzJP9nBkvW8PvPWUp8FJrC5UeRK+zOurC18eHkN0tR8PQiARERGFDAYdIvI/WTK9rjkKAG2Bbe8Aortg5GnPWQ2Dl00ZZ/vxdh+u6sMePCIiCj0MOkRE/iLLpp9Gw7X1LChLGPImOPkQ4KSqQHV6B6CNAlSRgCri6k9lBO9XIyIiCwYdIiKqGVkGZGMtHKdq6OWJrXA69FIQTPek2Qcgm/f24UjLHikiojDFoENEROFBlgF9henlKUFwDEVKV+Go6r1Sw3BERFQHMOgQEdG1S5aByjLTy1OCWH1Pkf0yhYrhiIioljHoEBEReUOWgMpS08tTotIqAFXTY2QdjoiIyGcMOkRERIEmGQBdsenlKYXS8x4j85A7Bf+3TkRkxn8RiYiIQpHRABiLgIoiz7dRqqu/x8h+magI3HcgIgoiBh0iIqJwYag0vSoKPd/GPFOdIgJAPHB4PaCNdNGbpOU03kRUZzDoEBERXcsMOtNLLgQQD+QfcD59txmn8SaiOsKnoDN//ny88cYbyMvLQ8eOHZGZmYlevXo5LTt27FgsXbrUYXmHDh2wf/9+Xw5PREREweKvabydhSKlltN4E5HfeB10VqxYgSlTpmD+/Pno2bMnFi5ciEGDBuHAgQNo1qyZQ/m3334br776quWzwWBA165d8be//a1mNSciIqK6gdN4E1EQeB105s2bh/T0dEyYMAEAkJmZiW+//RYLFizA3LlzHcrHxcUhLi7O8nnNmjW4fPkyxo0bV4NqExERUVir8TTeLu4x4jTeRNcMr4JOZWUldu/ejenTp9ssT0tLw7Zt2zzaR1ZWFvr374/mzZt7c2giIiIi9ziNNxFZ8epvakFBAYxGIxITE22WJyYm4ty5c9Vun5eXh2+++QaffPKJ23I6nQ46nc7yuajINLWmXq+HXq/3psp+Zz6+XuaMM4Fgble2b2CwfQOL7RtYbN/Aumbb1yABhhKgvMTzbZQqQGnuIbK6t8jy0+7hsEot9EbTBA/Bvo4JZ5ZrNLZxQIRS+3paB59+JSHYjX+VZdlhmTPZ2dmIj4/Hvffe67bc3LlzMWvWLIflGzZsQGRkpFd1DZSckjbBrkJYY/sGFts3sNi+gcX2DSy2ry9kAGVVL/dycnICXptrHds4sEKhfcvKPLvfz6ug06BBAygUCofem/z8fIdeHnuyLGPx4sUYNWoU1Gq127LPPfccpk6davlcVFSElJQUpKWlITY21psq+51er0dOTg4GRB+Byt30m+QTvSwip6QN2zdA2L6BxfYNLLZvYLF9Ayuk21cQAEEBiAIAhekhsoL5p2h6iaLVMut1VT8ty8Wryx22MZe33kZ0sr3V/kWFVZ3M5Zz/ct1yjTZgAFQq3nvlb6HUvubRXtXxKuio1Wp0794dOTk5GDp0qGV5Tk4OhgwZ4nbbTZs24Y8//kB6enq1x9FoNNBoNA7LVSpV0BvWTCVIofcPVRhh+wYW2zew2L6BxfYNLLZvYIVs+8pGwFj13ui2ZPC5Cl9QAkiE6rcVUClEF4FN4Tx8OV3uItw5lLUPbvbrwmsmwFC4Hvf0+F4PXZs6dSpGjRqFHj16IDU1FYsWLcKpU6cwceJEAKbemDNnzuCjjz6y2S4rKwu33HILOnXq5O0hiYiIiIhMZMn0ss+LsgggESjJd//Q29rmskfMx0DldWBztj93+w+fYOZ10BkxYgQuXryI2bNnIy8vD506dcK6desss6jl5eXh1KlTNtsUFhZi1apVePvtt/1TayIiIiKiukCWAKMEwBDsmnjGVaBKaBvsmnnNp8kIMjIykJGR4XRddna2w7K4uDiPbxoiIiIiIqIgkYxwOn7Rm2dahYhrbA5JIiIiIiK6FvCJV14Sv38JHc78DlF9CRBQNY6xaiyj+b1lbKPgYpndOuv1Pu3Pw/Vuj+dJHeyXwW7bapZ5/D2IiIiIiGqGQcdL4o4P0Nqoq74g+UQJ4G6IVZnHVTiq+uxRaLTf1sMA6Da8udjGrwHRxf6cblPd8a5+JxEiWlb+CaHsMqBUm14KDaDUXP0pKlz++RARERHVFQw6XpJueRzH/jiM61QXoYAMmF+yXFXC/F42rbJZb7XMaXn7Zbi6zuUyN/sL+PHs1lsv85Hp0ly6upua7Y7sKAB0qa6QoDAFHqUGUKitQpCTUGS/3nqdwJGxREREFDwMOl6Sbn8BB8rXoUXMIShCaerCUFNdEHIRBvWygI0l16Nf1O9QCUYn5Zzsr9rABrvj25dDNftzdtzqtrGvg/2y6trFw+Bs/R0cjme/DSDJMs7po5CEAohGHWDUAQYdYKw0vQBANgL6MtOrJkSVXThSOw9J5mX2n83bckgjERER+YBBhwLDZgiVF9vJInSqeEATg5CaAz9MGGURu4rbYXDMIYj27StLprBjsAo/BnMYqrQKRS7Wm99LVdNnSnqgUg+ghrO0KNSOPUrmoGT57Ka3SakxhS4GJiIiomsKgw4RmQgioNSaXjUhGe1CkJtQ5DRIVa2Xq6a2NPc2Vdboy7nuMXI3BM8+SIlKBiYiIqI6gkGHiPxLVABiBKCKqNl+JINt8DGHIvvPDr1NVp8NOliG7pmH6tVkLhFBdH3fklIDUaFFa8MRiMVFVuvse5vUpsBEREREAcX/2xJRaBKVgFoJIMr3fciyKTBZgo8XQ/DsAxVgGt5nqDC9nFAA6OBJvewnfPBkCJ6z4Xuc8IGIiMglBh0iCl+CAChUppc62vf9yPLVIXRu7lsyGvT4s1yLFDHfNNmDfdlanfChmvuWLGXVDExERBSWGHSIiKojCFcDgibGZTFJFrG3uB0aO5vsAbCd8MHlxA9u7lsyf66NCR/cTR3urKxCzfuXiIgopDDoEBHVloBN+ODmPiWnE0DUxoQP7nqXnPQ2ccIHIiLyMwYdIqK6xq8TPri5b8mT3qZanPBBVGjQwfgLxMgSq14mJ9OOKzWc8IGIiBh0iIiuWZYJHyJ934fNhA/Wkzk4mwDCRS+UhxM+KAC0xh7P6iUoqnlgrf1U4vbTj1ctExW+tw0REQUVgw4REfnOZsKHGuxHlk33HLnpSTIa9DheFonrFOeuTvZgX9Zmwody06smRKXr+5echiT73iit6ScnfCCiUGX+hZWkB4z6quHMVe+lq5/FK2eQWNwQwOBg19hjDDpERBR8gnB1UgMXEz5Isoj9xe3Q3NVkD0DVDHnVTOZgXuau10nSVx3UYHrpazjhg6iq5t4lT8ITAxPRNU0yVoUPqyBiDiWS64BiKWcJMk4+Q6728AoATeNvCfjX9CcGHSIiCh+C4J8JH2SpmkkePAxPDjPk1RBnyCMKbbJkFTTsw4ergOImsFhvJ7v4BY8/iUrTL2bMPfUKteWzFNkQl+XrkRj4WvgNgw4REZE9QQRUWtOrJhxmyHP3kFo3PUx+nSEPTkOSQqlBVykRYkS56QLHpkdJ6xioRBUDE9VNlnsLPQwfzsq56h0x/3IjkATR9HdUVFsFEvvPasfAolBVLbMqZ//ZTa+xsWEnHDsqoV3gv6HfMOgQEREFSm3PkOfq3iXztubfCJuDlXVVAbTAQS8qJbiY6MFVD5OLh9hySnFyxfyLAo96PiohGg3oWvEjFIr86od01QZfw4bNdk624SQpHmPQISIiCnV+nSHP+ZThRkMljpTFoo3iDBTGCheTQlTaTiluniGvxlOKO5sFT+t+iJ798DxOKR4c5qFaDj0dnvaOuOkh8XKolgJAC2/rLyprGD7stzOvY4APBfxXgYiI6FpgM0NelMNqSRZxpLgdro85BIWryR4AqxnynEwR7vIZTC7ubwKq7ocqN71q9P0U7nuU7B9O66o3Khx/W27+M/P0xnRngcXVDe7mYZWBZH6+VjWBxCiqccTQGG0iC6FQKD0YwuV+qBbVfQw6RERE5DnrGfIQ7ft+ZNlquJ2TIFTtJBCVV8MVUDWleJnpVRPWU4pX95wldz1M3l5Ay7LpO9R0Ji1X2wWc4DxIeDpcy1kPinm5h+HT47BO1wwGHSIiIqp9gnA1PGhqsB9ZsgpA7u5ZqiY0BXhKcYVCg5vlBCjEK1UBxcnwLQ+m+K0xFzNqeTw8yyGQVH0WFByqRSGHQYeIiIjqLkH045TizmbDsx+OV+F+WJ6LKcVFAMk44eF3Uji596O6z9XcT2LehmGEriE+BZ358+fjjTfeQF5eHjp27IjMzEz06tXLZXmdTofZs2fj448/xrlz59C0aVM8//zzGD9+vM8VJyIiIvIbQTTNjlfjGfKMjgHIoIPBoMdvZQnoFHkRSoXCTQ8JHwxL5C9eB50VK1ZgypQpmD9/Pnr27ImFCxdi0KBBOHDgAJo1a+Z0m/vvvx/nz59HVlYWrr/+euTn58NgqIV5xomIiIhqk6gAxEhAZTtDniyLOFncDh1jDgG8f4SoVngddObNm4f09HRMmDABAJCZmYlvv/0WCxYswNy5cx3Kr1+/Hps2bcKxY8eQkJAAAGjRokXNak1EREREROSGV0GnsrISu3fvxvTp022Wp6WlYdu2bU63Wbt2LXr06IHXX38dy5YtQ1RUFO655x689NJLiIhw3j2s0+mg012dlL+oqAgAoNfrodfXxswhrpmPr5fZrRwI5nZl+wYG2zew2L6BxfYNLLZvYLF9A49tHFh6yTRZRrCvxb2pg1dBp6CgAEajEYmJiTbLExMTce7cOafbHDt2DD/88AO0Wi1Wr16NgoICZGRk4NKlS1i8eLHTbebOnYtZs2Y5LN+wYQMiI2vwsDQ/yilpE+wqhDW2b2CxfQOL7RtYbN/AYvsGFtvXfwRBgEJhO/W0Ugl8X9EhSDUKc6er2vf77wN+KKPRCFl2PQthWZln08j7NBmBYDdjhyzLDsvMJEmCIAhYvnw54uLiAJiGvw0fPhzvv/++016d5557DlOnTrV8LioqQkpKCtLS0hAbG+tLlf1Gr9cjJycHA6KPQMUxtn6nl0XklLRh+/pKoQQE66c8277Xy0rknAAGNCmDylAKVJZUPeWc/IHnb2CxfQOL7RtYbF//kQHka1qgSNvU5hk7MoAKSQWtqAfnlvM/WVShwgBotVqX1/3+FBsbi0aNGjk9lnm0V3W8CjoNGjSAQqFw6L3Jz8936OUxS05ORpMmTSwhBwDat28PWZbx559/onXr1g7baDQaaDSOk+qrVCqoVCpvqhwwKkHiP1QBFJbtK4h204IqbWfbEa1DiS/lPJg2VK8HTqyDquvwq3+XjFXPi9CVAJVV4afS/L4U0BVffS+H2Z9JgITl+RtC2L6BxfYNLLZvzeWpW6I4qiUSG9RHpEZl+V+fBKBE0iJarAAHr/mfpNSipBKIjo6GKAauhWVZRllZGfLz86FQKJCcnOxQxtM84FXQUavV6N69O3JycjB06FDL8pycHAwZMsTpNj179sTKlStRUlKC6GjTE5SPHDkCURTRtGlTbw5PFFiuAoVCbRculLbPKHBZzi6EePhk51qnUAKKOEAb576cLJueOG4OQzqrMFRZbBuMjMEfv0tEROHHCAWuaFPQqEF91I+1vZ1BAlApqaEVjQw6ASCp1KiEqUcnkEEHgGXEV35+Pho1auQwRNFTXg9dmzp1KkaNGoUePXogNTUVixYtwqlTpzBx4kQApmFnZ86cwUcffQQAeOihh/DSSy9h3LhxmDVrFgoKCvDMM89g/PjxLicjIHIgKrwIFErPej/sy/Ehau4JAqCOMr3QyH1ZQ6VVr1CJk3BU1WukLzcFKCIiIg/oRQ0gKhCpCY0RPhQ45vvy9Xp97QWdESNG4OLFi5g9ezby8vLQqVMnrFu3Ds2bNwcA5OXl4dSpU5by0dHRyMnJweTJk9GjRw/Ur18f999/P+bMmeNThSkECYL7QOEQLqyf2qy0LSeJwA97gB7jAE3E1VATqr0h5JxSDSgTgMgE9+Ukyc2wObteI4nP3iIiIhP+bjIABAEQFIAoVv1UmIbdm99DNI3aqLXq1PwP2afJCDIyMpCRkeF0XXZ2tsOydu3aIScnx5dDkT+ISsdAUW3wUNtuY9P7YVdOVPjvXxy9HsAeIKIeECL3Y1EAiSKgiTG93JFlwFDhumfIMoSuBNBX1E7diYiI6gpXIUZUmD4LVddy7q7npLp3b5lPQYf8SBDcDKdyEzxstrG/b0Rpuy7A4yiJAk4QAFWE6RXVwH1Zo8Fu2FzVe4dwVMbJFYiIqO5zGmKsPnsSYsIUg46v6rUAVO6Ch4ezZvmzN4SITH/fIuJNL3ckCTCUVwWgau4n4uQKRERUQ2Mfn4IrhYVY88kSzzcKUIgxDwv78ccfceutt1qW63Q6NG7cGJcuXcL333+Pvn37erXfUMOg46tOwzi0iqguE0WryRWcT49vYdBVM2zOvJzPJCIiIg8FuScmJSUFS5YssQk6q1evRnR0NC5duhSQY9Y2jmkiIqqOUmOaWCG+GZDYAUi5CWh1O9D+bqDrA8BNE4C/TAF6Pmkqf8PDQOfhQNuBQMteQONuQIPWQGxjQBvLyTWIiMKdeaImpcY07FoTfXW0T1QDIDqx6tUQiKwPRMRj0/afcfNf+kITFYfkps0w/R//gMFotOzyiy++QOfOnREREYH69eujf//+KC0tBQDk5ubi5ptvRlRUFOLj49GzZ0+cPHnSbRXHjBmDzz77DOXl5ZZlixcvxpgxYxzKnjlzBg888ABatGiBhg0bYsiQIThx4oRl/a5duzBgwAA0aNAAcXFx6NOnD37++We7JhHw4YcfYujQoYiMjETr1q2xdu1aX1rXY+zRISLyF3OAiUl03+NrnlzBYdick/uJDOwlIiJyR5ZllOslSADKJSOUgX6OjiBYel8i1EoICqVnPTFiVTmF4/8fzpw5g8GDB2Ps2LH46KOPcOjQITzyyCPQarWYOXMm8vLy8OCDD+L111/H0KFDUVxcjC1btkCWZRgMBtx777145JFH8Omnn6KyshI7d+6sdtay7t27o2XLlli1ahVGjhyJ06dPY/PmzXj//ffx0ksvWcqVlZXh9ttvx1/+8hd8/fXXiI+PxyuvvIKBAwfi119/hVqtRnFxMcaMGYN33nkHAPDWW29h8ODB+P333xETc3XCoVmzZuH111/HG2+8gXfffRcPP/wwTp48iYSEamZp9RGDDhFRbbOeXAEN3Zc16t1MqGA1bE5fxmcSEdE1qVwvocObvwTl2Adm34lIdc0vp+fPn4+UlBS89957EAQB7dq1w9mzZ/Hss8/ixRdfRF5eHgwGA4YNG2Z5pEvnzp0BAJcuXUJhYSHuuusutGrVCgDQvn17j447btw4LF68GCNHjsSSJUswePBgNGxo+/+lzz77DKIo4t///jeKi4sRGxuLJUuWID4+Hrm5uUhLS8Mdd9xhs83ChQtRr149bNq0CXfddZdl+dixY/Hggw8CAF555RW8++672LlzJwYOHOhbw1WDQYeIKJQpVKbp1iPquS8nSaaw47R3yC4gGflMIiKiUHLw4EGkpqba9ML07NkTJSUl+PPPP9G1a1f069cPnTt3xp133om0tDQMHz4c9erVQ0JCAsaOHYs777wTAwYMQP/+/XH//fcjOTm52uOOHDkS06dPx7Fjx5CdnW3pkbG2e/du/PHHH4iLi7NZXlFRgaNHjwIA8vPz8eKLL+K7777D+fPnYTQaUVZWZvNsTQDo0qWL5X1UVBRiYmKQn5/vVVt5g0GHiCgciKJpDLgmGm4nV5BlwFhZzbC5qvf6ctf7ISIKEREqEQeeuQGSoECxQYUYDSDa3Nhvfl6M/we0Raj8c8+lLMsOQ83kql56QRCgUCiQk5ODbdu2YcOGDXj33Xfx/PPPY8eOHWjZsiWWLFmCv//971i/fj1WrFiBF154ATk5OTYTDThTv3593HXXXUhPT0dFRQUGDRqE4mLbh4JKkoTu3btj2bJlKCkpQXR0NMSqR5eYe3/Gjh2LCxcuIDMzE82bN4dGo0FqaioqKytt9qWyG9YtCAKkAD6fh0GHiOhaIgimm2OVGiCqvvuyktFNz5DdLHR8JhERBYpSDWhiTS9VPGDUAOoYIDICEBQQRAUiBRGSJMFQVITImFjLhXhd0aFDB6xatcom8Gzbtg0xMTFo0qQJAFMo6NmzJ3r27IkXX3wRzZs3x+rVqzF16lQAQLdu3dCtWzc899xzSE1NxSeffFJt0AGA8ePHY/DgwXj22WehUDgGtxtvvBErVqxAo0aN0KhRI8TGOrbvli1bMH/+fAwePBgAcPr0aRQUFNSoTfyBQcdH+cU6NEng9NJEFMZEBaCNM73ckWVT709lKVBZXPXTxbA5Q6X7fRHRtcU6xGhirr60VsuUmqvlKyqA48cBdQSg1Aav3j4qLCzE3r17bZYlJCQgIyMDmZmZmDx5MiZNmoTDhw9jxowZmDp1KkRRxI4dO7Bx40akpaWhUaNG2LFjBy5cuID27dvj+PHjWLRoEe655x40btwYhw8fxpEjRzB69GiP6jRw4EBcuHABsbGxTtc//PDDeOONNzB06FA888wzaNu2Lf788098+eWXeOaZZ9C0aVNcf/31WLZsGXr06IGioiI888wziIiIqGlz1RiDjo9W/nQaDeMi0aVJPNokRUOj5HSxRHSNEgRAHWl6VTe5gqHSKvy4GzbHyRWI6jylpiq4xNoFmBjnIeYakJubi27dutksGzNmDLKzs7Fu3To888wz6Nq1KxISEpCeno4XXngBABAbG4vNmzcjMzMTRUVFaN68Od566y0MGjQI58+fx6FDh7B06VJcvHgRycnJmDRpEh577DGP6iQIAho0aOByfWRkJDZv3oxp06Zh9OjRKCkpQZMmTdCvXz9LOFq8eDEeffRRdOvWDc2aNcMrr7yCp59+2sdW8h8GnRrIL9Lhf0Xnsfn3C2ibGIPOTeOQGFv3frtARFRrlGpAmWB6LpE7kgTo7XqGyouB3y4BiZ0AwQhIetPwOslgmp1OMlR91lstM3JYHVEg2IcYrXWPzLUZYqqTnZ2N7Oxsl+v79OmDnTt3Ol3Xvn17rF+/3um6xMRErF692qu6yG5+kRQfH++wPikpCdnZ2SgqKnI6dK1bt27YtWuXzbLhw4dXe8wrV654VW9vMej4QaVBwr4zhdh3phCNYjXo3CQObZNi2MtDROQrUbx60WR+BINeD/y2DmiT5v45RfYk6Wr4kQymWeckg9Uyo1VQ8jA8WW9v2Z/19sbq60UUqqxDjNZuSBlDDNUhDDp+ll+kw8aifGz5vQBtEmPQuUkcEmM11T60iYiIAkQUAVEDoBYvzCTJSfixDk9WL4/DkwfbE1XHHGK0cXYBJgbQxJlmbmSIoTDBoBMglQYJv50pxG9nCtEw5movj9ZP0xASEVEIE0VAVANQ194xZdku/HjZG6WvBA6VAYkdARjcBCq73jEKHUqN7U38Nr0w5p6YWjwniYKMQacWXCjW4btD+djy+wW0TYplLw8REfmfIJgeMKtQASofZjvS64FD64A2d3o+NFCW/dQb5WxoodX29tteixNVqLSON/Zr7G/sZ4ghssagU4v0Rpm9PEREFD4EAVAoTS/U0mQ8smyaYMJZcKo2PDm7L8t+W4Pz7QM5qYVDiHF2Yz9DDJG3GHSCxLqXp03VjG1JsVr28hAREbkjCFVPvK/lXxJ6NamFk/CkNwCHy4DmtwGRcQwxRLWAQSfI9EYZ+88WYf/ZIjSo6uVpx14eIiKi0FLTSS30euDwOqDZrd7NGkhEPmPQCSEFxTp8fygfP/x+Aa2rZmxLjmMvDxERERGRtxh0QpDeKOPA2SIcYC8PEREREZFPxOqLUDCZe3k+3HIM3+4/h7NXyt0+zZaIiIiIqLa0aNECmZmZwa6GU+zRqSNsenmi1ejUJA7tk2PZy0NEREQEIPN/v0On00GjqZ1HeDw1oI1X5ceOHYsrV65gzZo1gamQF77//nvMnj0bv/zyCyoqKtCkSRPcdtttyMrKglKpRHZ2NqZMmYIrV64Eu6o14lOPzvz589GyZUtotVp0794dW7ZscVk2NzcXgiA4vA4dOuRzpa91BSWVyD18Af/efAzrfzuHM+zlISIiIiIP7N+/H4MGDcJNN92EzZs3Y9++fXj33XehUqkgSQGcRj0IvA46K1aswJQpU/D8889jz5496NWrFwYNGoRTp0653e7w4cPIy8uzvFq3bu1zpcnEIMk4mFeEz3edxsfbT2LPqcuo0BuDXS0iIiIi8tKmTZtw8803Q6PRIDk5GdOnT4fBYLCs/+KLL9C5c2dERESgfv366N+/P0pLSwGYOhZuvvlmREVFIT4+Hj179sTJkyedHicnJwfJycl4/fXX0alTJ7Rq1QoDBw7Ehx9+CLVajdzcXIwbNw6FhYWWDoqZM2cCAC5cuIB77rkHERERaNmyJZYvXx7wdqkJr4POvHnzkJ6ejgkTJqB9+/bIzMxESkoKFixY4Ha7Ro0aISkpyfJSKDjkyp/Yy0NERERUN505cwaDBw/GTTfdhF9++QULFixAVlYW5syZAwDIy8vDgw8+iPHjx+PgwYPIzc3FsGHDIMsyDAYD7r33XvTp0we//vorfvzxRzz66KMuh+8lJSUhLy8Pmzdvdrr+tttuQ2ZmJmJjYy0dFE8//TQAICMjAydPnsR3332HL774AvPnz0d+fn5gGsUPvLpHp7KyErt378b06dNtlqelpWHbtm1ut+3WrRsqKirQoUMHvPDCC7j99ttdltXpdNDpdJbPRUVFAAC9Xg+9Xu9Nlf3OfHxBDs2eE6MROHT2Mg6dvYyEKDU6NI5Fu8QYaOrIvTzm9g32n3O4YvsGFts3sNi+gcX2DSy2r3/o9XrIsgxJkpwMs5ItP2vjd73eDvOSZdlSd3vvv/8+UlJS8M4770AQBLRp0wZnzpzB9OnT8cILL+DMmTOWQNOsWTMAQMeOHQEAly5dQmFhIQYPHoyWLVsCANq2beuyjvfddx/Wr1+PPn36ICkpCbfccgv69euHUaNGITY2FkqlEjExMRAEAY0aNbJsd/jwYfzvf//D1q1bccsttwAA/v3vf6Njx44uv1dNSJIEWZah1+sdOkg8/XvkVdApKCiA0WhEYmKizfLExEScO3fO6TbJyclYtGgRunfvDp1Oh2XLlqFfv37Izc1F7969nW4zd+5czJo1y2H5hg0bEBkZ6U2VA6ZFxdFgV6F65cCZAuBMsOvhg5ycnGBXIayxfQOL7RtYbN/AYvsGFtu3ZpRKJZKSklBSUoLKykqbdTpdpc3PQDP/It5Ter0eBoPB6Xb79u1D9+7dUVxcbFnWpUsXlJSU4ODBg2jZsiX69OmDrl274o477sDtt9+OIUOGID4+HkqlEg899BAGDRqEvn37om/fvrj33nuRlJTksi6ZmZmYNm0aNm/ejJ9++gkvv/wyXn31VWzcuBFJSUmoqKiALMs2dd2zZw+USiXatm1rWd64cWPExcWhoqLC6/aoTmVlJcrLy7F582abIXwAUFZW5tE+fJp1zb4rTJZll91jbdu2taRKAEhNTcXp06fx5ptvugw6zz33HKZOnWr5XFRUhJSUFKSlpSE2NtaXKvuNXq9HTk4OTmhbQRbqRi+JtXqRV3t5tOrQq7+5fQcMGAAVnxztd2zfwGL7BhbbN7DYvoHF9vWPiooKnD59GtHR0dBqtTbrNBo1dLpKaDRqAIGfdc3ba1KVSgWlUul0O4VCAbVabbMuKirKcpx69eph48aN2LZtG3JycpCVlYWXX34ZP/74I1q2bIlly5Zh6tSp+Pbbb7F27Vq8/PLL+Pbbb3Hrrbe6rX+7du3w6KOP4vLly2jXrh0++eQTzJw5E1qt6YH11vUxt3dsbKxND4sgCNBqtX6/Rq+oqEBERAR69+7t8GftaajyKug0aNAACoXCofcmPz/foZfHnVtvvRUff/yxy/UajQYajcZhuUqlCpl/HGRBUSeDzqVyI344ehnbj19B68RodGoShybxEbUyDaM3QunPOhyxfQOL7RtYbN/AYvsGFtu3ZoxGIwRBgCiKEEX7W80Fy8/auK5xPL575hv7nW3XsWNHrFq1ylIGALZv346YmBikpKRYtunVqxd69eqFGTNmoHnz5vjPf/5j6Rzo3r07unfvjn/84x9ITU3FZ599httuu82jutWvXx/JyckoKyuDKIrQarUwGo02dW3fvj0MBgN2795tCVCHDx/GlStXXH6vmhBFEYIgOP074+nfIa+CjlqtRvfu3ZGTk4OhQ4dalufk5GDIkCEe72fPnj1ITk725tDkZ6YZ24pxMK8YCVGm5/J0SI5FRAj28hARERGFg8LCQuzdu9dmWUJCAjIyMpCZmYnJkydj0qRJOHz4MGbMmIGpU6dCFEXs2LEDGzduRFpaGho1aoQdO3bgwoULaN++PY4fP45FixbhnnvuQePGjXH48GEcOXIEo0ePdlqHhQsXYu/evRg6dChatWqFiooKfPTRR9i/fz/effddAKaHgJaUlGDjxo3o2rUrIiMj0bZtW/Tr1w+PPfYYFi1aBKVSiSlTpiAiIiLQzeYzr4euTZ06FaNGjUKPHj2QmpqKRYsW4dSpU5g4cSIA07CzM2fO4KOPPgJgGgPYokULdOzYEZWVlfj444+xatUqrFq1yr/fhHx2qbQSm49cwNY/CtC6kamXp2m90OvlISIiIqrLcnNz0a1bN5tlY8aMQXZ2NtatW4dnnnkGXbt2RUJCAtLT0/HCCy8AMA0X27x5MzIzM1FUVITmzZvjrbfewqBBg3D+/HkcOnQIS5cuxcWLF5GcnIxJkybhsccec1qHm2++GT/88AMmTpyIs2fPIjo6Gh07dsSaNWvQp08fAKaZ1yZOnIgRI0bg4sWLmDFjBl588UW8//77mDp1Kvr06YPExETMmTMH//znPwPbaDXgddAxf+HZs2cjLy8PnTp1wrp169C8eXMApunvrJ+pU1lZiaeffhpnzpxBREQEOnbsiK+//hqDBw/237cgvzBKMg6dK8ahc8WoF6lC56Zx6JAcx14eIiIiCnlT+rdGUVERYmNj/T6Myh+ys7ORnZ3tcn2fPn2wc+dOp+vat2+P9evXO12XmJiI1atXe1yPbt26YdmyZdWWW7Bggc3jYyRJQmJiIr766iub9h01apTHx65tPk1GkJGRgYyMDKfr7P8Ap02bhmnTpvlyGAqiy2V6bD5SgK1/XMT1jaLRmb08RERERFSH+BR06NphlGQcPleMw1a9PO2TYxGp5qlDRERERKGLV6vkMfbyEBEREVFdwaBDXrPu5YmPVKFzkzh0aMxeHiIiIiIKHbwypRq5UqbHlt8LsO3oRbRqaOrlSUlgLw8RERERBReDDvmFUZJx5HwxjpxnLw8RERERBR+vQsnv2MtDRERERMHGoEMBY93LExdhfi5PLKI0PO2IiIiIKLB4xUm1orBcjx9+L8C2Py6iVaModG4Sh2YJkezlISIiIqKAYNChWiXJMn4/X4Lfz5cgLkKFTk3i0LExe3mIiIiIyL/EYFeArl2F5Xps/aMAH245jv/+ehYnL5ZCluVgV4uIiIgoYLZt2waFQoGBAwfW+rGzs7MhCALat2/vsO7zzz+HIAho0aJFrdcrUBh0KOjMvTxf/nwGy7efAgCUVhqCXCsiIiIi/1u8eDEmT56MH374AadOnar140dFRSE/Px8//vijQ72aNWtW6/UJJAYdCimFFXoAwEfbTuKrX87iRAF7eYiIiCg8lJaW4vPPP8fjjz+Ou+66C9nZ2Q5l1q5dix49ekCr1aJBgwYYNmyYZZ1Op8O0adOQkpICjUaD1q1bIysry6s6KJVKPPTQQ1i8eLFl2Z9//onc3Fw89NBDDuW/+uordO/eHZGRkbjhhhswe/ZsGAxXfyE9b948dO7cGVFRUUhJSUFGRgZKSkos67OzsxEfH49vv/0W7du3R3R0NAYOHIi8vDyv6u0LBh0KSZIs44/8EqzecwaLt57AjmMXUaJjLw8RERHZkWWgstT00pddfV8bLy9/GbtixQq0bdsWbdu2xciRI7FkyRKbX+h+/fXXGDZsGP76179iz5492LhxI3r06GFZP3r0aHz22Wd45513cPDgQXzwwQeIjo72usnS09OxYsUKlJWVATCFkYEDByIxMdGm3LfffouRI0fi73//O3777Tf861//wtKlS/Hyyy9byoiiiHfeeQe//fYbli5diu+++w7Tpk2z2U9ZWRnefPNNLFu2DJs3b8apU6fw9NNPe11vb/EOcAp5ReV6bDt6EduPXULLhqYZ25onREIUOWMbERHRNU9fBrzSGCKA+No+9j/OAuooj4tnZWVh5MiRAICBAweipKQEGzduRP/+/QEAL7/8Mh544AHMmjXLsk3Xrl0BAEeOHMHnn3+OnJwcS/nrrrvOp2rfcMMNaNWqFb744guMGjUK2dnZmDdvHo4dO2ZT7uWXX8b06dMxZswYSJKEBg0aYNasWZg+fTpmzJgBAJgyZYqlfMuWLfHSSy/h8ccfx/z58y3L9Xo9PvjgA7Rq1QoAMGnSJMyePdununuDQYfqDEmWcTS/BEfzSxAboUKnxrHo2CQO0ZyxjYiIiELc4cOHsXPnTnz55ZcATEPIRowYgcWLF1uCy969e/HII4843X7v3r1QKBTo06ePX+ozfvx4LFmyBM2aNUNJSQkGDx6M9957z6bM7t27sWvXLpseHKPRiIqKCpSVlSEyMhLff/89XnnlFRw4cABFRUUwGAyoqKhAaWkpoqJMITAyMtIScgAgOTkZ+fn5fvke7vAKkeok9vIQERERAEAVCfzjLCRJQlFxMWJjYiCKtXR3hirS46JZWVkwGAxo0qSJZZksy1CpVLh8+TLq1auHiIgIl9u7W+eLhx9+GNOmTcPMmTMxevRoKJWOsUCSJMyaNQvDhg2DJEkoKSlBdHQ0RFGEVqvFyZMnMXjwYEycOBEvvfQSEhIS8MMPPyA9PR16vd6yH5VKZbNfQRBq5R5sBh2q06x7eWK0SstzeWK0quo3JiIiorpPEEzDxyQJUBlN72sr6HjIYDDgo48+wltvvYW0tDSbdffddx+WL1+OSZMmoUuXLti4cSPGjRvnsI/OnTtDkiRs2rTJ0gNUEwkJCbjnnnvw+eef44MPPnBa5sYbb8Thw4dx/fXXm4JkURFiY2MtQfKnn36CwWDAW2+9ZVn2+eef17hu/sKgQ2GjuMKAH49exPZjF9GygamXp0X9KPbyEBERUVD997//xeXLl5Geno64uDibdcOHD0dWVhYmTZqEGTNmoF+/fmjVqhUeeOABGAwGfPPNN5g2bRpatGiBMWPGYPz48XjnnXfQtWtXnDx5Evn5+bj//vsBAO3atcPcuXMxdOhQj+qVnZ2N+fPno379+k7Xv/jii7jrrruQkpKC++67D2VlZTh27Bj279+POXPmoFWrVjAYDHj33Xdx9913Y+vWrS5DUzCEVtwl8gNZBo5dKMV/9p7F4q3Hsf3YRRRX6KvfkIiIiCgAsrKy0L9/f4eQA5h6dPbu3Yuff/4Zffv2xcqVK7F27VrccMMNuOOOO7Bjxw5L2QULFmD48OHIyMhAu3bt8Mgjj6C0tNSy/vDhwygsLPS4XhERES5DDgDceeed+O9//4ucnBzccsstGDBgADIzM9G8eXMApkkN5s2bh9deew2dOnXC8uXLMXfuXI+PH2iCXAceUlJUVIS4uDgUFhYiNjY2qHXR6/VYt24djke0gSwoglqXcCTIRrQsP+L39hUEsJcHV8/fwYMHO4yXpZpj+wYW2zew2L6Bxfb1j4qKChw/fhwtW7aEVqu1WedsaBX5T223r7s/a0+zAYeu0TXB3Mtz7EIpYrRKdGwch45NYhHLe3mIiIiIwhKDDl1ziisM2H7sInYcN93L06lJHFpew708REREROGIQYeuWfa9PB0ax6JTkzj28hARERGFAQYdIph6eXYcu4Sdxy+xl4eIiIgoDPh0J9H8+fMtNwZ1794dW7Zs8Wi7rVu3QqlU4oYbbvDlsEQBZ+7lWbv3LLJ+OI5tRwtQWM4Z24iIiIjqGq+DzooVKzBlyhQ8//zz2LNnD3r16oVBgwbh1KlTbrcrLCzE6NGj0a9fP58rS1SbSnSmXp4lW49jzZ4z+CO/BJIU8pMUEhERERF8CDrz5s1Deno6JkyYgPbt2yMzMxMpKSlYsGCB2+0ee+wxPPTQQ0hNTfW5skTBIMvA8YJSfPVLVS/PH+zlISIiIgp1Xt2jU1lZid27d2P69Ok2y9PS0rBt2zaX2y1ZsgRHjx7Fxx9/jDlz5lR7HJ1OB51OZ/lcVFQEwDQHvV4f3AtM8/EF2RjUeoQrc7uGavuWVhix89gF7Dp+Ac0SItEhOQ7N60dCUUfu5TGfv8H+exSu2L6BxfYNLLZvYLF9/UOv10OWZUiSBEmSbNaZHw1pXk/+VdvtK0kSZFmGXq+HQmH7bEVP/x55FXQKCgpgNBqRmJhoszwxMRHnzp1zus3vv/+O6dOnY8uWLVAqPTvc3LlzMWvWLIflGzZsQGRkpDdVDpgWFUeDXYWwVifatww49CdwKNj18EFOTk6wqxDW2L6BxfYNLLZvYLF9a0apVCIpKQklJSWorKx0Wqa4uLiWa3Vtqa32raysRHl5OTZv3gyDwWCzrqyszKN9+DTrmiDY/vZalmWHZQBgNBrx0EMPYdasWWjTpo3H+3/uuecwdepUy+eioiKkpKQgLS3N7dNPa4Ner0dOTg5OaFtBFhTVb0BeEWQjWlQcrZPtKwio6uWJRfP6USHZy2M+fwcMGMAncwcA2zew2L6BxfYNLLavf1RUVOD06dOIjo6GVqu1WSfLMoqLixETE+P0upRqxr59c3Nz0a9fP1y8eBHx8fF+P15FRQUiIiLQu3dvhz9r82iv6ngVdBo0aACFQuHQe5Ofn+/QywOYEt9PP/2EPXv2YNKkSQCudkMplUps2LABd9xxh8N2Go0GGo3GYblKpQqZfxxkQVHnLsTrkrrYvjKAE5d0OHHpAqI0l9CxcRw6NY5DXGRonLPWQunvUjhi+wYW2zew2L6BxfatGaPRCEEQIIoiRNH2VnP5u1eg1ekgajS1E3Ruf86nzbZt24ZevXphwIABWL9+vZ8r5Z7RaMTrr7+OpUuX4uTJk4iIiECbNm3w2GOPYdy4cQCAvn374oYbbkBmZqbNtubhavbt7+zPwh9EUYQgCE7/znj6d8iroKNWq9G9e3fk5ORg6NChluU5OTkYMmSIQ/nY2Fjs27fPZtn8+fPx3Xff4YsvvkDLli29OTxRnVGqM2LncdNzeZrXj0TnJnG4rmF0SPbyEBERUe1ZvHgxJk+ejA8//BCnTp1Cs2bNau3YM2fOxKJFi/Dee++hR48eKCoqwk8//YTLly/XWh1qk9fxa+rUqfjwww+xePFiHDx4EE899RROnTqFiRMnAjANOxs9erRp56KITp062bwaNWoErVaLTp06ISoqyr/fhigEnbxYhv/+moesH45h6x8FKCzjjahERETXotLSUnz++ed4/PHHcddddyE7O9uhzNq1a9GjRw9otVo0aNAAw4YNs6zT6XSYNm0aUlJSoNFo0Lp1a2RlZXl8/K+++goZGRn429/+hpYtW6Jr165IT0+33DIyduxYbNq0CW+//TYEQYAgCDhx4gQAYN26dejRoweioqJw++23W5aHMq+DzogRI5CZmYnZs2fjhhtuwObNm7Fu3To0b94cAJCXl1ftM3WIrkXmXp7FW4/jy5//xO/ni2Hkc3mIiIiuGStWrEDbtm3Rtm1bjBw5EkuWLLHMZgYAX3/9NYYNG4a//vWv2LNnDzZu3IgePXpY1o8ePRqfffYZ3nnnHRw8eBAffPABoqOjPT5+UlISvvvuO1y4cMHp+rfffhupqal45JFHkJeXh7y8PKSkpOD06dMYPnw4BgwYgJ9//hkTJkxwmIU5FPk0GUFGRgYyMjKcrnOWTK3NnDkTM2fO9OWwRGHj5MUynLxYhki1wnQvT5NYxEeqg10tIiIiCqCsrCyMHDkSADBw4ECUlJRg48aN6N+/PwDg5ZdfxgMPPGAz+3DXrl0BAEeOHMHnn3+OnJwcS/nrrrvOq+PPmzcPw4cPR1JSEjp27IjbbrsNQ4YMwaBBgwAAcXFxUKvViIyMRFJSkmW7BQsW4LrrrsMrr7yCuLg4tG/fHvv27cNrr73me2PUAv/fOUREHiurNGLXiUtYsvUEVu3+E0fYy0NERBSWDh8+jJ07d+KBBx4AYJoqe8SIEVi8eLGlzN69e9GvXz+n2+/duxcKhQJ9+vTxuQ4dOnTAb7/9hu3bt2PcuHE4f/487r77bkyYMMHtdgcPHsQtt9xiM8lDamqqz/WoLT716BCR/526VIZTl0y9PB0ax6Jzkzj28hAREYWJrKwsGAwGNGnSxLJMlmWoVCpcvnwZ9erVQ0REhMvt3a3zhiiKuOmmm3DTTTfhqaeewscff4xRo0bh+eefdzlRmPXwurqEPTpEIaas0oifTlxmLw8REVGYMBgM+Oijj/DWW29h7969ltcvv/yC5s2bY/ny5QCALl26YOPGjU730blzZ0iShE2bNvm1bh06dABgmigBMM2ybDQaHcrs2LHDZtn27dv9Wo9AYNAhCmGnLpXh61/z8OGWY9jy+wVcLnX+FGgiIiIKXf/9739x+fJlpKenO8xIPHz4cMvMaTNmzMCnn36KGTNm4ODBg9i3bx9ef/11AECLFi0wZswYjB8/HmvWrMHx48eRm5uLzz//3HKcdu3aYfXq1S7rMXz4cPzrX//Cjh07cPLkSeTm5uKJJ55AmzZt0K5dO8txduzYgRMnTqCgoACSJGHixIk4evQonn/+eRw+fBiffPJJtfflhwIGHaI6wNzLk73tBL7Y/ScOnyuGwSgFu1pERETkgaysLPTv3x9xcXEO6+677z7s3bsXP//8M/r27YuVK1di7dq1uOGGG3DHHXfY9KQsWLAAw4cPR0ZGBtq1a4dHHnnE0hMDmO4DKiwsdFmPO++8E1999RXuvvtutGnTBmPGjEG7du2wYcMGKJWmO1qefvppKBQKdOjQAQ0bNrQ862flypVYv349unXrhg8++ACvvPKKH1soMHiPDlEdc/pSGU5fKkOEWoEOyaZ7eepF8V4eIiK6tsl9p6OiqAjq2FgIYmj9Lv+rr75yue7GG2+0uQdm2LBhNs/OsabVajFv3jzMmzfP6frq7qV55JFH8Mgjj7gt06ZNG/z4448Oy++66y707t0bsbGxEKvad9y4cW73FWwMOl6674PtKC5SQBuVj9gINeIiVJZXpFphMxsFUSCVVxqx++Rl7D55GU3rRaBL03i0ahgFpSK0/nEnIiIiCgYGHS/oDEbsO1sEWRaA4hKH9UpRsISe2AgV4qt+mj4roQyx3y5Q+Pjzcjn+vFxu6eXp1CQOCezlISIiomsYg44XlKKILx+7Ff/ZuBW/GxuisMKAK+V6FJXrUVxhgEGScbG0Ehdd3DAerVHa9ABZv7Qqkb1BVGP2vTydm8bh+obR7OUhIiKiaw6DjhcUooBOTWJxqoGM+Ih4yILCss4oySiu0KOw3PlLb5RRojOgRGfAmSvlDvtWK0TERjgPQjFaFRQiQxB5x7qXp33VvTwxap5HREREdG1g0PEThSggPlLt9AGPsiyjXG9EUbkBV8orUVRusAlBJToDKo0SCkoqUVDi2BskAIjRKm2Gw1kPi9OqFA7bEJmVVxrx88nL+PnkZTSJU0MD4LczhVCrVVArRKgUIlRKESpRsLxXigLUChEiAzYREYWguvoAS/KcP/6MGXRqgSAIiFQrEalWIilO67DeYJRQVGEbfoqs3hskGUUVBhRVGPDnZcfeII1SdDkkLlqrhMghcVTl7JVytASw6cgFmx5JV5SicDX4KE2hyP69SilaApNSITh9r1IIUCrM5QQoRIFDNYmIyGsqlQoAUFZWhoiIiCDXhgKprKwMwNU/c18w6IQApUJEQpTa6c3jsiyjrNLockhcWaUROoOE/GId8ot1DtuLAhCjdR6C4iJUUCt57wa5ZpBkGCqN1Rf0kiAAKqvgYwlBSgFK0RSO1HbvTcHK6n3VtvbBigGKiCh8KRQKxMfHIz8/HwAQGRlp+XdfkiRUVlaioqLCMv0x+U9tta8syygrK0N+fj7i4+OhUPg+colBJ8QJgoAojRJRGiUaxzv+5kJvlCw9QFfseoKKyg0wyrLlszMRKoVlVrj4CLXNz2iNkheNFBCyDFQaJFQa/P/QU3NwMgUpx1BkE5CqeqtMYcrJe6veKt4nR0SekmUZRkmGUZYhSYBBklBZafr/cFG5HhpJgFIUIApVP/nvi1eSkpIAwBJ2zGRZRnl5OSIiInj9EgC13b7x8fGWP2tfMejUcSqFiAbRGjSI1jisk2XTBAj2vUDme4TK9UbL61yR474VooBYrfMJEmIjVFBxJi8KQXqjDL3RiHL4tydKFASolFVhqSoEmXullFWBSgVTcNt14hI0arXNOlfvVQoO4yPyhSzLkGTTZEDmUGF5L8mQ7D7br78aRFyXkWQZBpv9AUZJMv2UZRiNEowyIEmmctbHtCfIRrQEsGz7SYehw4JgGiqsEEUoRJh+CoCi6t8bhWAa8mv/Mock6zJKhTlAiRBF04yxTsubt1HY7l9ZB4YWC4KA5ORkNGrUCHr91V/k6vV6bN68Gb17967RcCdyrjbbV6VS1agnx4xBJ4wJgoAYrWnWtqb1HNfrDEaHiRHMr+IKPYySjMtlelwuc94bFKlWuBwSx4enUriRZBk6vQyd3nUvlPlCZufxSx7dA2Vm3ZtkPzGEWlE1hE9pCkUqq/ue7HuozO9V7IUiP5FlGbIMm4t4g1QVDlyEAvM6g9F52DD3cDgLDpLd/i3v7YKIuVy4kOWrv6Qx8f+QYW/YBCqHAGUXlkSr3ilFVVhzEsyUdp+t92tf3jqkiQJcXk8oFAqbi2GFQgGDwQCtVsugEwB1sX0ZdK5hGqUCDWMUaBjj2BskSabeoCsuhsXpDBLKKo0oqzQir7DCYXtXD0+Nj1Ahhg9PJbJRaZBgmm/Rvxc3CvFqr5F1QLJ5rxShEu3eK531PF3dti78xreucQgOTi76XfY0OAQHx304BBG7fUqyDKPegAQAS7Yeh14WLcfl5FbXHle9UsGiFJ33PF0NXKaeMBGmOv/v4HmolCq3PWDmYHZ1CKFjD5hDkOMwwzqHQYecEkUBsVXhxJkKvfMJEmr68NR4rcj/qRL5ielixYgK552yPjNPJuE2QFneu1hn7qESbd8H8iJCtu6RcDOsSTIPSzL3NPgwNKq63g/bYVOmHsNgE2QjEgCUVRoh81qOQojBwx48c6/64XPFXvWqe0MUhKvDC6t+uh9C6NgDZu7Rsh5m6FEPmMI2dHEG0+ox6JBPtCoFtCoFEmMdp8uu6cNTNQoFYiL+5MNTiULU1ckkAH/3Qjmb0twyRblgutDJOXAekiC6HjZlE2auDo0KhTBBRHWbJMuQjAjJYYbWPVXWYcgxQFnfC+YYzFz1aEUo696/oQw65HfePjzV+iGqJToDdEYBumoenupqWJyGD08lqtPcTWlu/m3tkfOB+20tEVFdUpvDDNsnRdbKcfyJQYdqVXUPTzUa9Igt/AP7paa4UmG0GRJn//DU03x4KhERERG5wKBDIUWpEJEYAZRFRDr8xpYPTyUiIiIiTzHoUJ3hy8NTzT+9eXjq1WFxfHgqERERUV3lU9CZP38+3njjDeTl5aFjx47IzMxEr169nJb94Ycf8Oyzz+LQoUMoKytD8+bN8dhjj+Gpp56qUcWJ7Pny8FTzA1RtH57qOF02H55KREREVLd4HXRWrFiBKVOmYP78+ejZsycWLlyIQYMG4cCBA2jWrJlD+aioKEyaNAldunRBVFQUfvjhBzz22GOIiorCo48+6pcvQVSdQD88NUqtQCwfnkpEREQUMrwOOvPmzUN6ejomTJgAAMjMzMS3336LBQsWYO7cuQ7lu3Xrhm7dulk+t2jRAl9++SW2bNnCoEMho7qHpxZX9QbZPzz1SrkelQYJpZVGlPLhqUREREQhw6ugU1lZid27d2P69Ok2y9PS0rBt2zaP9rFnzx5s27YNc+bMcVlGp9NBp7t6M3lRUREAQK/XQ6/385PvvGQ+viAHd970cGVu11BqX4UAxGtFxGs1QD3HIGR6eKrBEnyKyg0orNDjSrkBJR49PNV8b5DSFIK0V99rVaJfe4NCsX3DCds3sNi+gcX2DSy2b+CxjQNLlkztGuxrcW/q4FXQKSgogNFoRGJios3yxMREnDt3zu22TZs2xYULF2AwGDBz5kxLj5Azc+fOxaxZsxyWb9iwAZGRoTGHd4uKo8GuQlirc+2rqnrF2i42SMBlHVCgE3CxAiioEHBRB1ysEFBQAegkASU6I0p0Rpy54rhbjUJGAw1QX2v1Uws00MqopwZ8vTWozrVvHcP2DSy2b2CxfQOL7Rt4bOPAqDxh+pmTkxPUegBAWVmZR+V8mozA/jfMsixX+1vnLVu2oKSkBNu3b8f06dNx/fXX48EHH3Ra9rnnnsPUqVMtn4uKipCSkoK0tDTExsY63aa26PV65OTk4IS2FR9YFwCCbESLiqPh1b5RgBZAk6qXmenhqVLVvUBW9wdVmB+eaoTOKOBMGXCmzPHvl+3DU5WI06oQF1n1M0IJrZOHp4Zl+4YQtm9gsX0Di+0bWGzfwGMbB1bbxEhUntiLAQMGQKVSBbUu5tFe1fEq6DRo0AAKhcKh9yY/P9+hl8dey5YtAQCdO3fG+fPnMXPmTJdBR6PRQKNxHCKkUqmC3rBmsqDgX6IAuibaVwAiNECERo2keMfVBqOEogrHCRKcPzzVcXtnD0+N1yoQCaBIkCEqZChEAUpRhCg4/gKDfHdNnL9BxPYNLLZvYLF9A49tHBiCaGrTULge9/T4XgUdtVqN7t27IycnB0OHDrUsz8nJwZAhQzzejyzLNvfgEJEjpUJEQpQaCVFqh3U1e3iqEsAph32aQo8ARdVLafkpOixzfC+6Xe+wH0XVcsH2PcMWERER+YvXQ9emTp2KUaNGoUePHkhNTcWiRYtw6tQpTJw4EYBp2NmZM2fw0UcfAQDef/99NGvWDO3atQNgeq7Om2++icmTJ/vxaxBdWzx9eKp1L5B5trhyXSX0sgijJNtsY5Rkh2W1zdNAZRvE3K/3NrQxbBEREYUHr4POiBEjcPHiRcyePRt5eXno1KkT1q1bh+bNmwMA8vLycOrU1d8WS5KE5557DsePH4dSqUSrVq3w6quv4rHHHvPftyAiG64enirIRrQsP4LjEa0gQYRRNoUbg1G2fV8VegySZHlv+uz43rqMs/XWZZytt2Ze53x+utqhEOx7ogQoFFW9T1XvlaJoKqewC0wCcEQSUKS+AoVC4XNoY9giIqJAkWXT/3slGZAhQ5Zherl4L0EGZOBCsQ5B/R+0D3yajCAjIwMZGRlO12VnZ9t8njx5MntviEKQIAhQCgKUIqDx6V+CmpNlGZIMp2HJIShVhTGDJDu+9yBQuQplBvuwJZv2CZ9nJ1UAuFSjdhEF2AShGvdymYOakyGDrnq5RIYtIqohWZYho+rC2fq99UW05aLbRVm79xIAyNblq/YFAJIR5ToB50rLIEGAXLVfU3mr41pvZ7fcUt7DAFBdfW33aVtfh7LWdTC3iX19nbw3/2/sannn382mrXzUrb6IB2qwfW0L0uUNEZEpbCkEQCEG76ZR8/8kPOmdqrb3ymhEpP4KLguxMFr26RjMDJJkCVTmba3/xyPJQKVRqkHYqjlRgEOYsr/vyt/3ctmEMlGAKIZv2DL/RhWouuiwuviwnA22P2C1iU0Z63Pn6r6s9i87rrfeh2WZbMTFCqAQesiCZKmjzUWRdT3tv4PdsSzHsPlutm8sdaimjtZlbL6vk3a0qY9dA9nX0+9tbb8v6+PIMv7UCyhQFUKSBacX/fZhQJJhd9Fd8wtqtxfy1ttZ1wHVXEQ7eR+8kdAKAO4feUKeEwVAgABBMN/LKwW7Sl5h0CGia9rVsCVADR8fTGTel2xEy/JLOB7RyOsZfyQvhvu5651yt766/VhfmEgyIBll6I0yTJdNtU8QYApC5mGCggCtoEClcNrpRS3g48V3Nest+/Lw4tv64tdVPUOXEsDpYFcijCkAXAx2JUKOANPfd0EQHN6be5dF8zLBXN6ubNVyrayDXtQ6lnXx3vz7FNGyv6r1Vhf41stFmBZYBwDBzfur5a3qa13WbrkowK68Y5s4tI8AiNV8T/P7q+3p/nua31trnxQJ3bHdATkHAoVBh4goBIiiAHWQezAkyarnyRyEjFVBSLa6f8vqva/3aLkqYx22ZBnQG2XoIQMG07JCCACC/1Tuukio+o9gs+zqAgGACAkSRAiC1XrLxle3vbov23NWEGzX2+zDar3NvuwupuzraV0HwaqQs+9h/zfI4Xs4q4NdGetltvWxWm+zneP3uLrMej8yoowlKFPGVF1MurjAtVwE1+AC17qs1QVudRf99gGg2gt563p4eyFv9T395ep9qE04vTQBYNAhIqIqoihAhAAnz5qtNZLsOgwZjQY0Kj+FPE0KICgcL3adXNTaXjDD8sHTi29vLmp9uvh2ESBs9mGzjc0BnF58uwoQ1bGerIQXif53tX0T2b5EtYRBh4iIQoYoCBAVzsOWICvRUg0oIiJ4oUhERNWq2YB0IiIiIiKiEMSgQ0REREREYYdBh4iIiIiIwg6DDhERERERhR0GHSIiIiIiCjsMOkREREREFHYYdIiIiIiIKOww6BARERERUdhh0CEiIiIiorDDoENERERERGFHGewKEBERERGRf4mCAJVSgFohQmV5CVArTe/VChEqZdUyqzJqpWBVXoRaaSoLyYANx4L9rbzDoENEREREFET+DiUqhQCFKEAQBL/VUa+X/Lav2sKgQxSmREFAbIQSsVoV4iJUiItUIUop4PfdRzCkaxMICgUkWYYkA3LVT9NnGbLlPao+V72X3Jc3VrPeYX/Wx5es17suL8vBblkiIrqWuQsllmVBDiVkwqBDVIdFaRSmEBOhQqxWhVjz+wgVYjRKiKLtP5p6vR6/A2iaEAGVShWcStdQtSFKurrO69Al2Qct63XV789o1EN3DGidGANZEK3KO9u3m9BnF+ysQyAREXmOoeTaxqBDFMLUStESXkxhRnn1fYQKKsW1N5+IIAhQCIACofc/Gb1ej3XHgLQOiQEJkrLToORFsJNchDg3Ic+joCjZBzX/9+YZJYY8onBX01AiyhJ+3noEo1KbI1KjYSghBh2iYHI2vMzyPkIFrUrkP9BkIQgCBAEQQzDk1YbKykp8880RPNLrOiiUyqs9aZIpEBkl25BlrAp2puW24c7VOsvnqvBmlM37vhq8jDa9hlafq45/dZurYc6mbpJ5v8FuUSLfhWJPiV6vBwDEalVQqRT++qpUhzHoEAWYeXiZOcBUN7yMiJwzXwCplWJYXMS4CmdX74fzLLjZhzNvgptNODMagNNA03qRkATRZXAz180c1hjcQp8/QonNTfHsKaE6gkGHqIY4vIyIfCEIApSK0LlQ1Ov1WHd6D4bc0NinoZeeBDdzIDIHMF961TwObi6CY3XBzbpuwcBQQuQ/PgWd+fPn44033kBeXh46duyIzMxM9OrVy2nZL7/8EgsWLMDevXuh0+nQsWNHzJw5E3feeWeNKk5UWzi8jIioeqEW3GrKVa+au3DkrlfNoDfgxN4j6Nc+EREaFUMJUS3wOuisWLECU6ZMwfz589GzZ08sXLgQgwYNwoEDB9CsWTOH8ps3b8aAAQPwyiuvID4+HkuWLMHdd9+NHTt2oFu3bn75EkQ1xeFlRERkTRQFv94Pp9frcWIv0C4pps7OeklU13gddObNm4f09HRMmDABAJCZmYlvv/0WCxYswNy5cx3KZ2Zm2nx+5ZVX8J///AdfffUVgw7VGg4vIyIiIrq2eBV0KisrsXv3bkyfPt1meVpaGrZt2+bRPiRJQnFxMRISErw5NJFbHF5GRERERNa8CjoFBQUwGo1ITEy0WZ6YmIhz5855tI+33noLpaWluP/++12W0el00Ol0ls9FRUUATN2+5qkDg8V8fEE2BrUe4crcrs7aN0qjQKxWhRitqUcmtmqoWUyEEtFqd8PLJBgMUgBrXXeYz99g/z0KV2zfwGL7BhbbN7DYvoHHNg6sUGpfT+vg02QE9r8Zl2XZo9+Wf/rpp5g5cyb+85//oFGjRi7LzZ07F7NmzXJYvmHDBkRGRnpf4QBoUXE02FUIa07bt9z0Qw/gYtWLfJOTkxPsKoQ1tm9gsX0Di+0bWGzfwGMbB1YotG9ZWZlH5bwKOg0aNIBCoXDovcnPz3fo5bG3YsUKpKenY+XKlejfv7/bss899xymTp1q+VxUVISUlBSkpaUhNjbWmyr7nV6vR05ODk5oW0EW6v5zHGqTKAiI0ZqGl8VqlYiNVCFGU3Xjv1YJjUqEwWBATk4OBgwYwJs1A8B8/rJ9A4PtG1hs38Bi+wYW2zfw2MaBFUrtax7tVR2vgo5arUb37t2Rk5ODoUOHWpbn5ORgyJAhLrf79NNPMX78eHz66af461//Wu1xNBoNNBqNw3KVShX0hjWTBQWDjhM1nb3M3DMYSn/W4YjtG1hs38Bi+wYW2zew2L6BxzYOrFBoX0+P7/XQtalTp2LUqFHo0aMHUlNTsWjRIpw6dQoTJ04EYOqNOXPmDD766CMAppAzevRovP3227j11lstvUERERGIi4vz9vAUZJy9jIiIiIjqAq+DzogRI3Dx4kXMnj0beXl56NSpE9atW4fmzZsDAPLy8nDq1ClL+YULF8JgMOCJJ57AE088YVk+ZswYZGdn1/wbkF9x9jIiIiIiCgc+TUaQkZGBjIwMp+vsw0tubq4vh6AA4sMxiYiIiCjc+RR0KLRZDy+zHlrG4WVEREREdK1g0KmDOLyMiIiIiMg9Bp0QxeFlRERERES+Y9AJEg4vIyIiIiIKHAadADE/HNMSYDi8jIiIiIio1jDo1ACHlxERERERhSYGHR890us6REVogl0NIiIiIiJygjeC+EitZNMREREREYUqXq0TEREREVHYYdAhIiIiIqKww6BDRERERERhh0GHiIiIiIjCDoMOERERERGFHQYdIiIiIiIKOww6REREREQUdhh0iIiIiIgo7CiDXQFPyLIMACgqKgpyTQC9Xo+ysjIUFRVBpVIFuzphh+0bWGzfwGL7BhbbN7DYvoHF9g08tnFghVL7mjOBOSO4UieCTnFxMQAgJSUlyDUhIiIiIqJQUFxcjLi4OJfrBbm6KBQCJEnC2bNnERMTA0EQglqXoqIipKSk4PTp04iNjQ1qXcIR2zew2L6BxfYNLLZvYLF9A4vtG3hs48AKpfaVZRnFxcVo3LgxRNH1nTh1okdHFEU0bdo02NWwERsbG/Q/5HDG9g0stm9gsX0Di+0bWGzfwGL7Bh7bOLBCpX3d9eSYcTICIiIiIiIKOww6REREREQUdhh0vKTRaDBjxgxoNJpgVyUssX0Di+0bWGzfwGL7BhbbN7DYvoHHNg6suti+dWIyAiIiIiIiIm+wR4eIiIiIiMIOgw4REREREYUdBh0iIiIiIgo7DDpERERERBR2GHTsbN68GXfffTcaN24MQRCwZs2aarfZtGkTunfvDq1Wi+uuuw4ffPBB4CtaR3nbvrm5uRAEweF16NCh2qlwHTJ37lzcdNNNiImJQaNGjXDvvffi8OHD1W7H89czvrQvz1/PLViwAF26dLE8iC41NRXffPON22147nrO2/bluVszc+fOhSAImDJlittyPId940n78hz2zsyZMx3aKikpye02deH8ZdCxU1paiq5du+K9997zqPzx48cxePBg9OrVC3v27ME//vEP/P3vf8eqVasCXNO6ydv2NTt8+DDy8vIsr9atWweohnXXpk2b8MQTT2D79u3IycmBwWBAWloaSktLXW7D89dzvrSvGc/f6jVt2hSvvvoqfvrpJ/z000+44447MGTIEOzfv99peZ673vG2fc147npv165dWLRoEbp06eK2HM9h33javmY8hz3XsWNHm7bat2+fy7J15vyVySUA8urVq92WmTZtmtyuXTubZY899ph86623BrBm4cGT9v3+++9lAPLly5drpU7hJD8/XwYgb9q0yWUZnr++86R9ef7WTL169eQPP/zQ6TqeuzXnrn157vqmuLhYbt26tZyTkyP36dNHfvLJJ12W5TnsPW/al+ewd2bMmCF37drV4/J15fxlj04N/fjjj0hLS7NZduedd+Knn36CXq8PUq3CT7du3ZCcnIx+/frh+++/D3Z16oTCwkIAQEJCgssyPH9950n7mvH89Y7RaMRnn32G0tJSpKamOi3Dc9d3nrSvGc9d7zzxxBP461//iv79+1dbluew97xpXzOew577/fff0bhxY7Rs2RIPPPAAjh075rJsXTl/lcGuQF137tw5JCYm2ixLTEyEwWBAQUEBkpOTg1Sz8JCcnIxFixahe/fu0Ol0WLZsGfr164fc3Fz07t072NULWbIsY+rUqfjLX/6CTp06uSzH89c3nrYvz1/v7Nu3D6mpqaioqEB0dDRWr16NDh06OC3Lc9d73rQvz13vffbZZ/j555+xa9cuj8rzHPaOt+3Lc9g7t9xyCz766CO0adMG58+fx5w5c3Dbbbdh//79qF+/vkP5unL+Muj4gSAINp9lWXa6nLzXtm1btG3b1vI5NTUVp0+fxptvvsl/qNyYNGkSfv31V/zwww/VluX56z1P25fnr3fatm2LvXv34sqVK1i1ahXGjBmDTZs2ubwY57nrHW/al+eud06fPo0nn3wSGzZsgFar9Xg7nsOe8aV9eQ57Z9CgQZb3nTt3RmpqKlq1aoWlS5di6tSpTrepC+cvh67VUFJSEs6dO2ezLD8/H0ql0mkCppq79dZb8fvvvwe7GiFr8uTJWLt2Lb7//ns0bdrUbVmev97zpn2d4fnrmlqtxvXXX48ePXpg7ty56Nq1K95++22nZXnues+b9nWG565ru3fvRn5+Prp37w6lUgmlUolNmzbhnXfegVKphNFodNiG57DnfGlfZ3gOey4qKgqdO3d22V515fxlj04Npaam4quvvrJZtmHDBvTo0QMqlSpItQpve/bsCZku0VAiyzImT56M1atXIzc3Fy1btqx2G56/nvOlfZ3h+es5WZah0+mcruO5W3Pu2tcZnruu9evXz2GGqnHjxqFdu3Z49tlnoVAoHLbhOew5X9rXGZ7DntPpdDh48CB69erldH2dOX+DNAlCyCouLpb37Nkj79mzRwYgz5s3T96zZ4988uRJWZZlefr06fKoUaMs5Y8dOyZHRkbKTz31lHzgwAE5KytLVqlU8hdffBGsrxDSvG3ff/3rX/Lq1avlI0eOyL/99ps8ffp0GYC8atWqYH2FkPX444/LcXFxcm5urpyXl2d5lZWVWcrw/PWdL+3L89dzzz33nLx582b5+PHj8q+//ir/4x//kEVRlDds2CDLMs/dmvK2fXnu1pz9rGA8h/2ruvblOeyd//u//5Nzc3PlY8eOydu3b5fvuusuOSYmRj5x4oQsy3X3/GXQsWOejtD+NWbMGFmWZXnMmDFynz59bLbJzc2Vu3XrJqvVarlFixbyggULar/idYS37fvaa6/JrVq1krVarVyvXj35L3/5i/z1118Hp/Ihzlm7ApCXLFliKcPz13e+tC/PX8+NHz9ebt68uaxWq+WGDRvK/fr1s1yEyzLP3Zrytn157tac/YU4z2H/qq59eQ57Z8SIEXJycrKsUqnkxo0by8OGDZP3799vWV9Xz19BlqvuHCIiIiIiIgoTnIyAiIiIiIjCDoMOERERERGFHQYdIiIiIiIKOww6REREREQUdhh0iIiIiIgo7DDoEBERERFR2GHQISIiIiKisMOgQ0REYU0QBKxZsybY1SAiolrGoENERAEzduxYCILg8Bo4cGCwq0ZERGFOGewKEBFReBs4cCCWLFlis0yj0QSpNkREdK1gjw4REQWURqNBUlKSzatevXoATMPKFixYgEGDBiEiIgItW7bEypUrbbbft28f7rjjDkRERKB+/fp49NFHUVJSYlNm8eLF6NixIzQaDZKTkzFp0iSb9QUFBRg6dCgiIyPRunVrrF27NrBfmoiIgo5Bh4iIguqf//wn7rvvPvzyyy8YOXIkHnzwQRw8eBAAUFZWhoEDB6JevXrYtWsXVq5cif/97382QWbBggV44okn8Oijj2Lfvn1Yu3Ytrr/+eptjzJo1C/fffz9+/fVXDB48GA8//DAuXbpUq9+TiIhqlyDLshzsShARUXgaO3YsPv74Y2i1Wpvlzz77LP75z39CEARMnDgRCxYssKy79dZbceONN2L+/Pn497//jWeffRanT59GVFQUAGDdunW4++67cfbsWSQmJqJJkyYYN24c5syZ47QOgiDghRdewEsvvQQAKC0tRUxMDNatW8d7hYiIwhjv0SEiooC6/fbbbYIMACQkJFjep6am2qxLTU3F3r17AQAHDx5E165dLSEHAHr27AlJknD48GEIgoCzZ8+iX79+buvQpUsXy/uoqCjExMQgPz/f169ERER1AIMOEREFVFRUlMNQsuoIggAAkGXZ8t5ZmYiICI/2p1KpHLaVJMmrOhERUd3Ce3SIiCiotm/f7vC5Xbt2AIAOHTpg7969KC0ttazfunUrRFFEmzZtEBMTgxYtWmDjxo21WmciIgp97NEhIqKA0ul0OHfunM0ypVKJBg0aAABWrlyJHj164C9/+QuWL1+OnTt3IisrCwDw8MMPY8aMGRgzZgxmzpyJCxcuYPLkyRg1ahQSExMBADNnzsTEiRPRqFEjDBo0CMXFxdi6dSsmT55cu1+UiIhCCoMOEREF1Pr165GcnGyzrG3btjh06BAA04xon332GTIyMpCUlITly5ejQ4cOAIDIyEh8++23ePLJJ3HTTTchMjIS9913H+bNm2fZ15gxY1BRUYF//etfePrpp9GgQQMMHz689r4gERGFJM66RkREQSMIAlavXo1777032FUhIqIww3t0iIiIiIgo7DDoEBERERFR2OE9OkREFDQcPU1ERIHCHh0iIiIiIgo7DDpERERERBR2GHSIiIiIiCjsMOgQEREREVHYYdAhIiIiIqKww6BDRERERERhh0GHiIiIiIjCDoMOERERERGFHQYdIiIiIiIKO/8PRmg4pfRWOI0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = \"MLMVN 48-10-11\"\n",
    "image_name = \"results/MLMVN_48-10-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-20-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 20)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(20, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=2937a1c5c09247d3afefe736b93cce93\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2937a1c5c09247d3afefe736b93cce93/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-20-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-20-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-20-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:33:02,557 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.1781452279297529\n",
      "Epoch 19 loss is 0.17121365507481565\n",
      "Epoch 29 loss is 0.23437533994636411\n",
      "Epoch 39 loss is 0.249442526042442\n",
      "Epoch 49 loss is 0.2709493982290144\n",
      "Epoch 59 loss is 0.22188192248227645\n",
      "Epoch 69 loss is 0.24606682756213247\n",
      "Epoch 79 loss is 0.291044741861424\n",
      "Epoch 89 loss is 0.24921788018106592\n",
      "Epoch 99 loss is 0.24654104628164147\n",
      "Epoch 109 loss is 0.2149965350030757\n",
      "Epoch 119 loss is 0.20888710558226523\n",
      "Epoch 129 loss is 0.18029340312631476\n",
      "Epoch 139 loss is 0.21206527705421888\n",
      "Epoch 149 loss is 0.16859814960066902\n",
      "Epoch 159 loss is 0.20565043305802141\n",
      "Epoch 169 loss is 0.20745351853536964\n",
      "Epoch 179 loss is 0.21776656653704443\n",
      "Epoch 189 loss is 0.20066393883063724\n",
      "Epoch 199 loss is 0.17997682184925615\n",
      "Train Acc.:  0.8812759047985301\n",
      "Val Acc.:  0.8766982824916688\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1074\n",
      "           1       0.90      0.83      0.86      1089\n",
      "           2       0.97      0.92      0.95      1044\n",
      "           3       0.94      0.90      0.92      1048\n",
      "           4       0.89      0.91      0.90      1057\n",
      "           5       0.86      0.90      0.88      1072\n",
      "           6       0.83      0.80      0.81      1066\n",
      "           7       1.00      0.98      0.99      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.87      0.89      0.88      1030\n",
      "          10       0.94      0.89      0.92      1012\n",
      "\n",
      "   micro avg       0.92      0.91      0.92     11703\n",
      "   macro avg       0.92      0.91      0.91     11703\n",
      "weighted avg       0.93      0.91      0.91     11703\n",
      " samples avg       0.89      0.91      0.90     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-20-11\"\n",
    "image_name = \"results/MLMVN_48-20-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-50-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 50)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(50, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=b9c663cb49f645d3b526ff62b971b678\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b9c663cb49f645d3b526ff62b971b678/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-50-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-50-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-50-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:36:28,294 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.12631168614962632\n",
      "Epoch 19 loss is 0.08741677563113538\n",
      "Epoch 29 loss is 0.07765135114449577\n",
      "Epoch 39 loss is 0.07199543979859238\n",
      "Epoch 49 loss is 0.05459176181501563\n",
      "Epoch 59 loss is 0.054154946830923104\n",
      "Epoch 69 loss is 0.0498279498984762\n",
      "Epoch 79 loss is 0.04969739252919254\n",
      "Epoch 89 loss is 0.05646039586774165\n",
      "Epoch 99 loss is 0.05206559631806764\n",
      "Epoch 109 loss is 0.04809522582296451\n",
      "Epoch 119 loss is 0.0522341919664429\n",
      "Epoch 129 loss is 0.0495262401204281\n",
      "Epoch 139 loss is 0.04954010092306082\n",
      "Epoch 149 loss is 0.061976915229967955\n",
      "Epoch 159 loss is 0.05182441637727222\n",
      "Epoch 169 loss is 0.05091017378293241\n",
      "Epoch 179 loss is 0.0561740020607465\n",
      "Epoch 189 loss is 0.06101975489402404\n",
      "Epoch 199 loss is 0.06845130197965535\n",
      "Train Acc.:  0.9619706875186942\n",
      "Val Acc.:  0.9557378449970093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      1074\n",
      "           1       0.95      0.92      0.93      1089\n",
      "           2       0.99      0.98      0.98      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.97      0.96      0.97      1057\n",
      "           5       0.95      0.94      0.95      1072\n",
      "           6       0.95      0.94      0.94      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.95      0.95      1030\n",
      "          10       0.99      0.97      0.98      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-50-11\"\n",
    "image_name = \"results/MLMVN_48-50-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=77bda608f5dd4cbf9fb5bf146c9bcdbe\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/77bda608f5dd4cbf9fb5bf146c9bcdbe/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-100-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-100-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-100-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:40:51,411 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.09760215896283031\n",
      "Epoch 19 loss is 0.07483332226588477\n",
      "Epoch 29 loss is 0.06169817582724667\n",
      "Epoch 39 loss is 0.04806632873441776\n",
      "Epoch 49 loss is 0.050257943968239405\n",
      "Epoch 59 loss is 0.045243461048822334\n",
      "Epoch 69 loss is 0.03916302966121763\n",
      "Epoch 79 loss is 0.03600947264373011\n",
      "Epoch 89 loss is 0.0327954003325785\n",
      "Epoch 99 loss is 0.034495561806988345\n",
      "Epoch 109 loss is 0.02976081583721455\n",
      "Epoch 119 loss is 0.03207888606505858\n",
      "Epoch 129 loss is 0.029421696533450108\n",
      "Epoch 139 loss is 0.02886477329200674\n",
      "Epoch 149 loss is 0.028011108627125445\n",
      "Epoch 159 loss is 0.02746762018733399\n",
      "Epoch 169 loss is 0.029159449466333746\n",
      "Epoch 179 loss is 0.028677871059498893\n",
      "Epoch 189 loss is 0.026269004340911915\n",
      "Epoch 199 loss is 0.02659348237350445\n",
      "Train Acc.:  0.9820749476562833\n",
      "Val Acc.:  0.9690677604033154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1074\n",
      "           1       0.96      0.96      0.96      1089\n",
      "           2       1.00      0.99      0.99      1044\n",
      "           3       1.00      0.99      0.99      1048\n",
      "           4       0.97      0.97      0.97      1057\n",
      "           5       0.96      0.96      0.96      1072\n",
      "           6       0.97      0.95      0.96      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.97      0.97      0.97      1030\n",
      "          10       0.99      0.98      0.98      1012\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     11703\n",
      "   macro avg       0.98      0.98      0.98     11703\n",
      "weighted avg       0.98      0.98      0.98     11703\n",
      " samples avg       0.97      0.98      0.97     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-100-11\"\n",
    "image_name = \"results/MLMVN_48-100-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-10-10-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 10)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(10, 10)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(10, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=7aa1a5aeeab040fda27730a9cb90d8ad\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/7aa1a5aeeab040fda27730a9cb90d8ad/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-10-10-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-10-10-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.6741966148101034\n",
      "Epoch 19 loss is 0.6345819678996693\n",
      "Epoch 29 loss is 0.6163333298521995\n",
      "Epoch 39 loss is 0.5973288617286425\n",
      "Epoch 49 loss is 0.5657958380010314\n",
      "Epoch 59 loss is 0.5930215843207252\n",
      "Epoch 69 loss is 0.614107440452483\n",
      "Epoch 79 loss is 0.6680024372385054\n",
      "Epoch 89 loss is 0.670419685702236\n",
      "Epoch 99 loss is 0.6396620484680041\n",
      "Epoch 109 loss is 0.7318713292921706\n",
      "Epoch 119 loss is 0.7326499528623525\n",
      "Epoch 129 loss is 0.7676124397701742\n",
      "Epoch 139 loss is 0.7363896056980302\n",
      "Epoch 149 loss is 0.7518392231032288\n",
      "Epoch 159 loss is 0.6876373192278051\n",
      "Epoch 169 loss is 0.684599693905956\n",
      "Epoch 179 loss is 0.6865932359686486\n",
      "Epoch 189 loss is 0.6739611875627026\n",
      "Epoch 199 loss is 0.6541066273988838\n",
      "Train Acc.:  0.3464940392257403\n",
      "Val Acc.:  0.3492266940100829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.70      0.63      1074\n",
      "           1       0.59      0.43      0.50      1089\n",
      "           2       0.59      0.23      0.33      1044\n",
      "           3       0.76      0.43      0.55      1048\n",
      "           4       0.57      0.27      0.36      1057\n",
      "           5       0.59      0.61      0.60      1072\n",
      "           6       0.40      0.08      0.13      1066\n",
      "           7       0.92      0.80      0.86      1103\n",
      "           8       0.97      0.96      0.97      1108\n",
      "           9       0.17      0.03      0.05      1030\n",
      "          10       0.49      0.28      0.36      1012\n",
      "\n",
      "   micro avg       0.67      0.44      0.53     11703\n",
      "   macro avg       0.60      0.44      0.48     11703\n",
      "weighted avg       0.60      0.44      0.49     11703\n",
      " samples avg       0.39      0.44      0.41     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-10-10-11\"\n",
    "image_name = \"results/MLMVN_48-10-10-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-20-20-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 20)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(20, 20)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(20, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=63f6d3900d26435baf5c0451ee2550cf\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/63f6d3900d26435baf5c0451ee2550cf/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-20-20-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-20-20-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.4918386432669506\n",
      "Epoch 19 loss is 0.5658443724932558\n",
      "Epoch 29 loss is 0.547344653493303\n",
      "Epoch 39 loss is 0.5003598398694803\n",
      "Epoch 49 loss is 0.45764246002628084\n",
      "Epoch 59 loss is 0.48134115218311874\n",
      "Epoch 69 loss is 0.46767609185504994\n",
      "Epoch 79 loss is 0.47236872848369066\n",
      "Epoch 89 loss is 0.45542004995148055\n",
      "Epoch 99 loss is 0.4707600822371793\n",
      "Epoch 109 loss is 0.4592737186466213\n",
      "Epoch 119 loss is 0.44952235101712684\n",
      "Epoch 129 loss is 0.4279650487928361\n",
      "Epoch 139 loss is 0.4390951851709622\n",
      "Epoch 149 loss is 0.4078203854663746\n",
      "Epoch 159 loss is 0.4626180199815107\n",
      "Epoch 169 loss is 0.4595541584940826\n",
      "Epoch 179 loss is 0.41118996909517436\n",
      "Epoch 189 loss is 0.4143808650128177\n",
      "Epoch 199 loss is 0.42120076453410465\n",
      "Train Acc.:  0.6030850745630902\n",
      "Val Acc.:  0.6048876356489788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1074\n",
      "           1       0.71      0.70      0.71      1089\n",
      "           2       0.79      0.67      0.73      1044\n",
      "           3       0.85      0.70      0.77      1048\n",
      "           4       0.78      0.54      0.64      1057\n",
      "           5       0.72      0.72      0.72      1072\n",
      "           6       0.71      0.45      0.55      1066\n",
      "           7       0.98      0.91      0.95      1103\n",
      "           8       0.98      0.96      0.97      1108\n",
      "           9       0.85      0.31      0.46      1030\n",
      "          10       0.70      0.63      0.66      1012\n",
      "\n",
      "   micro avg       0.82      0.67      0.74     11703\n",
      "   macro avg       0.82      0.67      0.72     11703\n",
      "weighted avg       0.82      0.67      0.73     11703\n",
      " samples avg       0.64      0.67      0.65     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-20-20-11\"\n",
    "image_name = \"results/MLMVN_48-20-20-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-50-50-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 50)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(50, 50)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(50, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=3ec0ba0e69fe4e8781f1f56f68bb4df0\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/3ec0ba0e69fe4e8781f1f56f68bb4df0/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-50-50-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-50-50-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.17595385331554345\n",
      "Epoch 19 loss is 0.24443298597993288\n",
      "Epoch 29 loss is 0.34861398016646006\n",
      "Epoch 39 loss is 0.35088833894092397\n",
      "Epoch 49 loss is 0.3184690939966281\n",
      "Epoch 59 loss is 0.29159015152507245\n",
      "Epoch 69 loss is 0.29664844727356443\n",
      "Epoch 79 loss is 0.3148698996217653\n",
      "Epoch 89 loss is 0.34718880768462584\n",
      "Epoch 99 loss is 0.32786482129724753\n",
      "Epoch 109 loss is 0.3249049482370644\n",
      "Epoch 119 loss is 0.31320303291619195\n",
      "Epoch 129 loss is 0.30243110019968583\n",
      "Epoch 139 loss is 0.30238450965742614\n",
      "Epoch 149 loss is 0.29449084419928934\n",
      "Epoch 159 loss is 0.2935125197293862\n",
      "Epoch 169 loss is 0.2872804696457938\n",
      "Epoch 179 loss is 0.29204265563094745\n",
      "Epoch 189 loss is 0.3058525140885775\n",
      "Epoch 199 loss is 0.30025929403593016\n",
      "Train Acc.:  0.827223005597573\n",
      "Val Acc.:  0.820986071947364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94      1074\n",
      "           1       0.87      0.82      0.84      1089\n",
      "           2       0.94      0.90      0.92      1044\n",
      "           3       0.92      0.88      0.90      1048\n",
      "           4       0.87      0.78      0.82      1057\n",
      "           5       0.86      0.87      0.87      1072\n",
      "           6       0.81      0.72      0.77      1066\n",
      "           7       0.99      0.96      0.98      1103\n",
      "           8       1.00      0.99      0.99      1108\n",
      "           9       0.85      0.80      0.82      1030\n",
      "          10       0.92      0.87      0.89      1012\n",
      "\n",
      "   micro avg       0.91      0.87      0.89     11703\n",
      "   macro avg       0.91      0.87      0.89     11703\n",
      "weighted avg       0.91      0.87      0.89     11703\n",
      " samples avg       0.84      0.87      0.85     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-50-50-11\"\n",
    "image_name = \"results/MLMVN_48-50-50-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(100, 100)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=d2c79e60ea7040fd879fc5e297a36346\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d2c79e60ea7040fd879fc5e297a36346/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-100-100-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-100-100-11]\",\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.14845134117007924\n",
      "Epoch 19 loss is 0.1329600740131187\n",
      "Epoch 29 loss is 0.1297421221557918\n",
      "Epoch 39 loss is 0.11658710957349895\n",
      "Epoch 49 loss is 0.1263711960559784\n",
      "Epoch 59 loss is 0.15283946197204235\n",
      "Epoch 69 loss is 0.20268996749466878\n",
      "Epoch 79 loss is 0.26901874406663706\n",
      "Epoch 89 loss is 0.3079409409709422\n",
      "Epoch 99 loss is 0.28158223191896325\n",
      "Epoch 109 loss is 0.27721301741939225\n",
      "Epoch 119 loss is 0.23720462062688716\n",
      "Epoch 129 loss is 0.23388674674232188\n",
      "Epoch 139 loss is 0.22834769155604162\n",
      "Epoch 149 loss is 0.22821928822954132\n",
      "Epoch 159 loss is 0.1975671042738866\n",
      "Epoch 169 loss is 0.18680390982844616\n",
      "Epoch 179 loss is 0.18587410530287377\n",
      "Epoch 189 loss is 0.19314962951484493\n",
      "Epoch 199 loss is 0.1799970473113\n",
      "Train Acc.:  0.888774943383327\n",
      "Val Acc.:  0.8714859437751004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      1074\n",
      "           1       0.92      0.88      0.90      1089\n",
      "           2       0.97      0.91      0.94      1044\n",
      "           3       0.93      0.92      0.92      1048\n",
      "           4       0.90      0.88      0.89      1057\n",
      "           5       0.90      0.89      0.90      1072\n",
      "           6       0.84      0.79      0.81      1066\n",
      "           7       1.00      0.98      0.99      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.89      0.88      0.89      1030\n",
      "          10       0.95      0.91      0.93      1012\n",
      "\n",
      "   micro avg       0.93      0.91      0.92     11703\n",
      "   macro avg       0.93      0.91      0.92     11703\n",
      "weighted avg       0.93      0.91      0.92     11703\n",
      " samples avg       0.89      0.91      0.90     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "list_losses = []\n",
    "list_scores = []\n",
    "list_acc = []\n",
    "list_loss = []\n",
    "list_f1 = []\n",
    "list_precision = []\n",
    "list_recall = []\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    model_dict: dict = {}\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n",
    "        X, y, neuronCats, train_index, test_index\n",
    "    )\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    list_scores.append(scores)\n",
    "    list_losses.append(losses)\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "    list_f1.append(\n",
    "        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n",
    "    )\n",
    "    list_precision.append(\n",
    "        precision_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "    list_recall.append(\n",
    "        recall_score(\n",
    "            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_mean\",\n",
    "    value=np.mean(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_accuracy_std\",\n",
    "    value=np.std(list_acc),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_mean\",\n",
    "    value=np.mean(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_f1_std\",\n",
    "    value=np.std(list_f1),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_mean\",\n",
    "    value=np.mean(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_precision_std\",\n",
    "    value=np.std(list_precision),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_mean\",\n",
    "    value=np.mean(list_recall),\n",
    ")\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"val_recall_std\",\n",
    "    value=np.std(list_recall),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"MLMVN 48-100-100-11\"\n",
    "image_name = \"results/MLMVN_48-100-100-11.png\"\n",
    "plot_loss_acc_list(title, list_losses, list_scores, image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
