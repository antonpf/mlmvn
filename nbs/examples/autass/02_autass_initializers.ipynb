{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensorless Drive Diagnosis\n",
    "\n",
    "> In this example, the main focus is the classification of individual states of a motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlmvn.layers import FirstLayer, HiddenLayer, OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss\n",
    "from mlmvn.optim import MySGD, ECL\n",
    "import mlmvn.init as cmplx_init\n",
    "from pathlib import Path\n",
    "from clearml import Task, Logger\n",
    "\n",
    "torch.manual_seed(0)  #  for repeatable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# --- helper functions ---\n",
    "def reverse_one_hot(x, neuronCats):\n",
    "    a = np.zeros(len(x))\n",
    "    x = torch.detach(x)\n",
    "    for i in range(len(x)):\n",
    "        a[i] = torch.max(x[i]) - 1 + np.argmax(x[i]) * neuronCats\n",
    "    return a\n",
    "\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    out = out.type(torch.double)\n",
    "    yb = yb.type(torch.double)\n",
    "    x = 0\n",
    "    for i in range(len(out)):\n",
    "        x += torch.equal(out[i], yb[i])\n",
    "    return x / len(out)\n",
    "\n",
    "\n",
    "def prepare_data(x_train, x_valid, y_train, y_valid, neuronCats):\n",
    "    # one-hot encoding\n",
    "    numSamples, numFeatures = x_valid.shape\n",
    "    y_valid_int = y_valid\n",
    "    y2 = y_valid + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    numClasses = max(y2)\n",
    "    target_ids = range(numClasses)\n",
    "    no = int(np.ceil(numClasses / neuronCats))  # number of output neurons\n",
    "    if no != 1:\n",
    "        y_valid = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_valid[i, k] = c\n",
    "    numSamples, numFeatures = x_train.shape\n",
    "    y_train_int = y_train\n",
    "    y2 = y_train + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    if no != 1:\n",
    "        y_train = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_train[i, k] = c\n",
    "    del y2\n",
    "\n",
    "    # Convert numpy arrays into torch tensors\n",
    "    x_train, y_train, x_valid, y_valid = map(\n",
    "        torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "    )\n",
    "    if y_train.size().__len__() == 1:\n",
    "        y_train = torch.unsqueeze(y_train, 1)\n",
    "        y_valid = torch.unsqueeze(y_valid, 1)\n",
    "\n",
    "    # convert angles to complex numbers on unit-circle\n",
    "    x_train = torch.exp(1.0j * x_train)\n",
    "    x_valid = torch.exp(1.0j * x_valid)\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data(X, y, neuronCats):\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=46806, random_state=42\n",
    "    )\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data_by_index(X, y, neuronCats, train_index, test_index):\n",
    "    x_train, x_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# --- Plots ---\n",
    "def plot_loss(title, losses, scores):\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    fig.suptitle(\"CVNN - Moons\")\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.legend([\"Acc\", \"Loss\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weights(title, ylabel_1, ylabel_2, weights_real, weights_imag):\n",
    "    # y_min = np.min([np.min(weights_real), np.min(weights_imag)])\n",
    "    # y_max = np.max([np.max(weights_real), np.max(weights_imag)])\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(14, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax[0].plot(np.linspace(1, len(weights_real), len(weights_real)), weights_real)\n",
    "    ax[0].set_xlabel(\"Step\")\n",
    "    ax[0].set_ylabel(ylabel_1)\n",
    "    # ax[0].set_title(\"Real Valued Weigts\")\n",
    "    ax[0].set_xlim(0, len(weights_real))\n",
    "    # ax[0].set_ylim(y_min, y_max)\n",
    "\n",
    "    ax[1].plot(np.linspace(1, len(weights_imag), len(weights_imag)), weights_imag)\n",
    "    ax[1].set_xlabel(\"Step\")\n",
    "    ax[1].set_ylabel(ylabel_2)\n",
    "    # ax[1].set_title(\"Imaginary Valued Weights\")\n",
    "    ax[1].set_xlim(0, len(weights_imag))\n",
    "    # ax[1].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_acc_list(title, list_losses, list_scores, image_name):\n",
    "    losses = np.mean(list_losses, axis=0)\n",
    "    scores = np.mean(list_scores, axis=0)\n",
    "\n",
    "    losses_std = np.std(list_losses, axis=0)\n",
    "    scores_std = np.std(list_scores, axis=0)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(losses), len(losses)),\n",
    "        losses + losses_std,\n",
    "        losses - losses_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(scores), len(scores)),\n",
    "        scores + scores_std,\n",
    "        scores - scores_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.legend([\"Loss Mean\", \"Loss Std\", \"Acc. Mean\", \"Acc. Std\"])\n",
    "    fig.savefig(image_name, format=\"png\", dpi=600)\n",
    "\n",
    "    plt.show()\n",
    "    # save\n",
    "    # fig.savefig(image_name + \".svg\", format=\"svg\", dpi=600)\n",
    "\n",
    "\n",
    "# --- Logging ---\n",
    "model_dict: dict = {}\n",
    "\n",
    "\n",
    "def fc_hook(layer_name, module, grad_input, grad_output):\n",
    "    if layer_name in model_dict:\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output\n",
    "    else:\n",
    "        model_dict[layer_name] = {}\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# control variables\n",
    "# number of categories a neuron can distinguish / parameter that determines the number of output neurons\n",
    "neuronCats = 1\n",
    "# number of categories per neuron, i.e. neuronCats (+ 1 for others in case of multiple Outputs)\n",
    "categories = 2\n",
    "# how often a classification sector occurs (1 means no periodicity)\n",
    "periodicity = 1\n",
    "# path to store best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\n",
    "    \"data/autass_data2.csv\",\n",
    "    header=None,\n",
    "    dtype=np.double,\n",
    ")\n",
    "data = np.array(train_csv.values[:, 1:50])\n",
    "del train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0:48]\n",
    "y = data[:, 48].astype(int) - 1\n",
    "\n",
    "yt = copy.copy(y)\n",
    "yt[yt == 0] = 20\n",
    "yt[yt == 1] = 21\n",
    "yt[yt == 2] = 22\n",
    "yt[yt == 3] = 23\n",
    "yt[yt == 4] = 26\n",
    "yt[yt == 5] = 24\n",
    "yt[yt == 6] = 27\n",
    "yt[yt == 7] = 29\n",
    "yt[yt == 8] = 30\n",
    "yt[yt == 9] = 25\n",
    "yt[yt == 10] = 28\n",
    "yt -= 20\n",
    "y = yt\n",
    "del yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 538\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "    def initialize_weights(self, initilizer=\"uniform\"):\n",
    "        if initilizer == \"uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"ones\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"zeros\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"xavier_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"xavier_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"standard\":\n",
    "            pass\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initilizers = [\n",
    "    \"uniform\",\n",
    "    \"normal\",\n",
    "    # \"zeros\",\n",
    "    # \"ones\",\n",
    "    \"kaiming_normal\",\n",
    "    \"kaiming_uniform\",\n",
    "    \"xavier_normal\" \"xavier_uniform\",\n",
    "    \"trabelsi_standard_glorot\",\n",
    "    \"trabelsi_independent_glorot\",\n",
    "    \"trabelsi_standard_xavier\",\n",
    "    \"trabelsi_independent_xavier\",\n",
    "    \"trabelsi_standard_kaiming\",\n",
    "    \"trabelsi_independent_kaiming\",\n",
    "    \"trabelsi_standard_he\",\n",
    "    \"trabelsi_independent_he\",\n",
    "    \"standard\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=ed8ee0c1a6a74b09baec87664f8f32d5\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/ed8ee0c1a6a74b09baec87664f8f32d5/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.10912378996962167\n",
      "Epoch 19 loss is 0.07350913106780935\n",
      "Epoch 29 loss is 0.06740907494990528\n",
      "Epoch 39 loss is 0.06109018905281342\n",
      "Epoch 49 loss is 0.04977515585823802\n",
      "Train Acc.:  0.9579113788830492\n",
      "Val Acc.:  0.9452277193881911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1074\n",
      "           1       0.95      0.95      0.95      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.97      0.96      0.96      1048\n",
      "           4       0.95      0.95      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.94      0.93      0.93      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.93      0.94      1030\n",
      "          10       0.98      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.96     11703\n",
      "   macro avg       0.97      0.96      0.96     11703\n",
      "weighted avg       0.97      0.96      0.96     11703\n",
      " samples avg       0.95      0.96      0.95     11703\n",
      "\n",
      "ClearML Task: created new task id=f733ecf39b204986894aba85be284101\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/f733ecf39b204986894aba85be284101/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.10360111410110406\n",
      "Epoch 19 loss is 0.07482460010860024\n",
      "Epoch 29 loss is 0.05720182708014626\n",
      "Epoch 39 loss is 0.05087588215150936\n",
      "Epoch 49 loss is 0.04891710590591009\n",
      "Train Acc.:  0.9592146306029141\n",
      "Val Acc.:  0.9476202683072716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1074\n",
      "           1       0.95      0.94      0.95      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.96      0.94      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.95      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.95      0.94      1030\n",
      "          10       0.99      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=2c73aa9ce9ad462ebcdb4bf6476d15ca\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2c73aa9ce9ad462ebcdb4bf6476d15ca/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.10360111410110406\n",
      "Epoch 19 loss is 0.07482460010860024\n",
      "Epoch 29 loss is 0.05720182708014626\n",
      "Epoch 39 loss is 0.05087588215150936\n",
      "Epoch 49 loss is 0.04891710590591009\n",
      "Train Acc.:  0.9592146306029141\n",
      "Val Acc.:  0.9476202683072716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1074\n",
      "           1       0.95      0.94      0.95      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.96      0.94      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.95      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.95      0.94      1030\n",
      "          10       0.99      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=b0a08ce72a814857ba4c5d065b508444\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b0a08ce72a814857ba4c5d065b508444/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.10360111410110406\n",
      "Epoch 19 loss is 0.07482460010860024\n",
      "Epoch 29 loss is 0.05720182708014626\n",
      "Epoch 39 loss is 0.05087588215150936\n",
      "Epoch 49 loss is 0.04891710590591009\n",
      "Train Acc.:  0.9592146306029141\n",
      "Val Acc.:  0.9476202683072716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1074\n",
      "           1       0.95      0.94      0.95      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.96      0.94      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.95      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.95      0.94      1030\n",
      "          10       0.99      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=d30f92af2e854a6eabd58979194fa97a\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d30f92af2e854a6eabd58979194fa97a/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.10360111410110406\n",
      "Epoch 19 loss is 0.07482460010860024\n",
      "Epoch 29 loss is 0.05720182708014626\n",
      "Epoch 39 loss is 0.05087588215150936\n",
      "Epoch 49 loss is 0.04891710590591009\n",
      "Train Acc.:  0.9592146306029141\n",
      "Val Acc.:  0.9476202683072716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1074\n",
      "           1       0.95      0.94      0.95      1089\n",
      "           2       0.99      0.98      0.99      1044\n",
      "           3       0.99      0.97      0.98      1048\n",
      "           4       0.96      0.94      0.95      1057\n",
      "           5       0.96      0.94      0.95      1072\n",
      "           6       0.95      0.92      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.94      0.95      0.94      1030\n",
      "          10       0.99      0.95      0.97      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.95      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=58bf2a98bd6647d98f251d2d26f6cf92\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/58bf2a98bd6647d98f251d2d26f6cf92/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12869048943684255\n",
      "Epoch 19 loss is 0.10809475548156226\n",
      "Epoch 29 loss is 0.06815886897276817\n",
      "Epoch 39 loss is 0.061323340771339635\n",
      "Epoch 49 loss is 0.04661124328544\n",
      "Train Acc.:  0.9635944109729522\n",
      "Val Acc.:  0.9541997778347432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.95      0.93      0.94      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.98      0.98      0.98      1048\n",
      "           4       0.96      0.95      0.96      1057\n",
      "           5       0.94      0.93      0.94      1072\n",
      "           6       0.95      0.94      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.99      0.97      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     11703\n",
      "   macro avg       0.97      0.97      0.97     11703\n",
      "weighted avg       0.97      0.97      0.97     11703\n",
      " samples avg       0.96      0.97      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=727319e648e24f6880b8775a7a3b5405\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/727319e648e24f6880b8775a7a3b5405/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.12869048943684255\n",
      "Epoch 19 loss is 0.10809475548156226\n",
      "Epoch 29 loss is 0.06815886897276817\n",
      "Epoch 39 loss is 0.061323340771339635\n",
      "Epoch 49 loss is 0.04661124328544\n",
      "Train Acc.:  0.9635944109729522\n",
      "Val Acc.:  0.9541997778347432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.95      0.93      0.94      1089\n",
      "           2       1.00      0.98      0.99      1044\n",
      "           3       0.98      0.98      0.98      1048\n",
      "           4       0.96      0.95      0.96      1057\n",
      "           5       0.94      0.93      0.94      1072\n",
      "           6       0.95      0.94      0.94      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.99      0.97      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     11703\n",
      "   macro avg       0.97      0.97      0.97     11703\n",
      "weighted avg       0.97      0.97      0.97     11703\n",
      " samples avg       0.96      0.97      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=64ca3806b64b4a3b938eca94188128f4\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/64ca3806b64b4a3b938eca94188128f4/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09760215896283031\n",
      "Epoch 19 loss is 0.07483332226588477\n",
      "Epoch 29 loss is 0.06169817582724667\n",
      "Epoch 39 loss is 0.04806632873441776\n",
      "Epoch 49 loss is 0.050257943968239405\n",
      "Train Acc.:  0.9655599709438961\n",
      "Val Acc.:  0.9501837135777151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.94      0.91      0.93      1089\n",
      "           2       1.00      0.99      0.99      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.95      0.95      0.95      1057\n",
      "           5       0.94      0.93      0.93      1072\n",
      "           6       0.96      0.94      0.95      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.99      0.97      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n",
      "ClearML Task: created new task id=64ca5b655c32401eabad18928c7c9c4c\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/64ca5b655c32401eabad18928c7c9c4c/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17055/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.09760215896283031\n",
      "Epoch 19 loss is 0.07483332226588477\n",
      "Epoch 29 loss is 0.06169817582724667\n",
      "Epoch 39 loss is 0.04806632873441776\n",
      "Epoch 49 loss is 0.050257943968239405\n",
      "Train Acc.:  0.9655599709438961\n",
      "Val Acc.:  0.9501837135777151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1074\n",
      "           1       0.94      0.91      0.93      1089\n",
      "           2       1.00      0.99      0.99      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.95      0.95      0.95      1057\n",
      "           5       0.94      0.93      0.93      1072\n",
      "           6       0.96      0.94      0.95      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.96      0.96      1030\n",
      "          10       0.99      0.97      0.98      1012\n",
      "\n",
      "   micro avg       0.97      0.96      0.97     11703\n",
      "   macro avg       0.97      0.96      0.97     11703\n",
      "weighted avg       0.97      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for initilizer in initilizers:\n",
    "    model = Model(categories=categories, periodicity=periodicity)\n",
    "    model.initialize_weights(initilizer=initilizer)\n",
    "    criterion = ComplexMSELoss.apply\n",
    "    optimizer = ECL(model.parameters(), lr=lr)\n",
    "\n",
    "    task = Task.init(\n",
    "        project_name=\"mlmvn\",\n",
    "        task_name=\"SDD-mlmvn-[48-100-11]\",\n",
    "        tags=[\"mlmvn\", \"SDD\", \"initilizer\"],\n",
    "    )\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    #  capture a dictionary of hyperparameters with config\n",
    "    config_dict = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optim\": \"ECL\",\n",
    "        \"categories\": categories,\n",
    "        \"periodicity\": periodicity,\n",
    "        \"layer\": \"[48-100-11]\",\n",
    "        \"initilizer\": initilizer,\n",
    "    }\n",
    "    task.connect(config_dict)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Train Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    print(\"Val Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Val Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "\n",
    "    task.mark_completed()\n",
    "    task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
