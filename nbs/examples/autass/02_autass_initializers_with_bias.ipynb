{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensorless Drive Diagnosis\n",
    "\n",
    "> In this example, the main focus is the classification of individual states of a motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlmvn.layers import FirstLayer, HiddenLayer, OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss\n",
    "from mlmvn.optim import MySGD, ECL\n",
    "import mlmvn.init as cmplx_init\n",
    "from pathlib import Path\n",
    "from clearml import Task, Logger\n",
    "\n",
    "torch.manual_seed(0)  #  for repeatable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# --- helper functions ---\n",
    "def reverse_one_hot(x, neuronCats):\n",
    "    a = np.zeros(len(x))\n",
    "    x = torch.detach(x)\n",
    "    for i in range(len(x)):\n",
    "        a[i] = torch.max(x[i]) - 1 + np.argmax(x[i]) * neuronCats\n",
    "    return a\n",
    "\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    out = out.type(torch.double)\n",
    "    yb = yb.type(torch.double)\n",
    "    x = 0\n",
    "    for i in range(len(out)):\n",
    "        x += torch.equal(out[i], yb[i])\n",
    "    return x / len(out)\n",
    "\n",
    "\n",
    "def prepare_data(x_train, x_valid, y_train, y_valid, neuronCats):\n",
    "    # one-hot encoding\n",
    "    numSamples, numFeatures = x_valid.shape\n",
    "    y_valid_int = y_valid\n",
    "    y2 = y_valid + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    numClasses = max(y2)\n",
    "    target_ids = range(numClasses)\n",
    "    no = int(np.ceil(numClasses / neuronCats))  # number of output neurons\n",
    "    if no != 1:\n",
    "        y_valid = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_valid[i, k] = c\n",
    "    numSamples, numFeatures = x_train.shape\n",
    "    y_train_int = y_train\n",
    "    y2 = y_train + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    if no != 1:\n",
    "        y_train = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_train[i, k] = c\n",
    "    del y2\n",
    "\n",
    "    # Convert numpy arrays into torch tensors\n",
    "    x_train, y_train, x_valid, y_valid = map(\n",
    "        torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "    )\n",
    "    if y_train.size().__len__() == 1:\n",
    "        y_train = torch.unsqueeze(y_train, 1)\n",
    "        y_valid = torch.unsqueeze(y_valid, 1)\n",
    "\n",
    "    # convert angles to complex numbers on unit-circle\n",
    "    x_train = torch.exp(1.0j * x_train)\n",
    "    x_valid = torch.exp(1.0j * x_valid)\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data(X, y, neuronCats):\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=46806, random_state=42\n",
    "    )\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data_by_index(X, y, neuronCats, train_index, test_index):\n",
    "    x_train, x_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# --- Plots ---\n",
    "def plot_loss(title, losses, scores):\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    fig.suptitle(\"CVNN - Moons\")\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.legend([\"Acc\", \"Loss\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weights(title, ylabel_1, ylabel_2, weights_real, weights_imag):\n",
    "    # y_min = np.min([np.min(weights_real), np.min(weights_imag)])\n",
    "    # y_max = np.max([np.max(weights_real), np.max(weights_imag)])\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(14, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax[0].plot(np.linspace(1, len(weights_real), len(weights_real)), weights_real)\n",
    "    ax[0].set_xlabel(\"Step\")\n",
    "    ax[0].set_ylabel(ylabel_1)\n",
    "    # ax[0].set_title(\"Real Valued Weigts\")\n",
    "    ax[0].set_xlim(0, len(weights_real))\n",
    "    # ax[0].set_ylim(y_min, y_max)\n",
    "\n",
    "    ax[1].plot(np.linspace(1, len(weights_imag), len(weights_imag)), weights_imag)\n",
    "    ax[1].set_xlabel(\"Step\")\n",
    "    ax[1].set_ylabel(ylabel_2)\n",
    "    # ax[1].set_title(\"Imaginary Valued Weights\")\n",
    "    ax[1].set_xlim(0, len(weights_imag))\n",
    "    # ax[1].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_acc_list(title, list_losses, list_scores, image_name):\n",
    "    losses = np.mean(list_losses, axis=0)\n",
    "    scores = np.mean(list_scores, axis=0)\n",
    "\n",
    "    losses_std = np.std(list_losses, axis=0)\n",
    "    scores_std = np.std(list_scores, axis=0)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(losses), len(losses)),\n",
    "        losses + losses_std,\n",
    "        losses - losses_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(scores), len(scores)),\n",
    "        scores + scores_std,\n",
    "        scores - scores_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.legend([\"Loss Mean\", \"Loss Std\", \"Acc. Mean\", \"Acc. Std\"])\n",
    "    fig.savefig(image_name, format=\"png\", dpi=600)\n",
    "\n",
    "    plt.show()\n",
    "    # save\n",
    "    # fig.savefig(image_name + \".svg\", format=\"svg\", dpi=600)\n",
    "\n",
    "\n",
    "# --- Logging ---\n",
    "model_dict: dict = {}\n",
    "\n",
    "\n",
    "def fc_hook(layer_name, module, grad_input, grad_output):\n",
    "    if layer_name in model_dict:\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output\n",
    "    else:\n",
    "        model_dict[layer_name] = {}\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# control variables\n",
    "# number of categories a neuron can distinguish / parameter that determines the number of output neurons\n",
    "neuronCats = 1\n",
    "# number of categories per neuron, i.e. neuronCats (+ 1 for others in case of multiple Outputs)\n",
    "categories = 2\n",
    "# how often a classification sector occurs (1 means no periodicity)\n",
    "periodicity = 1\n",
    "# path to store best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\n",
    "    \"data/autass_data2.csv\",\n",
    "    header=None,\n",
    "    dtype=np.double,\n",
    ")\n",
    "data = np.array(train_csv.values[:, 1:50])\n",
    "del train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0:48]\n",
    "y = data[:, 48].astype(int) - 1\n",
    "\n",
    "yt = copy.copy(y)\n",
    "yt[yt == 0] = 20\n",
    "yt[yt == 1] = 21\n",
    "yt[yt == 2] = 22\n",
    "yt[yt == 3] = 23\n",
    "yt[yt == 4] = 26\n",
    "yt[yt == 5] = 24\n",
    "yt[yt == 6] = 27\n",
    "yt[yt == 7] = 29\n",
    "yt[yt == 8] = 30\n",
    "yt[yt == 9] = 25\n",
    "yt[yt == 10] = 28\n",
    "yt -= 20\n",
    "y = yt\n",
    "del yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 538\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "    def initialize_weights(self, initilizer=\"uniform\"):\n",
    "        if initilizer == \"uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n",
    "        elif initilizer == \"normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_normal_independent_(\n",
    "                            m.bias,\n",
    "                        )\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_normal_independent_(\n",
    "                            m.bias,\n",
    "                        )\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_normal_independent_(\n",
    "                            m.bias,\n",
    "                        )\n",
    "        elif initilizer == \"ones\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"zeros\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_normal_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_normal_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_normal_(m.bias)\n",
    "        elif initilizer == \"kaiming_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n",
    "        elif initilizer == \"xavier_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_normal_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_normal_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_normal_(m.bias)\n",
    "        elif initilizer == \"xavier_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_uniform_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_uniform_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_xavier_uniform_(m.bias)\n",
    "        elif initilizer == \"trabelsi_standard_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n",
    "        elif initilizer == \"trabelsi_independent_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n",
    "        elif initilizer == \"trabelsi_standard_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n",
    "        elif initilizer == \"trabelsi_independent_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n",
    "        elif initilizer == \"trabelsi_standard_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n",
    "        elif initilizer == \"trabelsi_independent_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n",
    "        elif initilizer == \"trabelsi_standard_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n",
    "        elif initilizer == \"trabelsi_independent_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        # nn.init.constant_(m.bias,0)\n",
    "                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initilizers = [\n",
    "    \"uniform\",\n",
    "    \"normal\",\n",
    "    # \"zeros\",\n",
    "    # \"ones\",\n",
    "    \"kaiming_normal\",\n",
    "    \"kaiming_uniform\",\n",
    "    \"xavier_normal\",\n",
    "    \"xavier_uniform\",\n",
    "    \"trabelsi_standard_glorot\",\n",
    "    \"trabelsi_independent_glorot\",\n",
    "    \"trabelsi_standard_xavier\",\n",
    "    \"trabelsi_independent_xavier\",\n",
    "    \"trabelsi_standard_kaiming\",\n",
    "    \"trabelsi_independent_kaiming\",\n",
    "    \"trabelsi_standard_he\",\n",
    "    \"trabelsi_independent_he\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=48155d4999064c8ab3c734ff756ae73a\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/48155d4999064c8ab3c734ff756ae73a/output/log\n",
      "======> WARNING! Git diff to large to store (4715kb), skipping uncommitted changes <======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14706/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 15:59:22,437 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.13520533966014758\n",
      "Epoch 19 loss is 0.09242117989507001\n",
      "Epoch 29 loss is 0.06890200001316225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m task\u001b[39m.\u001b[39mconnect(config_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m x_train, x_valid, y_train, y_valid \u001b[39m=\u001b[39m get_splitted_data(X, y, neuronCats)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m losses, scores \u001b[39m=\u001b[39m fit(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     model,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     x_train,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     y_train,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     categories\u001b[39m=\u001b[39;49mcategories,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     periodicity\u001b[39m=\u001b[39;49mperiodicity,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(PATH))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_train)\n",
      "\u001b[1;32m/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb Cell 17\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=350'>351</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=351'>352</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=352'>353</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(inputs\u001b[39m=\u001b[39;49mxb, layers\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(model\u001b[39m.\u001b[39;49mchildren()))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=354'>355</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(batch_loss) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(batch_loss))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/02_autass_initializers_with_bias.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=355'>356</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m9\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmvn/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/mlmvn/mlmvn/optim.py:85\u001b[0m, in \u001b[0;36mECL.step\u001b[0;34m(self, inputs, layers, closure)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m             momentum_buffer_list\u001b[39m.\u001b[39mappend(state[\u001b[39m\"\u001b[39m\u001b[39mmomentum_buffer\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m ecl(\n\u001b[1;32m     86\u001b[0m     params_with_grad,\n\u001b[1;32m     87\u001b[0m     d_p_list,\n\u001b[1;32m     88\u001b[0m     inputs,\n\u001b[1;32m     89\u001b[0m     layers,\n\u001b[1;32m     90\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     91\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     92\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     93\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     94\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     95\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     96\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     97\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m     98\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Documents/mlmvn/mlmvn/optim.py:144\u001b[0m, in \u001b[0;36mecl\u001b[0;34m(params, d_p_list, inputs, layers, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_ecl\n\u001b[0;32m--> 144\u001b[0m func(\n\u001b[1;32m    145\u001b[0m     params,\n\u001b[1;32m    146\u001b[0m     d_p_list,\n\u001b[1;32m    147\u001b[0m     inputs,\n\u001b[1;32m    148\u001b[0m     layers,\n\u001b[1;32m    149\u001b[0m     momentum_buffer_list,\n\u001b[1;32m    150\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    151\u001b[0m     momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    152\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    153\u001b[0m     dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[1;32m    154\u001b[0m     nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[1;32m    155\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m    156\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    157\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/mlmvn/mlmvn/optim.py:184\u001b[0m, in \u001b[0;36m_single_tensor_ecl\u001b[0;34m(params, d_p_list, inputs, layers, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mfor\u001b[39;00m i, param \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(params):\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m toggle \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m         x_pinv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mpinv(\n\u001b[1;32m    185\u001b[0m             torch\u001b[39m.\u001b[39;49mcat(\n\u001b[1;32m    186\u001b[0m                 [torch\u001b[39m.\u001b[39;49mones(\u001b[39m1\u001b[39;49m, input_layer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)), input_layer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mT[\u001b[39m0\u001b[39;49m:]]\n\u001b[1;32m    187\u001b[0m             )\n\u001b[1;32m    188\u001b[0m         )\u001b[39m.\u001b[39mT\n\u001b[1;32m    189\u001b[0m         angle_pinv \u001b[39m=\u001b[39m x_pinv[\u001b[39m1\u001b[39m:, :]\n\u001b[1;32m    190\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(params) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmvn/lib/python3.9/site-packages/clearml/task.py:3794\u001b[0m, in \u001b[0;36mTask.__register_at_exit.<locals>.ExitHooks.signal_handler\u001b[0;34m(self, sig, frame)\u001b[0m\n\u001b[1;32m   3791\u001b[0m \u001b[39m# if this is a sig term, we wait until __at_exit is called (basically do nothing)\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m \u001b[39mif\u001b[39;00m sig \u001b[39m==\u001b[39m signal\u001b[39m.\u001b[39mSIGINT:\n\u001b[1;32m   3793\u001b[0m     \u001b[39m# return original handler result\u001b[39;00m\n\u001b[0;32m-> 3794\u001b[0m     \u001b[39mreturn\u001b[39;00m org_handler \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(org_handler) \u001b[39melse\u001b[39;00m org_handler(sig, frame)\n\u001b[1;32m   3796\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_recursion_protection_flag:\n\u001b[1;32m   3797\u001b[0m     \u001b[39m# call original\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m     os\u001b[39m.\u001b[39mkill(os\u001b[39m.\u001b[39mgetpid(), sig)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for initilizer in initilizers:\n",
    "    model = Model(categories=categories, periodicity=periodicity)\n",
    "    model.initialize_weights(initilizer=initilizer)\n",
    "    criterion = ComplexMSELoss.apply\n",
    "    optimizer = ECL(model.parameters(), lr=lr)\n",
    "\n",
    "    task = Task.init(\n",
    "        project_name=\"mlmvn\",\n",
    "        task_name=\"SDD-mlmvn-[48-100-11]\",\n",
    "        tags=[\"mlmvn\", \"SDD\", \"initilizer_with_bias\"],\n",
    "    )\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    #  capture a dictionary of hyperparameters with config\n",
    "    config_dict = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optim\": \"ECL\",\n",
    "        \"categories\": categories,\n",
    "        \"periodicity\": periodicity,\n",
    "        \"layer\": \"[48-100-11]\",\n",
    "        \"initilizer\": initilizer,\n",
    "    }\n",
    "    task.connect(config_dict)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    y_pred = model.predict(x_train)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Train Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(x_valid)\n",
    "    acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "    print(\"Val Acc.: \", acc)\n",
    "    Logger.current_logger().report_single_value(\n",
    "        name=\"Val Acc.\",\n",
    "        value=acc,\n",
    "    )\n",
    "    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n",
    "\n",
    "    task.mark_completed()\n",
    "    task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
