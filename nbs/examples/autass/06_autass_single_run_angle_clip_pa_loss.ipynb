{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensorless Drive Diagnosis\n",
    "\n",
    "> In this example, the main focus is the classification of individual states of a motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlmvn.layers import FirstLayer, HiddenLayer, OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss, ComplexMSE_adjusted_error\n",
    "from mlmvn.optim import MySGD, ECL\n",
    "from pathlib import Path\n",
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# --- helper functions ---\n",
    "def reverse_one_hot(x, neuronCats):\n",
    "    a = np.zeros(len(x))\n",
    "    x = torch.detach(x)\n",
    "    for i in range(len(x)):\n",
    "        a[i] = torch.max(x[i]) - 1 + np.argmax(x[i]) * neuronCats\n",
    "    return a\n",
    "\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    out = out.type(torch.double)\n",
    "    yb = yb.type(torch.double)\n",
    "    x = 0\n",
    "    for i in range(len(out)):\n",
    "        x += torch.equal(out[i], yb[i])\n",
    "    return x / len(out)\n",
    "\n",
    "\n",
    "def prepare_data(x_train, x_valid, y_train, y_valid, neuronCats):\n",
    "    # one-hot encoding\n",
    "    numSamples, numFeatures = x_valid.shape\n",
    "    y_valid_int = y_valid\n",
    "    y2 = y_valid + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    numClasses = max(y2)\n",
    "    target_ids = range(numClasses)\n",
    "    no = int(np.ceil(numClasses / neuronCats))  # number of output neurons\n",
    "    if no != 1:\n",
    "        y_valid = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_valid[i, k] = c\n",
    "    numSamples, numFeatures = x_train.shape\n",
    "    y_train_int = y_train\n",
    "    y2 = y_train + 1  # auxiliary variable so that classes start at 1 and not 0\n",
    "    if no != 1:\n",
    "        y_train = torch.zeros(numSamples, no)\n",
    "        for i in range(numSamples):\n",
    "            k = int(np.ceil(y2[i] / neuronCats)) - 1\n",
    "            c = np.mod((y2[i] - 1), neuronCats) + 1\n",
    "            y_train[i, k] = c\n",
    "    del y2\n",
    "\n",
    "    # Convert numpy arrays into torch tensors\n",
    "    x_train, y_train, x_valid, y_valid = map(\n",
    "        torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "    )\n",
    "    if y_train.size().__len__() == 1:\n",
    "        y_train = torch.unsqueeze(y_train, 1)\n",
    "        y_valid = torch.unsqueeze(y_valid, 1)\n",
    "\n",
    "    # convert angles to complex numbers on unit-circle\n",
    "    x_train = torch.exp(1.0j * x_train)\n",
    "    x_valid = torch.exp(1.0j * x_valid)\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data(X, y, neuronCats):\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=46806, random_state=42\n",
    "    )\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def get_splitted_data_by_index(X, y, neuronCats, train_index, test_index):\n",
    "    x_train, x_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    x_train, x_valid, y_train, y_valid = prepare_data(\n",
    "        x_train, x_valid, y_train, y_valid, neuronCats\n",
    "    )\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# --- Plots ---\n",
    "def plot_loss(title, losses, scores):\n",
    "    plt.rcParams[\"axes.grid\"] = True\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    fig.suptitle(\"CVNN - Moons\")\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_xlim(0, len(losses))\n",
    "\n",
    "    ax1.legend([\"Acc\", \"Loss\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_weights(title, ylabel_1, ylabel_2, weights_real, weights_imag):\n",
    "    # y_min = np.min([np.min(weights_real), np.min(weights_imag)])\n",
    "    # y_max = np.max([np.max(weights_real), np.max(weights_imag)])\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(14, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax[0].plot(np.linspace(1, len(weights_real), len(weights_real)), weights_real)\n",
    "    ax[0].set_xlabel(\"Step\")\n",
    "    ax[0].set_ylabel(ylabel_1)\n",
    "    # ax[0].set_title(\"Real Valued Weigts\")\n",
    "    ax[0].set_xlim(0, len(weights_real))\n",
    "    # ax[0].set_ylim(y_min, y_max)\n",
    "\n",
    "    ax[1].plot(np.linspace(1, len(weights_imag), len(weights_imag)), weights_imag)\n",
    "    ax[1].set_xlabel(\"Step\")\n",
    "    ax[1].set_ylabel(ylabel_2)\n",
    "    # ax[1].set_title(\"Imaginary Valued Weights\")\n",
    "    ax[1].set_xlim(0, len(weights_imag))\n",
    "    # ax[1].set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_acc_list(title, list_losses, list_scores, image_name):\n",
    "    losses = np.mean(list_losses, axis=0)\n",
    "    scores = np.mean(list_scores, axis=0)\n",
    "\n",
    "    losses_std = np.std(list_losses, axis=0)\n",
    "    scores_std = np.std(list_scores, axis=0)\n",
    "\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    fig.suptitle(title)\n",
    "    ax1.plot(np.linspace(1, len(losses), len(losses)), losses)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(losses), len(losses)),\n",
    "        losses + losses_std,\n",
    "        losses - losses_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "\n",
    "    ax1.plot(np.linspace(1, len(scores), len(scores)), scores)\n",
    "    ax1.fill_between(\n",
    "        np.linspace(1, len(scores), len(scores)),\n",
    "        scores + scores_std,\n",
    "        scores - scores_std,\n",
    "        alpha=0.5,\n",
    "        linewidth=0,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.legend([\"Loss Mean\", \"Loss Std\", \"Acc. Mean\", \"Acc. Std\"])\n",
    "    fig.savefig(image_name, format=\"png\", dpi=600)\n",
    "\n",
    "    plt.show()\n",
    "    # save\n",
    "    # fig.savefig(image_name + \".svg\", format=\"svg\", dpi=600)\n",
    "\n",
    "\n",
    "# --- Logging ---\n",
    "model_dict: dict = {}\n",
    "\n",
    "\n",
    "def fc_hook(layer_name, module, grad_input, grad_output):\n",
    "    if layer_name in model_dict:\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output\n",
    "    else:\n",
    "        model_dict[layer_name] = {}\n",
    "        model_dict[layer_name][\"weights\"] = module.weights.detach().clone()\n",
    "        model_dict[layer_name][\"bias\"] = module.bias.detach().clone()\n",
    "        model_dict[layer_name][\"grad_input\"] = grad_input\n",
    "        model_dict[layer_name][\"grad_output\"] = grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# control variables\n",
    "# number of categories a neuron can distinguish / parameter that determines the number of output neurons\n",
    "neuronCats = 1\n",
    "# number of categories per neuron, i.e. neuronCats (+ 1 for others in case of multiple Outputs)\n",
    "categories = 2\n",
    "# how often a classification sector occurs (1 means no periodicity)\n",
    "periodicity = 1\n",
    "# path to store best model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\n",
    "    \"data/autass_data2.csv\",\n",
    "    header=None,\n",
    "    dtype=np.double,\n",
    ")\n",
    "data = np.array(train_csv.values[:, 1:50])\n",
    "del train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0:48]\n",
    "y = data[:, 48].astype(int) - 1\n",
    "\n",
    "yt = copy.copy(y)\n",
    "yt[yt == 0] = 20\n",
    "yt[yt == 1] = 21\n",
    "yt[yt == 2] = 22\n",
    "yt[yt == 3] = 23\n",
    "yt[yt == 4] = 26\n",
    "yt[yt == 5] = 24\n",
    "yt[yt == 6] = 27\n",
    "yt[yt == 7] = 29\n",
    "yt[yt == 8] = 30\n",
    "yt[yt == 9] = 25\n",
    "yt[yt == 10] = 28\n",
    "yt -= 20\n",
    "y = yt\n",
    "del yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 538\n",
    "lr = 1\n",
    "clip_angle_value = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-10-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 10)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(10, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=f6cccdb112e04accb6c3c9cbde04c8bc\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/f6cccdb112e04accb6c3c9cbde04c8bc/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-10-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-10-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-10-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32398/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:20:09,938 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.17674986948574978\n",
      "Epoch 19 loss is 0.23642975395965105\n",
      "Epoch 29 loss is 0.2685157911344387\n",
      "Epoch 39 loss is 0.2685280753926382\n",
      "Epoch 49 loss is 0.3102966055564806\n",
      "Epoch 59 loss is 0.3219121349999901\n",
      "Epoch 69 loss is 0.3843922643317746\n",
      "Epoch 79 loss is 0.23714397742070334\n",
      "Epoch 89 loss is 0.20704108010521566\n",
      "Epoch 99 loss is 0.23827630941739453\n",
      "Epoch 109 loss is 0.3212852750370261\n",
      "Epoch 119 loss is 0.3255609412204039\n",
      "Epoch 129 loss is 0.3939142005220449\n",
      "Epoch 139 loss is 0.34759228266556114\n",
      "Epoch 149 loss is 0.4047799303303979\n",
      "Epoch 159 loss is 0.45838090802974407\n",
      "Epoch 169 loss is 0.3851789373210605\n",
      "Epoch 179 loss is 0.4465964512492697\n",
      "Epoch 189 loss is 0.4651111775653979\n",
      "Epoch 199 loss is 0.3356218916278391\n",
      "Train Acc.:  0.8616843994359698\n",
      "Val Acc.:  0.8605485772878749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      1074\n",
      "           1       0.87      0.81      0.84      1089\n",
      "           2       0.97      0.93      0.95      1044\n",
      "           3       0.96      0.92      0.94      1048\n",
      "           4       0.82      0.81      0.82      1057\n",
      "           5       0.87      0.88      0.87      1072\n",
      "           6       0.84      0.75      0.79      1066\n",
      "           7       0.99      0.98      0.99      1103\n",
      "           8       1.00      0.99      0.99      1108\n",
      "           9       0.83      0.82      0.83      1030\n",
      "          10       0.94      0.85      0.90      1012\n",
      "\n",
      "   micro avg       0.91      0.88      0.90     11703\n",
      "   macro avg       0.91      0.88      0.89     11703\n",
      "weighted avg       0.91      0.88      0.90     11703\n",
      " samples avg       0.87      0.88      0.87     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-20-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 20)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(20, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=de47274577644738b8ac0464a1d7bb9b\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/de47274577644738b8ac0464a1d7bb9b/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-20-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-20-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-20-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32398/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:23:15,126 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.1781452279297529\n",
      "Epoch 19 loss is 0.17121365507481565\n",
      "Epoch 29 loss is 0.23437533994636411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m x_train, x_valid, y_train, y_valid \u001b[39m=\u001b[39m get_splitted_data(X, y, neuronCats)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m losses, scores \u001b[39m=\u001b[39m fit(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     x_train,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     y_train,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     categories\u001b[39m=\u001b[39;49mcategories,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     periodicity\u001b[39m=\u001b[39;49mperiodicity,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(PATH))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_train)\n",
      "\u001b[1;32m/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb Cell 24\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     batch_loss\u001b[39m.\u001b[39mappend((torch\u001b[39m.\u001b[39mabs(loss))\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep(inputs\u001b[39m=\u001b[39mxb, layers\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mchildren()))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B194.94.231.187/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/04_autass_single_run_angle_clip.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(batch_loss) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(batch_loss))\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmvn/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlmvn/lib/python3.9/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Documents/mlmvn/mlmvn/layers.py:226\u001b[0m, in \u001b[0;36mOutputLayerFB.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    224\u001b[0m     grad_input \u001b[39m=\u001b[39m grad_output\u001b[39m.\u001b[39mmm(cinv)\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mneeds_input_grad[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 226\u001b[0m     x_pinv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mpinv(\n\u001b[1;32m    227\u001b[0m         torch\u001b[39m.\u001b[39;49mcat([torch\u001b[39m.\u001b[39;49mones(\u001b[39m1\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)), \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mT[\u001b[39m0\u001b[39;49m:]])\n\u001b[1;32m    228\u001b[0m     )\u001b[39m.\u001b[39mT\n\u001b[1;32m    229\u001b[0m     angle_pinv \u001b[39m=\u001b[39m x_pinv[\u001b[39m1\u001b[39m:, :]\n\u001b[1;32m    230\u001b[0m     grad_weight \u001b[39m=\u001b[39m angle_pinv \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mdiv(grad_output, torch\u001b[39m.\u001b[39mabs(output))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-50-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 50)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(50, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=b9c663cb49f645d3b526ff62b971b678\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b9c663cb49f645d3b526ff62b971b678/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-50-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-50-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-50-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:36:28,294 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.12631168614962632\n",
      "Epoch 19 loss is 0.08741677563113538\n",
      "Epoch 29 loss is 0.07765135114449577\n",
      "Epoch 39 loss is 0.07199543979859238\n",
      "Epoch 49 loss is 0.05459176181501563\n",
      "Epoch 59 loss is 0.054154946830923104\n",
      "Epoch 69 loss is 0.0498279498984762\n",
      "Epoch 79 loss is 0.04969739252919254\n",
      "Epoch 89 loss is 0.05646039586774165\n",
      "Epoch 99 loss is 0.05206559631806764\n",
      "Epoch 109 loss is 0.04809522582296451\n",
      "Epoch 119 loss is 0.0522341919664429\n",
      "Epoch 129 loss is 0.0495262401204281\n",
      "Epoch 139 loss is 0.04954010092306082\n",
      "Epoch 149 loss is 0.061976915229967955\n",
      "Epoch 159 loss is 0.05182441637727222\n",
      "Epoch 169 loss is 0.05091017378293241\n",
      "Epoch 179 loss is 0.0561740020607465\n",
      "Epoch 189 loss is 0.06101975489402404\n",
      "Epoch 199 loss is 0.06845130197965535\n",
      "Train Acc.:  0.9619706875186942\n",
      "Val Acc.:  0.9557378449970093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      1074\n",
      "           1       0.95      0.92      0.93      1089\n",
      "           2       0.99      0.98      0.98      1044\n",
      "           3       0.99      0.98      0.98      1048\n",
      "           4       0.97      0.96      0.97      1057\n",
      "           5       0.95      0.94      0.95      1072\n",
      "           6       0.95      0.94      0.94      1066\n",
      "           7       1.00      0.99      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.96      0.95      0.95      1030\n",
      "          10       0.99      0.97      0.98      1012\n",
      "\n",
      "   micro avg       0.98      0.96      0.97     11703\n",
      "   macro avg       0.98      0.96      0.97     11703\n",
      "weighted avg       0.98      0.96      0.97     11703\n",
      " samples avg       0.96      0.96      0.96     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act2(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=77bda608f5dd4cbf9fb5bf146c9bcdbe\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/77bda608f5dd4cbf9fb5bf146c9bcdbe/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-100-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-100-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-100-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:40:51,411 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.09760215896283031\n",
      "Epoch 19 loss is 0.07483332226588477\n",
      "Epoch 29 loss is 0.06169817582724667\n",
      "Epoch 39 loss is 0.04806632873441776\n",
      "Epoch 49 loss is 0.050257943968239405\n",
      "Epoch 59 loss is 0.045243461048822334\n",
      "Epoch 69 loss is 0.03916302966121763\n",
      "Epoch 79 loss is 0.03600947264373011\n",
      "Epoch 89 loss is 0.0327954003325785\n",
      "Epoch 99 loss is 0.034495561806988345\n",
      "Epoch 109 loss is 0.02976081583721455\n",
      "Epoch 119 loss is 0.03207888606505858\n",
      "Epoch 129 loss is 0.029421696533450108\n",
      "Epoch 139 loss is 0.02886477329200674\n",
      "Epoch 149 loss is 0.028011108627125445\n",
      "Epoch 159 loss is 0.02746762018733399\n",
      "Epoch 169 loss is 0.029159449466333746\n",
      "Epoch 179 loss is 0.028677871059498893\n",
      "Epoch 189 loss is 0.026269004340911915\n",
      "Epoch 199 loss is 0.02659348237350445\n",
      "Train Acc.:  0.9820749476562833\n",
      "Val Acc.:  0.9690677604033154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1074\n",
      "           1       0.96      0.96      0.96      1089\n",
      "           2       1.00      0.99      0.99      1044\n",
      "           3       1.00      0.99      0.99      1048\n",
      "           4       0.97      0.97      0.97      1057\n",
      "           5       0.96      0.96      0.96      1072\n",
      "           6       0.97      0.95      0.96      1066\n",
      "           7       1.00      1.00      1.00      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.97      0.97      0.97      1030\n",
      "          10       0.99      0.98      0.98      1012\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     11703\n",
      "   macro avg       0.98      0.98      0.98     11703\n",
      "weighted avg       0.98      0.98      0.98     11703\n",
      " samples avg       0.97      0.98      0.97     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-10-10-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 10)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(10, 10)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(10, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=7aa1a5aeeab040fda27730a9cb90d8ad\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/7aa1a5aeeab040fda27730a9cb90d8ad/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-10-10-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-10-10-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.6741966148101034\n",
      "Epoch 19 loss is 0.6345819678996693\n",
      "Epoch 29 loss is 0.6163333298521995\n",
      "Epoch 39 loss is 0.5973288617286425\n",
      "Epoch 49 loss is 0.5657958380010314\n",
      "Epoch 59 loss is 0.5930215843207252\n",
      "Epoch 69 loss is 0.614107440452483\n",
      "Epoch 79 loss is 0.6680024372385054\n",
      "Epoch 89 loss is 0.670419685702236\n",
      "Epoch 99 loss is 0.6396620484680041\n",
      "Epoch 109 loss is 0.7318713292921706\n",
      "Epoch 119 loss is 0.7326499528623525\n",
      "Epoch 129 loss is 0.7676124397701742\n",
      "Epoch 139 loss is 0.7363896056980302\n",
      "Epoch 149 loss is 0.7518392231032288\n",
      "Epoch 159 loss is 0.6876373192278051\n",
      "Epoch 169 loss is 0.684599693905956\n",
      "Epoch 179 loss is 0.6865932359686486\n",
      "Epoch 189 loss is 0.6739611875627026\n",
      "Epoch 199 loss is 0.6541066273988838\n",
      "Train Acc.:  0.3464940392257403\n",
      "Val Acc.:  0.3492266940100829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.70      0.63      1074\n",
      "           1       0.59      0.43      0.50      1089\n",
      "           2       0.59      0.23      0.33      1044\n",
      "           3       0.76      0.43      0.55      1048\n",
      "           4       0.57      0.27      0.36      1057\n",
      "           5       0.59      0.61      0.60      1072\n",
      "           6       0.40      0.08      0.13      1066\n",
      "           7       0.92      0.80      0.86      1103\n",
      "           8       0.97      0.96      0.97      1108\n",
      "           9       0.17      0.03      0.05      1030\n",
      "          10       0.49      0.28      0.36      1012\n",
      "\n",
      "   micro avg       0.67      0.44      0.53     11703\n",
      "   macro avg       0.60      0.44      0.48     11703\n",
      "weighted avg       0.60      0.44      0.49     11703\n",
      " samples avg       0.39      0.44      0.41     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-20-20-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 20)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(20, 20)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(20, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=63f6d3900d26435baf5c0451ee2550cf\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/63f6d3900d26435baf5c0451ee2550cf/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-20-20-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-20-20-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.4918386432669506\n",
      "Epoch 19 loss is 0.5658443724932558\n",
      "Epoch 29 loss is 0.547344653493303\n",
      "Epoch 39 loss is 0.5003598398694803\n",
      "Epoch 49 loss is 0.45764246002628084\n",
      "Epoch 59 loss is 0.48134115218311874\n",
      "Epoch 69 loss is 0.46767609185504994\n",
      "Epoch 79 loss is 0.47236872848369066\n",
      "Epoch 89 loss is 0.45542004995148055\n",
      "Epoch 99 loss is 0.4707600822371793\n",
      "Epoch 109 loss is 0.4592737186466213\n",
      "Epoch 119 loss is 0.44952235101712684\n",
      "Epoch 129 loss is 0.4279650487928361\n",
      "Epoch 139 loss is 0.4390951851709622\n",
      "Epoch 149 loss is 0.4078203854663746\n",
      "Epoch 159 loss is 0.4626180199815107\n",
      "Epoch 169 loss is 0.4595541584940826\n",
      "Epoch 179 loss is 0.41118996909517436\n",
      "Epoch 189 loss is 0.4143808650128177\n",
      "Epoch 199 loss is 0.42120076453410465\n",
      "Train Acc.:  0.6030850745630902\n",
      "Val Acc.:  0.6048876356489788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1074\n",
      "           1       0.71      0.70      0.71      1089\n",
      "           2       0.79      0.67      0.73      1044\n",
      "           3       0.85      0.70      0.77      1048\n",
      "           4       0.78      0.54      0.64      1057\n",
      "           5       0.72      0.72      0.72      1072\n",
      "           6       0.71      0.45      0.55      1066\n",
      "           7       0.98      0.91      0.95      1103\n",
      "           8       0.98      0.96      0.97      1108\n",
      "           9       0.85      0.31      0.46      1030\n",
      "          10       0.70      0.63      0.66      1012\n",
      "\n",
      "   micro avg       0.82      0.67      0.74     11703\n",
      "   macro avg       0.82      0.67      0.72     11703\n",
      "weighted avg       0.82      0.67      0.73     11703\n",
      " samples avg       0.64      0.67      0.65     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-50-50-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 50)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(50, 50)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(50, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=3ec0ba0e69fe4e8781f1f56f68bb4df0\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/3ec0ba0e69fe4e8781f1f56f68bb4df0/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-50-50-11]'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": 1,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-50-50-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss is 0.17595385331554345\n",
      "Epoch 19 loss is 0.24443298597993288\n",
      "Epoch 29 loss is 0.34861398016646006\n",
      "Epoch 39 loss is 0.35088833894092397\n",
      "Epoch 49 loss is 0.3184690939966281\n",
      "Epoch 59 loss is 0.29159015152507245\n",
      "Epoch 69 loss is 0.29664844727356443\n",
      "Epoch 79 loss is 0.3148698996217653\n",
      "Epoch 89 loss is 0.34718880768462584\n",
      "Epoch 99 loss is 0.32786482129724753\n",
      "Epoch 109 loss is 0.3249049482370644\n",
      "Epoch 119 loss is 0.31320303291619195\n",
      "Epoch 129 loss is 0.30243110019968583\n",
      "Epoch 139 loss is 0.30238450965742614\n",
      "Epoch 149 loss is 0.29449084419928934\n",
      "Epoch 159 loss is 0.2935125197293862\n",
      "Epoch 169 loss is 0.2872804696457938\n",
      "Epoch 179 loss is 0.29204265563094745\n",
      "Epoch 189 loss is 0.3058525140885775\n",
      "Epoch 199 loss is 0.30025929403593016\n",
      "Train Acc.:  0.827223005597573\n",
      "Val Acc.:  0.820986071947364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94      1074\n",
      "           1       0.87      0.82      0.84      1089\n",
      "           2       0.94      0.90      0.92      1044\n",
      "           3       0.92      0.88      0.90      1048\n",
      "           4       0.87      0.78      0.82      1057\n",
      "           5       0.86      0.87      0.87      1072\n",
      "           6       0.81      0.72      0.77      1066\n",
      "           7       0.99      0.96      0.98      1103\n",
      "           8       1.00      0.99      0.99      1108\n",
      "           9       0.85      0.80      0.82      1030\n",
      "          10       0.92      0.87      0.89      1012\n",
      "\n",
      "   micro avg       0.91      0.87      0.89     11703\n",
      "   macro avg       0.91      0.87      0.89     11703\n",
      "weighted avg       0.91      0.87      0.89     11703\n",
      " samples avg       0.84      0.87      0.85     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMVN [48-100-100-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(48, 100)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        self.hidden_layer = HiddenLayer(100, 100)\n",
    "        self.phase_act2 = cmplx_phase_activation()\n",
    "        self.linear_out = OutputLayer(100, 11)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "        # Hooks\n",
    "        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n",
    "            self.first_layer_backward_hook\n",
    "        )\n",
    "        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n",
    "            self.hidden_layer_backward_hook\n",
    "        )\n",
    "        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n",
    "            self.output_layer_backward_hook\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.phase_act2(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def angle2class(self, x: torch.tensor) -> torch.tensor:\n",
    "        tmp = x.angle() + 2 * np.pi\n",
    "        angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "        # This will be the discrete output (the number of sector)\n",
    "        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n",
    "        return torch.remainder(o, self.categories)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return self.angle2class(output)\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Epoch {i} loss is {losses[-1]}\")\n",
    "        y_pred = model.predict(X)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSE_adjusted_error.apply\n",
    "optimizer = ECL(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=8a5c61e854984460908997574d046477\n",
      "ClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/8a5c61e854984460908997574d046477/output/log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1,\n",
       " 'epochs': 200,\n",
       " 'batch_size': 538,\n",
       " 'optim': 'ECL',\n",
       " 'categories': 2,\n",
       " 'periodicity': 1,\n",
       " 'layer': '[48-100-100-11]',\n",
       " 'loss': 'ComplexMSE_adjusted_error'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name=\"mlmvn\",\n",
    "    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n",
    "    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\", \"clip_angle_value\"],\n",
    ")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "    \"layer\": \"[48-100-100-11]\",\n",
    "    \"loss\": \"ComplexMSE_adjusted_error\",\n",
    "    \"clip_angle_value\": clip_angle_value,\n",
    "}\n",
    "task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32141/161459083.py:46: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 16:02:25,641 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\n",
      "Epoch 9 loss is 0.21820951591243778\n",
      "Epoch 19 loss is 0.13771403913523564\n",
      "Epoch 29 loss is 0.120086614450749\n",
      "Epoch 39 loss is 0.12088089901905073\n",
      "Epoch 49 loss is 0.11705438347144771\n",
      "Epoch 59 loss is 0.10653988779460959\n",
      "Epoch 69 loss is 0.09691959760010557\n",
      "Epoch 79 loss is 0.101146825508346\n",
      "Epoch 89 loss is 0.09530371019935284\n",
      "Epoch 99 loss is 0.09040586311643674\n",
      "Epoch 109 loss is 0.09619028988814168\n",
      "Epoch 119 loss is 0.09716312047009845\n",
      "Epoch 129 loss is 0.09171073510214407\n",
      "Epoch 139 loss is 0.0974171694397457\n",
      "Epoch 149 loss is 0.08935183357045892\n",
      "Epoch 159 loss is 0.09083963674472327\n",
      "Epoch 169 loss is 0.10475286591439345\n",
      "Epoch 179 loss is 0.10757160543310926\n",
      "Epoch 189 loss is 0.10028915323892916\n",
      "Epoch 199 loss is 0.09654894830466294\n",
      "Train Acc.:  0.9274451993334188\n",
      "Val Acc.:  0.9025036315474665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      1074\n",
      "           1       0.89      0.85      0.87      1089\n",
      "           2       0.97      0.96      0.97      1044\n",
      "           3       0.99      0.96      0.97      1048\n",
      "           4       0.93      0.92      0.92      1057\n",
      "           5       0.91      0.90      0.90      1072\n",
      "           6       0.91      0.89      0.90      1066\n",
      "           7       1.00      0.99      0.99      1103\n",
      "           8       1.00      1.00      1.00      1108\n",
      "           9       0.93      0.91      0.92      1030\n",
      "          10       0.95      0.92      0.94      1012\n",
      "\n",
      "   micro avg       0.95      0.93      0.94     11703\n",
      "   macro avg       0.95      0.93      0.94     11703\n",
      "weighted avg       0.95      0.93      0.94     11703\n",
      " samples avg       0.92      0.93      0.92     11703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n",
    "\n",
    "losses, scores = fit(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    categories=categories,\n",
    "    periodicity=periodicity,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "acc = accuracy(y_pred.squeeze(), y_train)\n",
    "print(\"Train Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Train Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_valid)\n",
    "acc = accuracy(y_pred.squeeze(), y_valid)\n",
    "print(\"Val Acc.: \", acc)\n",
    "Logger.current_logger().report_single_value(\n",
    "    name=\"Val Acc.\",\n",
    "    value=acc,\n",
    ")\n",
    "print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.mark_completed()\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
