{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Binary Classifier\n",
    "\n",
    "> In this example, the moons dataset is used to build a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'res'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmlmvn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minit\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcmplx_init\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmlmvn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprocess\u001b[39;00m \u001b[39mimport\u001b[39;00m angle2class\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mres\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplot_lib\u001b[39;00m \u001b[39mimport\u001b[39;00m set_default, plot_data, plot_loss, plot_confusion_matrix\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mres\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mres\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m get_moons\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'res'"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "# Setup\n",
    "import numpy as np\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)  #  for repeatable results\n",
    "\n",
    "# MLMVN\n",
    "from mlmvn.layers import FirstLayer, HiddenLayer, OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss\n",
    "from mlmvn.optim import MySGD, ECL\n",
    "import mlmvn.init as cmplx_init\n",
    "\n",
    "from mlmvn.process import angle2class\n",
    "from res.plot_lib import set_default, plot_data, plot_loss, plot_confusion_matrix\n",
    "from res.metrics import accuracy\n",
    "from res.datasets import get_moons\n",
    "\n",
    "# Tracking\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In this example the moons dataset is used to build a binary classifcator. The aim is to find the boundary beetween two cresecent moons. The entire data set consists of 1500 data points and is shown in the image below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "#%matplotlib widget\n",
    "X, y = get_moons(noise=0.2)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# X = transform(X, alpha=np.pi / 8)\n",
    "\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = str(Path.cwd() / \"models/moons-mlmvn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_hook(module, grad_input, grad_output):\n",
    "    print(\"module:\", module)\n",
    "    print(\"grad_input:\", grad_input)\n",
    "    print(\"grad_output:\", grad_output)\n",
    "\n",
    "\n",
    "model_dict: dict = {}\n",
    "\n",
    "\n",
    "def fc_hook(layer_name, module, grad_input, grad_output):\n",
    "    if layer_name in model_dict:\n",
    "        model_dict[layer_name][\"weights\"].append(module.weights.detach().clone())\n",
    "        model_dict[layer_name][\"bias\"].append(module.bias.detach().clone())\n",
    "        model_dict[layer_name][\"grad_input\"].append(grad_input)\n",
    "        model_dict[layer_name][\"grad_output\"].append(grad_output)\n",
    "    else:\n",
    "        model_dict[layer_name] = {}\n",
    "        model_dict[layer_name][\"weights\"] = []\n",
    "        model_dict[layer_name][\"weights\"].append(module.weights.detach().clone())\n",
    "        model_dict[layer_name][\"bias\"] = []\n",
    "        model_dict[layer_name][\"bias\"].append(module.bias.detach().clone())\n",
    "        model_dict[layer_name][\"grad_input\"] = []\n",
    "        model_dict[layer_name][\"grad_input\"].append(grad_input)\n",
    "        model_dict[layer_name][\"grad_output\"] = []\n",
    "        model_dict[layer_name][\"grad_output\"].append(grad_output)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, categories, periodicity):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.periodicity = periodicity\n",
    "        self.first_linear = FirstLayer(2, 5)\n",
    "        self.phase_act1 = cmplx_phase_activation()\n",
    "        # self.hidden_linear = HiddenLayer(4, 4)\n",
    "        # self.phase_act2 = cmplx_phase_activation()\n",
    "        self.output_linear = OutputLayer(5, 1)\n",
    "        self.phase_act3 = cmplx_phase_activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = self.phase_act1(x)\n",
    "        # x = self.hidden_linear(x)\n",
    "        # x = self.phase_act2(x)\n",
    "        x = self.output_linear(x)\n",
    "        x = self.phase_act3(x)\n",
    "        return x\n",
    "\n",
    "    def first_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"first_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def output_layer_backward_hook(self, module, grad_input, grad_output):\n",
    "        fc_hook(\"output_layer\", module, grad_input, grad_output)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs the prediction task of the network\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          Most likely class i.e., Label with the highest score\n",
    "        \"\"\"\n",
    "        # Pass the data through the networks\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # # Choose the label with the highest score\n",
    "        # return torch.argmax(output, 1)\n",
    "        return output\n",
    "\n",
    "    def initialize_weights(self, initilizer=\"uniform\"):\n",
    "        if initilizer == \"uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_normal_independent_(\n",
    "                        m.weights,\n",
    "                    )\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"ones\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.ones_(m.weights, imag_zero=True)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"zeros\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.zeros_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"kaiming_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"xavier_normal\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_normal_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"xavier_uniform\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_xavier_uniform_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_glorot\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_xavier\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_kaiming\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_standard_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"trabelsi_independent_he\":\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, FirstLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, HiddenLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, OutputLayer):\n",
    "                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        elif initilizer == \"standard\":\n",
    "            pass\n",
    "\n",
    "\n",
    "def fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n",
    "    # List of losses for visualization\n",
    "    losses = []\n",
    "    scores = []\n",
    "    acc_best = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Pass the data through the network and compute the loss\n",
    "        # We'll use the whole dataset during the training instead of using batches\n",
    "        # in to order to keep the code simple for now.\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for j in range((X.shape[0] - 1) // batch_size + 1):\n",
    "            start_j = j * batch_size\n",
    "            end_j = start_j + batch_size\n",
    "            xb = X[start_j:end_j]\n",
    "            yb = y[start_j:end_j]\n",
    "\n",
    "            y_pred = model(xb)\n",
    "            loss = criterion(y_pred, yb, categories, periodicity)\n",
    "            batch_loss.append((torch.abs(loss)).detach().numpy())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step(inputs=xb, layers=list(model.children()))\n",
    "\n",
    "        losses.append(sum(batch_loss) / len(batch_loss))\n",
    "        y_pred = model(X)\n",
    "        y_pred = angle2class(y_pred, categories, periodicity)\n",
    "        scores.append(accuracy(y_pred.squeeze(), y))\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Loss\", losses[-1], i)\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy\", scores[-1], i)\n",
    "\n",
    "        for key in model_dict:\n",
    "            for key_layer in model_dict[key]:\n",
    "                if key_layer in [\"weights\", \"bias\"]:\n",
    "                    log_label = str(key) + \"_\" + str(key_layer)\n",
    "                    log_label.replace(\" \", \"\")\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_real\", model_dict[key][key_layer].real, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n",
    "                    )\n",
    "\n",
    "        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n",
    "        if scores[-1] > acc_best:\n",
    "            acc_best = scores[-1]\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    writer.close()\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "# Convert numpy arrays into torch tensors\n",
    "X_train_t, y_train_t, X_test_t, y_test_t = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test)\n",
    ")\n",
    "\n",
    "X_train_t = X_train_t.type(torch.cdouble)\n",
    "X_test_t = X_test_t.type(torch.cdouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initilizers = [\n",
    "    \"uniform\",\n",
    "    \"normal\",\n",
    "    # \"zeros\",\n",
    "    # \"ones\",\n",
    "    \"kaiming_normal\",\n",
    "    \"kaiming_uniform\",\n",
    "    \"xavier_normal\",\n",
    "    \"xavier_uniform\",\n",
    "    \"trabelsi_standard_xavier\",\n",
    "    \"trabelsi_independent_xavier\",\n",
    "    \"trabelsi_standard_kaiming\",\n",
    "    \"trabelsi_independent_kaiming\",\n",
    "    \"standard\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "categories = 2\n",
    "periodicity = 1\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for initilizer in initilizers:\n",
    "    model = Model(categories=categories, periodicity=periodicity)\n",
    "    model.initialize_weights(initilizer=initilizer)\n",
    "    criterion = ComplexMSELoss.apply\n",
    "    optimizer = ECL(model.parameters(), lr=lr)\n",
    "\n",
    "    # task = Task.init(\n",
    "    #     project_name=\"mlmvn\",\n",
    "    #     task_name=\"moons-mlmvn-[2-5-1]\",\n",
    "    #     tags=[\"mlmvn\", \"moons\", \"initilizer\"],\n",
    "    # )\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    #  capture a dictionary of hyperparameters with config\n",
    "    config_dict = {\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optim\": \"ECL\",\n",
    "        \"categories\": categories,\n",
    "        \"periodicity\": periodicity,\n",
    "        \"layer\": \"[2-5-1]\",\n",
    "        \"initilizer\": initilizer,\n",
    "    }\n",
    "    # task.connect(config_dict)\n",
    "\n",
    "    losses, scores = fit(\n",
    "        model,\n",
    "        X_train_t,\n",
    "        y_train_t,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        categories=categories,\n",
    "        periodicity=periodicity,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    y_pred = model(X_train_t)\n",
    "    y_pred = angle2class(y_pred, categories, periodicity)\n",
    "    acc = accuracy(y_pred.squeeze(), y_train_t)\n",
    "    print(\"Train Acc.: \", acc)\n",
    "\n",
    "    # Logger.current_logger().report_single_value(\n",
    "    #     name=\"Train Acc.\",\n",
    "    #     value=acc,\n",
    "    # )\n",
    "\n",
    "    y_pred = model(X_test_t)\n",
    "    y_pred = angle2class(y_pred, categories, periodicity)\n",
    "    acc = accuracy(y_pred.squeeze(), y_test_t)\n",
    "    print(\"Val Acc.: \", acc)\n",
    "\n",
    "    # Logger.current_logger().report_single_value(\n",
    "    #     name=\"Val Acc.\",\n",
    "    #     value=acc,\n",
    "    # )\n",
    "\n",
    "    # print(classification_report(y_test, y_pred.detach().numpy(), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "categories = 2\n",
    "periodicity = 1\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "lr = 1\n",
    "\n",
    "model = Model(categories=categories, periodicity=periodicity)\n",
    "criterion = ComplexMSELoss.apply\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "optimizer = ECL(model.parameters(), lr=lr)\n",
    "\n",
    "# start a new experiment\n",
    "# wandb.init(project=\"moons-model-mlmvn\", entity=\"antonpf\")\n",
    "# task = Task.init(project_name=\"mlmvn\", task_name=\"moons-test\")\n",
    "\n",
    "#  capture a dictionary of hyperparameters with config\n",
    "config_dict = {\n",
    "    \"learning_rate\": lr,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"optim\": \"ECL\",\n",
    "    \"categories\": categories,\n",
    "    \"periodicity\": periodicity,\n",
    "}\n",
    "# task.connect(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "plot_loss(\"CVNN - Moons\", losses, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
