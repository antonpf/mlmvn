{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR\n",
    "\n",
    "> An example of how to solve the XOR problem with MLMVN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR problem is an example of how a single real-valued neuron cannot learn a simple but non-linear relationship. At least, this holds if we do not extend the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ed4d7e050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from mlmvn.layers import OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss\n",
    "\n",
    "torch.manual_seed(0)  #  for repeatable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=20,\n",
    "    classes=2,\n",
    "    kernels=[2],\n",
    "    batch_size=4,\n",
    "    learning_rate=1,\n",
    "    dataset=\"XOR\",\n",
    "    architecture=\"MLMVN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains four input-output mappings with binary classes. The two-dimensional input $x$ is mapped to a class label $y$. The following table shows the truth table with associated labels for the XOR gate.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\begin{array}{cc|c|cc}\n",
    "        x_1 & x_2 & y & z & arg(z) \\\\\n",
    "        \\hline\n",
    "\t\t1 &  1\t& 0\t&  1+j &  45째 \\\\\n",
    "\t\t1 & -1\t& 1\t&  1-j & 315째 \\\\\n",
    "\t\t-1 &  1\t& 1\t& -1+j & 135째 \\\\\n",
    "\t\t-1 & -1\t& 0\t& -1-j & 225째 \\\\\n",
    "    \\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "If we consider $x_1$ as $Re(z)$ and $x_2$ as $Im(z)$, the problem can also be expressed graphically into the complex domain. \n",
    "\n",
    "<center>\n",
    "    <img src=\"fig/xor_complex_domain.png\" width=450 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "x = torch.Tensor([[1., 1.],\n",
    "               [1., -1.],\n",
    "               [-1., 1.],\n",
    "               [-1., -1.]])\n",
    "\n",
    "x = x.type(torch.cdouble)\n",
    "\n",
    "y = torch.Tensor([0., 1., 1., 0.]).reshape(x.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = OutputLayer(2, 20)\n",
    "        self.phase_act = cmplx_phase_activation()\n",
    "        self.linear1 = OutputLayer(20, 1)\n",
    "        self.phase_act = cmplx_phase_activation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.phase_act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel()\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "categories =  2\n",
    "periodicity = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tensor(4.5521e-06, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "19 tensor(3.9741e-12, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "29 tensor(1.1070e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "39 tensor(1.3549e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "49 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "59 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "69 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "79 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "89 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "99 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "109 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "119 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "129 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "139 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "149 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "159 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "169 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "179 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "189 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "199 tensor(1.3554e-16, dtype=torch.float64, grad_fn=<AbsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for t in range(200):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion(y_pred.view(-1), y, categories, periodicity)\n",
    "    # wandb.log({\"loss\": torch.abs(loss)})\n",
    "    \n",
    "    if t % 10 == 9: print(t, torch.abs(loss))\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # for idx, param in enumerate(model.parameters()):\n",
    "    #     wandb.log({\"weights_layer\"+str(idx)+\"_real\": param.real})\n",
    "    #     wandb.log({\"weights_layer\"+str(idx)+\"_imag\": param.imag})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, param in enumerate(model.parameters()):\n",
    "    param.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]], dtype=torch.float64, grad_fn=<RemainderBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def angle2class(x: torch.tensor, categories, periodicity) -> torch.tensor:\n",
    "    tmp = x.angle() + 2 * np.pi\n",
    "    angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "    # This will be the discrete output (the number of sector)\n",
    "    o = torch.floor(categories * periodicity * angle / (2 * np.pi))\n",
    "    return torch.remainder(o, categories)\n",
    "\n",
    "angle2class(predictions, 2, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlmvn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "540ed89e9a44056470e4ce5685025095e7b0527d797adade2b883c2a1d428cea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
