{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR\n",
    "\n",
    "> An example of how to solve the XOR problem with MLMVN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR problem is an example of how a single real-valued neuron cannot learn a simple but non-linear relationship. At least, this holds if we do not extend the dimensionality of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from mlmvn.layers import FirstLayer, OutputLayer, cmplx_phase_activation\n",
    "from mlmvn.loss import ComplexMSELoss\n",
    "from mlmvn.optim import ECL\n",
    "\n",
    "from res.utils import find_project_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "PROJECT_ROOT = find_project_root()\n",
    "SEED: int = 42\n",
    "MODEL_MLP_LEARNING_RATE: float = 0.1\n",
    "MODEL_MLMVN_LEARNING_RATE: float = 1\n",
    "MODEL_MLMVN_CATEGORIES: int = 2\n",
    "MODEL_MLMVN_PERIODICITY: int = 2\n",
    "MODEL_EPOCHS: int = 10\n",
    "MODEL_BATCH_SIZE: int = 120\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains four input-output mappings with binary classes. The two-dimensional input $x$ is mapped to a class label $y$. The following table shows the truth table with associated labels for the XOR gate.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\begin{array}{cc|c|cc}\n",
    "        x_1 & x_2 & y & z & arg(z) \\\\\n",
    "        \\hline\n",
    "\t\t1 &  1\t& 0\t&  1+j &  45째 \\\\\n",
    "\t\t1 & -1\t& 1\t&  1-j & 315째 \\\\\n",
    "\t\t-1 &  1\t& 1\t& -1+j & 135째 \\\\\n",
    "\t\t-1 & -1\t& 0\t& -1-j & 225째 \\\\\n",
    "    \\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "If we consider $x_1$ as $Re(z)$ and $x_2$ as $Im(z)$, the problem can also be expressed graphically into the complex domain. \n",
    "\n",
    "<center>\n",
    "    <img src=\"fig/xor_complex_domain.png\" width=320 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR problem inputs and outputs\n",
    "\n",
    "# complex case\n",
    "x = torch.Tensor([[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]])\n",
    "x = x.type(torch.cdouble)\n",
    "y = torch.Tensor([0.0, 1.0, 1.0, 0.0]).reshape(x.shape[0], 1)\n",
    "\n",
    "# real case\n",
    "inputs = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "outputs = torch.tensor([[0.0], [1.0], [1.0], [0.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.3599, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "1 tensor(0.3484, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "2 tensor(0.0889, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "3 tensor(0.0108, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "4 tensor(0.0019, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "5 tensor(0.0003, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "6 tensor(3.8336e-05, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "7 tensor(5.5209e-06, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "8 tensor(8.0576e-07, dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "9 tensor(1.1703e-07, dtype=torch.float64, grad_fn=<AbsBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7070+0.7072j],\n",
       "        [-0.7071+0.7071j],\n",
       "        [ 0.7071-0.7071j],\n",
       "        [-0.7070-0.7072j]], dtype=torch.complex128,\n",
       "       grad_fn=<phase_activationBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MLMVN model\n",
    "class MLMVN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = FirstLayer(2, 2)  # input layer\n",
    "        self.phase_act = cmplx_phase_activation()  # complex activation function\n",
    "        self.linear1 = OutputLayer(2, 1)  # output layer\n",
    "        self.phase_act1 = cmplx_phase_activation()  # complex activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.phase_act(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.phase_act1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the MLMVN\n",
    "model = MLMVN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = ComplexMSELoss.apply\n",
    "optimizer = ECL(model.parameters(), lr=MODEL_MLMVN_LEARNING_RATE)\n",
    "\n",
    "# Train the MLMVN\n",
    "for t in range(MODEL_EPOCHS):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = criterion(y_pred, y, MODEL_MLMVN_CATEGORIES, MODEL_MLMVN_PERIODICITY)\n",
    "    print(t, torch.abs(loss))\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step(inputs=x, layers=list(model.children()))\n",
    "\n",
    "# Test the MLMVN\n",
    "predictions = model(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is complex and can be converted into a real output using the function `angle2class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]], dtype=torch.float64, grad_fn=<RemainderBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def angle2class(x: torch.tensor, categories, periodicity) -> torch.tensor:\n",
    "    tmp = x.angle() + 2 * np.pi\n",
    "    angle = torch.remainder(tmp, 2 * np.pi)\n",
    "\n",
    "    # This will be the discrete output (the number of sector)\n",
    "    o = torch.floor(categories * periodicity * angle / (2 * np.pi))\n",
    "    return torch.remainder(o, categories)\n",
    "\n",
    "\n",
    "angle2class(predictions, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7061562538146973\n",
      "Epoch 1000, Loss: 0.6886123418807983\n",
      "Epoch 2000, Loss: 0.6580316424369812\n",
      "Epoch 3000, Loss: 0.41256237030029297\n",
      "Epoch 4000, Loss: 0.11984607577323914\n",
      "Epoch 5000, Loss: 0.058993514627218246\n",
      "Epoch 6000, Loss: 0.03796124458312988\n",
      "Epoch 7000, Loss: 0.027697304263710976\n",
      "Epoch 8000, Loss: 0.021699126809835434\n",
      "Epoch 9000, Loss: 0.017790351063013077\n",
      "Input: tensor([0., 0.]), Prediction: tensor([0.0146], grad_fn=<UnbindBackward0>)\n",
      "Input: tensor([0., 1.]), Prediction: tensor([0.9864], grad_fn=<UnbindBackward0>)\n",
      "Input: tensor([1., 0.]), Prediction: tensor([0.9808], grad_fn=<UnbindBackward0>)\n",
      "Input: tensor([1., 1.]), Prediction: tensor([0.0124], grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 2),  # input layer\n",
    "            nn.Sigmoid(),  # activation function\n",
    "            nn.Linear(2, 1),  # output layer\n",
    "            nn.Sigmoid(),  # output activation function\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Initialize the MLP\n",
    "model = MLP()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=MODEL_MLP_LEARNING_RATE)\n",
    "\n",
    "# Train the MLP\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, outputs)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test the MLP\n",
    "test_preds = model(inputs)\n",
    "for input, pred in zip(inputs, test_preds):\n",
    "    print(f\"Input: {input}, Prediction: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
