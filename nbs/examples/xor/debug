# %%
""" Demonstrates the easy of integration of a custom layer """
import math
import torch
import torch.nn as nn
import numpy as np

# %%
class MyLinearLayer(nn.Module):
    """ Custom Linear layer but mimics a standard linear layer """
    def __init__(self, size_in, size_out):
        super().__init__()
        self.size_in, self.size_out = size_in, size_out
        # weights = torch.Tensor(size_out, size_in)

        # initialize weights and biases
        weights = torch.randn(
            self.size_in, self.size_out, dtype=torch.cdouble
        ) / math.sqrt(self.size_in)
        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.
        
        bias = torch.unsqueeze(
            torch.zeros(size_out, dtype=torch.cdouble, requires_grad=True), 0
        )
        self.bias = nn.Parameter(bias)

    def forward(self, x):
        w_times_x= torch.mm(x, self.weights)
        return torch.add(w_times_x, self.bias)  # w times x + b
        #return x @ self.weights + self.bias

# %%
class BasicModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = MyLinearLayer(2, 1)

    def forward(self, x):
        x = self.linear(x)
        return x

# %%
torch.manual_seed(0)  #  for repeatable results
model = BasicModel()

# %%
model.linear.weights

# %% [markdown]
# ## XOR Example

# %%
x = torch.Tensor([[0., 0.],
               [0., 1.],
               [1., 0.],
               [1., 1.]])

x = torch.Tensor([[1., 1.],
               [1., -1.],
               [-1., 1.],
               [-1., -1.]])

y = torch.Tensor([0., 1., 1., 0.]).reshape(x.shape[0], 1)

# %%
x

# %%
x = x.type(torch.cdouble)
model.forward(x)

# %%
print('Forward computation thru model:', model(x))

# %%
#PyTorch
class Complex_RMSE(nn.Module):
    def __init__(self, categories: int = 2, periodicity: int = 2):
        super(Complex_RMSE, self).__init__()
        self.categories = categories
        self.periodicity = periodicity

    def forward(self, inputs, targets): 
        target_angles = self.class2angle(targets)
        predicted_angles = torch.remainder(inputs.angle() + 2 * np.pi, 2 * np.pi)
        # z = torch.abs(predicted) * torch.exp(1.j * predicted_angles)
        # error calculation
        errors = torch.exp(1.0j * target_angles) - torch.exp(1.0j * predicted_angles)
        if self.periodicity > 1:
            # select smallest error
            idx = torch.argmin(torch.abs(errors), dim=1, keepdim=True)
            return errors.gather(1, idx).mean()
        else:
            return errors.mean()

    def class2angle(self, actual: torch.tensor) -> torch.tensor:
        # Returns a new tensor with an extra dimension
        if actual.size().__len__() == 1:
            actual = torch.unsqueeze(actual, 1)

        # Class to several angles due to periodicity using bisector
        return (
            (self.categories * torch.arange(self.periodicity) + actual + 0.5)
            / (self.categories * self.periodicity)
            * 2
            * np.pi
        )

# %%
from torch.autograd import Function

#######################################################
class MyComplexRMSELoss(Function):

    @staticmethod
    def forward(ctx, y_pred, y):
        target_angles = class2angle(y)
        predicted_angles = torch.remainder(inputs.angle() + 2 * np.pi, 2 * np.pi)
        ctx.save_for_backward(y_pred, y)
        return ( (y - y_pred)**2 ).mean()
    
    @staticmethod
    def backward(ctx, grad_output):
        y_pred, y = ctx.saved_tensors
        grad_input = 2 * (y_pred - y) / y_pred.shape[0]        
        return grad_input, None

    def class2angle(self, actual: torch.tensor) -> torch.tensor:
        # Returns a new tensor with an extra dimension
        if actual.size().__len__() == 1:
            actual = torch.unsqueeze(actual, 1)

        # Class to several angles due to periodicity using bisector
        return (
            (self.categories * torch.arange(self.periodicity) + actual + 0.5)
            / (self.categories * self.periodicity)
            * 2
            * np.pi
        )
    
#######################################################

# %%
# criterion = torch.nn.MSELoss(reduction='sum')
# criterion = MSEC(categories=2, periodicity=2)
criterion = Complex_RMSE(categories=2, periodicity=2)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)

# %%
y_pred = model(x)

# %%
y_pred

# %%
loss = criterion(y_pred, y)
loss

# %%
loss.backward()

# %%
optimizer.step()

# %%

for t in range(2000):
    # Forward pass: Compute predicted y by passing x to the model
    y_pred = model(x)

    # Compute and print loss
    loss = criterion(y_pred, y)
    if t % 100 == 99:
        # print(t, torch.abs(loss))
        print(t, torch.square(
                np.pi - torch.abs(torch.abs(loss) - np.pi)
            ))
        

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print(f'Result: {model.string()}')

# %%


# %%


# %%
""" Demonstrates the easy of integration of a custom layer """
import math
import torch
import torch.nn as nn
import numpy as np

class MyLinearLayer(nn.Module):
    """ Custom Linear layer but mimics a standard linear layer """
    def __init__(self, size_in, size_out):
        super().__init__()
        self.size_in, self.size_out = size_in, size_out
        weights = torch.Tensor(size_out, size_in)
        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.
        bias = torch.Tensor(size_out)
        self.bias = nn.Parameter(bias)

        # initialize weights and biases
        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.bias, -bound, bound)  # bias init

    def forward(self, x):
        w_times_x= torch.mm(x, self.weights.t())
        return torch.add(w_times_x, self.bias)  # w times x + b


class BasicModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 128, 3)
        # self.linear = nn.Linear(256, 2)
        self.linear = MyLinearLayer(256, 2)

    def forward(self, x):
        x = self. conv(x)
        x = x.view(-1, 256)
        return self.linear(x)

torch.manual_seed(0)  #  for repeatable results
basic_model = BasicModel()
inp = np.array([[[[1,2,3,4],  # batch(=1) x channels(=1) x height x width
                  [1,2,3,4],
                  [1,2,3,4]]]])
x = torch.tensor(inp, dtype=torch.float)
print('Forward computation thru model:', basic_model(x))

# %%
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(basic_model.parameters(), lr=1e-6)

# %%
y_pred = basic_model(x)
# Compute and print loss
loss = criterion(y_pred, y)

# %%
y_pred

# %%
loss.backward()


