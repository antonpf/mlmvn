[
  {
    "objectID": "initialization.html",
    "href": "initialization.html",
    "title": "Initialization",
    "section": "",
    "text": "source\n\ncplx_kaiming_normal_\n\n cplx_kaiming_normal_ (tensor, a=0.0, mode='fan_in',\n                       nonlinearity='leaky_relu')\n\n\nsource\n\n\ncplx_kaiming_uniform_\n\n cplx_kaiming_uniform_ (tensor, a=0.0, mode='fan_in',\n                        nonlinearity='leaky_relu')\n\n\nsource\n\n\ncplx_xavier_normal_\n\n cplx_xavier_normal_ (tensor, gain=1.0)\n\n\nsource\n\n\ncplx_xavier_uniform_\n\n cplx_xavier_uniform_ (tensor, gain=1.0)\n\n\nsource\n\n\ncplx_trabelsi_standard_\n\n cplx_trabelsi_standard_ (tensor, kind='glorot')\n\nStandard complex initialization proposed in Trabelsi et al. (2018).\n\nsource\n\n\ncplx_trabelsi_independent_\n\n cplx_trabelsi_independent_ (tensor, kind='glorot')\n\nOrthogonal complex initialization proposed in Trabelsi et al. (2018).\n\nsource\n\n\ncplx_normal_independent_\n\n cplx_normal_independent_ (tensor, a=0.0, b=1.0)\n\n\nsource\n\n\ncplx_uniform_independent_\n\n cplx_uniform_independent_ (tensor, a=0.0, b=1.0)\n\n\nsource\n\n\nones_\n\n ones_ (tensor, imag_zero=False)\n\n\nsource\n\n\nzeros_\n\n zeros_ (tensor)\n\n\nweights = torch.randn(48, 100, dtype=torch.cdouble) / math.sqrt(48)\n\n\ncplx_kaiming_normal_(weights)\n\n\ncplx_trabelsi_standard_(weights)\n\n\nones_(weights, imag_zero=True)\n\n\ncplx_normal_independent_(weights)\n\n\ncplx_normal_independent_(weights)\n\n\nweights\n\ntensor([[ 0.4300+0.3978j,  0.4044+0.0886j, -0.0497+0.3637j,  ...,\n         -1.1625-0.2458j, -0.3606+1.3778j,  1.3233+0.1381j],\n        [-0.0352+0.4488j, -0.4317+0.1049j, -0.5743-0.0114j,  ...,\n          1.3755-0.1862j, -0.1263+1.6037j, -0.4251+0.0149j],\n        [ 0.1022+2.1415j, -0.3880-0.8916j,  0.9741+0.4967j,  ...,\n         -0.4281+1.6826j, -0.3992+0.4414j,  1.5657+0.3260j],\n        ...,\n        [-1.2332+1.4589j, -0.0185-3.0959j, -0.4712+2.1996j,  ...,\n         -0.4455+1.3777j,  0.4046-0.8535j,  1.3204+1.6157j],\n        [-1.3413-0.8369j, -1.7481+0.1788j,  0.2018-0.2512j,  ...,\n         -0.3099-1.4537j,  0.1969+3.0771j,  0.7079+1.1503j],\n        [ 1.7616+1.3624j,  0.6755+0.8201j, -0.2582+0.6527j,  ...,\n         -1.9779+0.6458j,  0.1358+0.4088j,  0.0482+0.3482j]],\n       dtype=torch.complex128)"
  },
  {
    "objectID": "loss.html",
    "href": "loss.html",
    "title": "Loss",
    "section": "",
    "text": "source\n\nComplexMSELoss\n\n ComplexMSELoss (*args, **kwargs)\n\nBase class to create custom autograd.Function\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n\nsource\n\n\nComplexMSE_adjusted_error\n\n ComplexMSE_adjusted_error (*args, **kwargs)\n\nBase class to create custom autograd.Function\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)"
  },
  {
    "objectID": "layers.html",
    "href": "layers.html",
    "title": "Layers",
    "section": "",
    "text": "The multi-layered feedforward structure with fully connected MVN is referred to as MLMVN. Considering a MLMVN structure \\([n\\)-\\(N_1\\)-\\(\\dots\\)-\\(N_{m-1}\\)-\\(N_m ]\\) with \\(n\\) inputs in the input layer, \\(m\\)-\\(1\\) hidden layers, and the output layer \\(m\\). The algorithm is divided into three steps. Before starting the iterative algorithm, the weights are randomly initialized, and the biases are set to zero."
  },
  {
    "objectID": "layers.html#mlmvn",
    "href": "layers.html#mlmvn",
    "title": "Layers",
    "section": "MLMVN",
    "text": "MLMVN\nThe algorithm is divided into three steps. Before starting the iterative algorithm, the weights are randomly initialized, and the biases are set to zero.\nFor weight adjustment three distinctions are made: the FirstLayer, HiddenLayer [\\(2\\) to \\(m-1\\)], and the OutputLayer. Thereby the weights are updated successively from layer \\(1\\) to layer \\(m\\). The \\(1st\\) hidden Layer is updated by \\[\\begin{equation*}\n    \\tilde{w}_0^{k1} = w_0^{k1} + \\frac{C_{k1}}{(n+1) \\cdot |z_{k1}|} \\cdot \\delta_{k1} \\,,\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    \\tilde{w}_i^{k1} = w_i^{k1} + \\frac{C_{k1}}{(n+1) \\cdot |z_{k1}|} \\cdot \\delta_{k1} \\cdot \\bar{x}_{i} \\,,\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    i = \\{1, \\dots, n\\} \\,.\n\\end{equation*}\\]\n\nsource\n\nFirstLayerFB\n\n FirstLayerFB (*args, **kwargs)\n\nBase class to create custom autograd.Function\n\nsource\n\n\nFirstLayer\n\n FirstLayer (size_in, size_out)\n\nCustom first layer, mimics a standard linear layer.\nThe hidden layer \\(2,\\dots,m-1\\) is updated by\n\\[\\begin{equation*}\n    \\tilde{w}_0^{kj} = w_0^{kj} + \\frac{C_{kj}}{(N_{j-1}+1) \\cdot |z_{kj}|} \\cdot \\delta_{kj}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    \\tilde{w}_i^{kj} = w_i^{kj} + \\frac{C_{kj}}{(N_{j-1}+1) \\cdot |z_{kj}|} \\cdot \\delta_{kj} \\cdot \\bar{\\tilde{Y}}_{i,j-1}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    i = \\{1, \\dots, N_{j-1}\\}; j = \\{2, \\dots, m-1\\}.\n\\end{equation*}\\]\n\nsource\n\n\nHiddenLayerFB\n\n HiddenLayerFB (*args, **kwargs)\n\nBase class to create custom autograd.Function\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n\nsource\n\n\nHiddenLayer\n\n HiddenLayer (size_in, size_out)\n\nCustom hidden layer, mimics a standard linear layer.\nFinally, the weights of the output layer \\(m\\) are updated \\[\\begin{equation*}\n    \\tilde{w}_0^{km} = w_0^{km} + \\frac{C_{km}}{N_{m-1}+1} \\cdot \\delta_{km} \\, ,\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    \\tilde{w}_i^{km} = w_i^{km} + \\frac{C_{km}}{N_{m-1}+1} \\cdot \\delta_{km} \\cdot \\bar{\\tilde{Y}}_{i,m-1} \\, ,\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    i = \\{1, \\dots, N_{m-1}\\} \\, ,\n\\end{equation*}\\] where \\(\\bar{\\tilde{Y}}_{i,j-1}\\) is the updated complex conjugated output of the \\(i\\)-th neuron from the \\(j-1\\)-th layer. The variable learning rate \\(\\frac{1}{|z|}\\) is an additional parameter for nonlinear mappings that makes learning smoother. The variable learning rate can be omitted in the output layer since the exact error is known here, and it is not computed heuristically as in the previous layers.\n\nsource\n\n\nOutputLayerFB\n\n OutputLayerFB (*args, **kwargs)\n\nBase class to create custom autograd.Function\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n\nsource\n\n\nOutputLayer\n\n OutputLayer (size_in, size_out)\n\nCustom output layer, mimics a standard linear layer."
  },
  {
    "objectID": "layers.html#activation",
    "href": "layers.html#activation",
    "title": "Layers",
    "section": "Activation",
    "text": "Activation\nThe activation function maps depending on the weighted sum \\(z\\) to the unit circle, which is divided into \\(k\\) sectors described by the set \\[\\begin{equation}\n    E_k = \\{1, \\varepsilon_k, \\varepsilon_k^2, \\dots, \\varepsilon_k^{k-1}  \\},\n\\end{equation}\\] with $ _k = e^{j} $, where \\(j\\) is the imaginary unit and \\(k \\in \\mathbb{N}_{>0}\\). Therefore, the activation function of a continuous MVN is defined by \\[\\begin{equation}\n    P(w_0 + w_1 x_1 + \\dots + w_n x_n) = P(z) = e^{j\\varphi} = \\frac{z}{|z|},\n\\end{equation}\\] where \\(w_0\\) is the bias, \\(w_i\\) is the corresponding weight to the input \\(x_i\\) with \\(i = \\{1,\\dots,n\\}\\) and \\(\\varphi \\in [0,2\\pi[\\) is the argument of the weighted sum \\(z\\). Fig. \\(\\ref{fig:complexActivation}\\) illustrates this context. The discrete activation function differs only in that the phase is adjusted to the nearest bisector, i.e. \\(P(z) \\in E_k \\cdot e^{j\\frac{\\pi}{k}}\\), where \\(e^{j\\frac{\\pi}{k}}\\) realizes a shift of half a sector to move from the sector borders to the bisectors.\n\nsource\n\ncmplx_phase_activation\n\n cmplx_phase_activation ()\n\nCustom Linear layer but mimics a standard linear layer\n\nsource\n\n\nphase_activation\n\n phase_activation (*args, **kwargs)\n\nBase class to create custom autograd.Function\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)"
  },
  {
    "objectID": "layers.html#dropout",
    "href": "layers.html#dropout",
    "title": "Layers",
    "section": "Dropout",
    "text": "Dropout\n\nsource\n\nMyDropout\n\n MyDropout (p:float=0.5)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nDropoutFB\n\n DropoutFB (*args, **kwargs)\n\nBase class to create custom autograd.Function\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)"
  },
  {
    "objectID": "examples/moons/moons_initializers.html",
    "href": "examples/moons/moons_initializers.html",
    "title": "Building a Binary Classifier",
    "section": "",
    "text": "In this example the moons dataset is used to build a binary classifcator. The aim is to find the boundary beetween two cresecent moons. The entire data set consists of 1500 data points and is shown in the image below."
  },
  {
    "objectID": "examples/moons/moons_initializers.html#mlmvn",
    "href": "examples/moons/moons_initializers.html#mlmvn",
    "title": "Building a Binary Classifier",
    "section": "MLMVN",
    "text": "MLMVN\n\nPATH = str(Path.cwd() / \"models/moons-mlmvn.pt\")\n\n\ndef backward_hook(module, grad_input, grad_output):\n    print(\"module:\", module)\n    print(\"grad_input:\", grad_input)\n    print(\"grad_output:\", grad_output)\n\n\nmodel_dict: dict = {}\n\n\ndef fc_hook(layer_name, module, grad_input, grad_output):\n    if layer_name in model_dict:\n        model_dict[layer_name][\"weights\"].append(module.weights.detach().clone())\n        model_dict[layer_name][\"bias\"].append(module.bias.detach().clone())\n        model_dict[layer_name][\"grad_input\"].append(grad_input)\n        model_dict[layer_name][\"grad_output\"].append(grad_output)\n    else:\n        model_dict[layer_name] = {}\n        model_dict[layer_name][\"weights\"] = []\n        model_dict[layer_name][\"weights\"].append(module.weights.detach().clone())\n        model_dict[layer_name][\"bias\"] = []\n        model_dict[layer_name][\"bias\"].append(module.bias.detach().clone())\n        model_dict[layer_name][\"grad_input\"] = []\n        model_dict[layer_name][\"grad_input\"].append(grad_input)\n        model_dict[layer_name][\"grad_output\"] = []\n        model_dict[layer_name][\"grad_output\"].append(grad_output)\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(2, 5)\n        self.phase_act1 = cmplx_phase_activation()\n        # self.hidden_linear = HiddenLayer(4, 4)\n        # self.phase_act2 = cmplx_phase_activation()\n        self.output_linear = OutputLayer(5, 1)\n        self.phase_act3 = cmplx_phase_activation()\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        # x = self.hidden_linear(x)\n        # x = self.phase_act2(x)\n        x = self.output_linear(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return output\n\n    def initialize_weights(self, initilizer=\"uniform\"):\n        if initilizer == \"uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"ones\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"zeros\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"kaiming_normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"kaiming_uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"xavier_normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"xavier_uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_glorot\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_glorot\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_xavier\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_xavier\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_kaiming\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_kaiming\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_he\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_he\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"standard\":\n            pass\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        y_pred = model(X)\n        y_pred = angle2class(y_pred, categories, periodicity)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\ninitilizers = [\n    \"uniform\",\n    \"normal\",\n    # \"zeros\",\n    # \"ones\",\n    \"kaiming_normal\",\n    \"kaiming_uniform\",\n    \"xavier_normal\",\n    \"xavier_uniform\",\n    \"trabelsi_standard_xavier\",\n    \"trabelsi_independent_xavier\",\n    \"trabelsi_standard_kaiming\",\n    \"trabelsi_independent_kaiming\",\n    \"standard\",\n]\n\n\nfor initilizer in initilizers:\n    model = Model(categories=categories, periodicity=periodicity)\n    model.initialize_weights(initilizer=initilizer)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    task = Task.init(\n        project_name=\"mlmvn\",\n        task_name=\"moons-mlmvn-[2-5-1]\",\n        tags=[\"mlmvn\", \"moons\", \"initilizer\"],\n    )\n    writer = SummaryWriter()\n\n    #  capture a dictionary of hyperparameters with config\n    config_dict = {\n        \"learning_rate\": lr,\n        \"epochs\": epochs,\n        \"batch_size\": batch_size,\n        \"optim\": \"ECL\",\n        \"categories\": categories,\n        \"periodicity\": periodicity,\n        \"layer\": \"[2-5-1]\",\n        \"initilizer\": initilizer,\n    }\n    task.connect(config_dict)\n\n    losses, scores = fit(\n        model,\n        X_train_t,\n        y_train_t,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    y_pred = model(X_train_t)\n    y_pred = angle2class(y_pred, categories, periodicity)\n    acc = accuracy(y_pred.squeeze(), y_train_t)\n    print(\"Train Acc.: \", acc)\n\n    Logger.current_logger().report_single_value(\n        name=\"Train Acc.\",\n        value=acc,\n    )\n\n    y_pred = model(X_test_t)\n    y_pred = angle2class(y_pred, categories, periodicity)\n    acc = accuracy(y_pred.squeeze(), y_test_t)\n    print(\"Val Acc.: \", acc)\n\n    Logger.current_logger().report_single_value(\n        name=\"Val Acc.\",\n        value=acc,\n    )\n\n    print(classification_report(y_test, y_pred.detach().numpy(), zero_division=0))\n\n    task.mark_completed()\n    task.close()\n\nClearML Task: created new task id=58568e2e6cef40a6a981180c326a81e6\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/58568e2e6cef40a6a981180c326a81e6/output/log\n2022-09-27 17:02:53,863 - clearml.frameworks - INFO - Found existing registered model id=8014ed23892d40fdbd99f3b2d1ec333c [/home/antonpfeifer/Documents/mlmvn/nbs/examples/moons/models/moons-mlmvn.pt] reusing it.\nTrain Acc.:  0.9722222222222222\nVal Acc.:  0.9633333333333334\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96       311\n           1       0.97      0.96      0.96       289\n\n    accuracy                           0.96       600\n   macro avg       0.96      0.96      0.96       600\nweighted avg       0.96      0.96      0.96       600\n\nClearML Task: created new task id=be04191c8fd64cba95f9bb65ad3ef50d\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/be04191c8fd64cba95f9bb65ad3ef50d/output/log\nTrain Acc.:  0.9733333333333334\nVal Acc.:  0.965\n              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97       311\n           1       0.98      0.95      0.96       289\n\n    accuracy                           0.96       600\n   macro avg       0.97      0.96      0.96       600\nweighted avg       0.97      0.96      0.96       600\n\nClearML Task: created new task id=98feb37c766847de91206bc1dea5a22e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/98feb37c766847de91206bc1dea5a22e/output/log\nTrain Acc.:  0.9733333333333334\nVal Acc.:  0.9683333333333334\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       311\n           1       0.98      0.96      0.97       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=defc78e47c854451aa46599968ba7aef\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/defc78e47c854451aa46599968ba7aef/output/log\nTrain Acc.:  0.9711111111111111\nVal Acc.:  0.9666666666666667\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97       311\n           1       0.97      0.96      0.97       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=5318bb3738ed4e1f994a20204aac1b7e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/5318bb3738ed4e1f994a20204aac1b7e/output/log\nTrain Acc.:  0.97\nVal Acc.:  0.97\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       311\n           1       0.98      0.96      0.97       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=c1fff7c84ec442f5b6d8d5e80993e74a\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/c1fff7c84ec442f5b6d8d5e80993e74a/output/log\nTrain Acc.:  0.97\nVal Acc.:  0.9666666666666667\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       311\n           1       0.98      0.95      0.96       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=d9f1a9be89854a3ca8b817c1dad068dd\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d9f1a9be89854a3ca8b817c1dad068dd/output/log\nTrain Acc.:  0.9722222222222222\nVal Acc.:  0.965\n              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.97       311\n           1       0.98      0.95      0.96       289\n\n    accuracy                           0.96       600\n   macro avg       0.97      0.96      0.96       600\nweighted avg       0.97      0.96      0.96       600\n\nClearML Task: created new task id=5b293ec3b4c440fd907e3a879dee8048\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/5b293ec3b4c440fd907e3a879dee8048/output/log\nTrain Acc.:  0.9722222222222222\nVal Acc.:  0.9716666666666667\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97       311\n           1       0.98      0.96      0.97       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=6f6f59c40b0648fa9b30ae5adc4b4c9e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6f6f59c40b0648fa9b30ae5adc4b4c9e/output/log\nTrain Acc.:  0.9722222222222222\nVal Acc.:  0.97\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       311\n           1       0.98      0.96      0.97       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=6d347fd5f6ca42a3ab0d310fe36dba9b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6d347fd5f6ca42a3ab0d310fe36dba9b/output/log\nTrain Acc.:  0.9733333333333334\nVal Acc.:  0.9666666666666667\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       311\n           1       0.98      0.95      0.96       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\nClearML Task: created new task id=1719ad4e6177407e98d00aeb7621e75f\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1719ad4e6177407e98d00aeb7621e75f/output/log\nTrain Acc.:  0.9722222222222222\nVal Acc.:  0.97\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97       311\n           1       0.98      0.96      0.97       289\n\n    accuracy                           0.97       600\n   macro avg       0.97      0.97      0.97       600\nweighted avg       0.97      0.97      0.97       600\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n         0.0       0.98      0.95      0.97       320\n         1.0       0.95      0.98      0.96       280\n\n    accuracy                           0.96       600\n   macro avg       0.96      0.97      0.96       600\nweighted avg       0.97      0.96      0.97       600"
  },
  {
    "objectID": "examples/moons/moons.html",
    "href": "examples/moons/moons.html",
    "title": "Building a Binary Classifier",
    "section": "",
    "text": "In this example the moons dataset is used to build a binary classifcator. The aim is to find the boundary beetween two cresecent moons. The entire data set consists of 1500 data points and is shown in the image below."
  },
  {
    "objectID": "examples/moons/moons.html#mlmvn",
    "href": "examples/moons/moons.html#mlmvn",
    "title": "Building a Binary Classifier",
    "section": "MLMVN",
    "text": "MLMVN\n\ndef backward_hook(module, grad_input, grad_output):\n    print(\"module:\", module)\n    print(\"grad_input:\", grad_input)\n    print(\"grad_output:\", grad_output)\n\n\nmodel_dict: dict = {}\n\n\ndef fc_hook(layer_name, module, grad_input, grad_output):\n    if layer_name in model_dict:\n        model_dict[layer_name][\"weights\"].append(module.weights.detach().clone())\n        model_dict[layer_name][\"bias\"].append(module.bias.detach().clone())\n        model_dict[layer_name][\"grad_input\"].append(grad_input)\n        model_dict[layer_name][\"grad_output\"].append(grad_output)\n    else:\n        model_dict[layer_name] = {}\n        model_dict[layer_name][\"weights\"] = []\n        model_dict[layer_name][\"weights\"].append(module.weights.detach().clone())\n        model_dict[layer_name][\"bias\"] = []\n        model_dict[layer_name][\"bias\"].append(module.bias.detach().clone())\n        model_dict[layer_name][\"grad_input\"] = []\n        model_dict[layer_name][\"grad_input\"].append(grad_input)\n        model_dict[layer_name][\"grad_output\"] = []\n        model_dict[layer_name][\"grad_output\"].append(grad_output)\n\n\nclass MLMVN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.first_linear = FirstLayer(2, 5)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_linear = HiddenLayer(5, 5)\n        self.phase_act2 = cmplx_phase_activation()\n        self.output_linear = OutputLayer(5, 1)\n        self.phase_act3 = cmplx_phase_activation()\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_linear(x)\n        x = self.phase_act2(x)\n        x = self.output_linear(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def predict(self, x, loss):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return output\n\n\n# Implement the train function given a training dataset X and correcsponding labels y\ndef train(\n    model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity\n):\n    # List of losses for visualization\n    losses = []\n    scores = []\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            # Forward pass: Compute predicted y by passing x to the model\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n\n            if i % 10 == 9:\n                print(torch.abs(loss))\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            # Zero gradients, perform a backward pass, and update the weights.\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        y_pred = model(X)\n        y_pred = angle2class(y_pred, categories, periodicity)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        log_dict = {\n            \"loss\": torch.tensor(losses[-1]),\n            \"acc\": torch.tensor(scores[-1]),\n        }\n        # wandb.log(log_dict)\n\n        # for key in model_dict:\n        #     for key_layer in model_dict[key]:\n        #         if key_layer in [\"weights\", \"bias\"]:\n        #             log_label = str(key) + \"_\" + str(key_layer)\n        #             log_label.replace(\" \", \"\")\n        #             wandb.log(\n        #                 {\n        #                     log_label\n        #                     + \"_real\": wandb.Histogram(\n        #                         model_dict[key][key_layer][-1].real\n        #                     ),\n        #                     log_label\n        #                     + \"_imag\": wandb.Histogram(\n        #                         model_dict[key][key_layer][-1].imag\n        #                     ),\n        #                     log_label\n        #                     + \"_mag\": torch.abs(model_dict[key][key_layer][-1]),\n        #                     log_label\n        #                     + \"_angle\": torch.angle(model_dict[key][key_layer][-1]),\n        #                 }\n        #             )\n\n    return (\n        losses,\n        scores,\n    )\n\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(10):\n    writer.add_scalar(\"Loss/train\", np.random.random(), n_iter)\n    writer.add_scalar(\"Loss/test\", np.random.random(), n_iter)\n    writer.add_scalar(\"Accuracy/train\", np.random.random(), n_iter)\n    writer.add_scalar(\"Accuracy/test\", np.random.random(), n_iter)\n    x = np.random.random(1000)\n    writer.add_histogram(\"distribution centers\", x + n_iter, n_iter)\n\nwriter.close()\n\n\n\n\n\n\n\n\nTrain Acc.:  0.9544444444444444\n\n\n\n\nTest Acc.:  0.9533333333333334\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n         0.0       0.98      0.93      0.96       329\n         1.0       0.92      0.98      0.95       271\n\n    accuracy                           0.95       600\n   macro avg       0.95      0.96      0.95       600\nweighted avg       0.96      0.95      0.95       600"
  },
  {
    "objectID": "examples/xor/xor.html",
    "href": "examples/xor/xor.html",
    "title": "XOR",
    "section": "",
    "text": "The dataset contains four input-output mappings with binary classes. The two-dimensional input \\(x\\) is mapped to a class label \\(y\\). The following table shows the truth table with associated labels for the XOR gate.\n\\[\n\\begin{aligned}\n    \\begin{array}{cc|c|cc}\n        x_1 & x_2 & y & z & arg(z) \\\\\n        \\hline\n        1 &  1  & 0 &  1+j &  45° \\\\\n        1 & -1  & 1 &  1-j & 315° \\\\\n        -1 &  1 & 1 & -1+j & 135° \\\\\n        -1 & -1 & 0 & -1-j & 225° \\\\\n    \\end{array}\n\\end{aligned}\n\\]\nIf we consider \\(x_1\\) as \\(Re(z)\\) and \\(x_2\\) as \\(Im(z)\\), the problem can also be expressed graphically into the complex domain.\n\n\n\n\n# Create data\nx = torch.Tensor([[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]])\n\nx = x.type(torch.cdouble)\n\ny = torch.Tensor([0.0, 1.0, 1.0, 0.0]).reshape(x.shape[0], 1)\n\n\nclass BasicModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = FirstLayer(2, 2)\n        self.phase_act = cmplx_phase_activation()\n        self.linear1 = OutputLayer(2, 1)\n        self.phase_act = cmplx_phase_activation()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.linear1(x)\n        x = self.phase_act(x)\n        return x\n\n\nmodel = BasicModel()\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=1)\ncategories = 2\nperiodicity = 2\n\n\nfor t in range(5):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n    loss = criterion(y_pred, y, categories, periodicity)\n    print(t, torch.abs(loss))\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step(inputs=x, layers=list(model.children()))\n\n0 tensor(1.4608, dtype=torch.float64, grad_fn=<AbsBackward0>)\n1 tensor(0.1080, dtype=torch.float64, grad_fn=<AbsBackward0>)\n2 tensor(0.0730, dtype=torch.float64, grad_fn=<AbsBackward0>)\n3 tensor(0.0492, dtype=torch.float64, grad_fn=<AbsBackward0>)\n4 tensor(0.0329, dtype=torch.float64, grad_fn=<AbsBackward0>)\n\n\n\nfor idx, param in enumerate(model.parameters()):\n    param.real\n\n\npredictions = model(x)\n\n\ndef angle2class(x: torch.tensor, categories, periodicity) -> torch.tensor:\n    tmp = x.angle() + 2 * np.pi\n    angle = torch.remainder(tmp, 2 * np.pi)\n\n    # This will be the discrete output (the number of sector)\n    o = torch.floor(categories * periodicity * angle / (2 * np.pi))\n    return torch.remainder(o, categories)\n\n\nangle2class(predictions, 2, 2)\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]], dtype=torch.float64, grad_fn=<RemainderBackward0>)"
  },
  {
    "objectID": "examples/initialization/00_initialization.html",
    "href": "examples/initialization/00_initialization.html",
    "title": "mlmvn",
    "section": "",
    "text": "def randn(shape):\n    return torch.randn(shape, dtype=torch.double)\n\n\nsx = 32, 12, 31, 47\nsw = 7, 12, 7, 11\n\nx = cplx.Cplx(randn(sx), randn(sx))\nw = cplx.Cplx(randn(sw), randn(sw))\nb = cplx.Cplx(randn(sw[0]), randn(sw[0]))\n\n# do the 2d convo manually\nre = F.conv2d(x.real, w.real, bias=b.real) - F.conv2d(x.imag, w.imag, bias=None)\nim = F.conv2d(x.real, w.imag, bias=b.imag) + F.conv2d(x.imag, w.real, bias=None)\n\n# use the function from cplx\ncc = cplx.conv2d(x, w, bias=b)\n\n\ndef cplx_trabelsi_independent_(mod):\n    if not hasattr(mod, \"weight\"):\n        return\n\n    # Trabelsi orthogonal weight initializer\n    if isinstance(mod.weight, cplx):\n        init.cplx_trabelsi_independent_(mod.weight)\n\n\nimport torch\nimport numpy as np\n\nfrom cplxmodule import Cplx\nfrom cplxmodule.nn import init, CplxLinear, CplxConv2d\n\n\ndef cplx_trabelsi_independent_(mod):\n    if not hasattr(mod, \"weight\"):\n        return\n\n    # Trabelsi orthogonal weight initializer\n    if isinstance(mod.weight, Cplx):\n        init.cplx_trabelsi_independent_(mod.weight)\n\n\n# a model with some structure\nmodule = torch.nn.ModuleDict(\n    {\n        \"linear\": CplxLinear(11, 17),\n        \"conv\": CplxConv2d(13, 19, 5),\n    }\n).double()\n\n# standard torch functionality `module.apply`\nmodule.apply(cplx_trabelsi_independent_)\n\n# according to Trabelsi et al. (2018) the reshaped weight bust be an almost unitary matrix\nw = module[\"conv\"].weight\nm = w.reshape(w.shape[:2].numel(), w.shape[2:].numel()).detach().numpy()\nmHm = m.conjugate().T @ m"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip.html",
    "href": "examples/autass/autass_multiple_run_angle_clip.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip.html#config",
    "href": "examples/autass/autass_multiple_run_angle_clip.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1\nclip_angle_value = 1000000"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip.html#single-layer",
    "href": "examples/autass/autass_multiple_run_angle_clip.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=854e9bc7dc7e496ea841a13f617da653\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/854e9bc7dc7e496ea841a13f617da653/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 14:25:57,872 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.312786317016275\nEpoch 19 loss is 0.25568547822538934\nEpoch 29 loss is 0.3389669520974066\nEpoch 39 loss is 0.28586026849296975\nEpoch 49 loss is 0.2642590965695329\nEpoch 59 loss is 0.2561917205608428\nEpoch 69 loss is 0.2538907377290161\nEpoch 79 loss is 0.25494624541893784\nEpoch 89 loss is 0.2586601196421547\nEpoch 99 loss is 0.26136219948247824\nEpoch 109 loss is 0.25920406138033625\nEpoch 119 loss is 0.257771638513778\nEpoch 129 loss is 0.256717965344817\nEpoch 139 loss is 0.2562337465502512\nEpoch 149 loss is 0.25599395140123643\nEpoch 159 loss is 0.2555283782128066\nEpoch 169 loss is 0.25500644305516845\nEpoch 179 loss is 0.2546311936985765\nEpoch 189 loss is 0.2545335270219525\nEpoch 199 loss is 0.2544423936517196\nTrain Acc.:  0.7605486358877945\n              precision    recall  f1-score   support\n\n           0       0.93      0.85      0.89      1063\n           1       0.83      0.75      0.79      1064\n           2       0.90      0.72      0.80      1064\n           3       0.84      0.73      0.78      1064\n           4       0.78      0.68      0.73      1064\n           5       0.82      0.88      0.85      1063\n           6       0.75      0.59      0.66      1064\n           7       0.99      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.81      0.81      0.81      1064\n          10       0.89      0.87      0.88      1064\n\n   micro avg       0.87      0.81      0.84     11702\n   macro avg       0.87      0.81      0.83     11702\nweighted avg       0.87      0.81      0.83     11702\n samples avg       0.78      0.81      0.79     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23875833944773336\nEpoch 19 loss is 0.35046091409537855\nEpoch 29 loss is 0.3474688264639837\nEpoch 39 loss is 0.3460845462685393\nEpoch 49 loss is 0.34521615010787904\nEpoch 59 loss is 0.34480239729514767\nEpoch 69 loss is 0.3445531601110381\nEpoch 79 loss is 0.344414452281505\nEpoch 89 loss is 0.3443463357528307\nEpoch 99 loss is 0.34433482782616937\nEpoch 109 loss is 0.3443202594365852\nEpoch 119 loss is 0.3443419606632198\nEpoch 129 loss is 0.34439574951313695\nEpoch 139 loss is 0.34447653188306476\nEpoch 149 loss is 0.34455745559313744\nEpoch 159 loss is 0.3446490512911633\nEpoch 169 loss is 0.34472025490019265\nEpoch 179 loss is 0.34483666034343385\nEpoch 189 loss is 0.3449189385351294\nEpoch 199 loss is 0.34502282438946064\nTrain Acc.:  0.7681329715640823\n              precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85      1063\n           1       0.88      0.68      0.77      1064\n           2       0.90      0.73      0.81      1064\n           3       0.86      0.74      0.80      1063\n           4       0.76      0.75      0.76      1064\n           5       0.81      0.74      0.77      1064\n           6       0.80      0.72      0.75      1064\n           7       0.99      0.97      0.98      1064\n           8       1.00      0.99      1.00      1064\n           9       0.88      0.84      0.86      1064\n          10       0.92      0.80      0.86      1064\n\n   micro avg       0.88      0.80      0.84     11702\n   macro avg       0.88      0.80      0.84     11702\nweighted avg       0.88      0.80      0.84     11702\n samples avg       0.78      0.80      0.79     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2971094108173737\nEpoch 19 loss is 0.3869108816673947\nEpoch 29 loss is 0.4365067504356039\nEpoch 39 loss is 0.4177013765514301\nEpoch 49 loss is 0.4124413908694452\nEpoch 59 loss is 0.40500117037391986\nEpoch 69 loss is 0.41131196047303786\nEpoch 79 loss is 0.4181249931351551\nEpoch 89 loss is 0.41661642708626356\nEpoch 99 loss is 0.4181854146942287\nEpoch 109 loss is 0.4133348665799685\nEpoch 119 loss is 0.4099116159030149\nEpoch 129 loss is 0.4199283736599332\nEpoch 139 loss is 0.417771504949945\nEpoch 149 loss is 0.4188665644250296\nEpoch 159 loss is 0.42150588067988737\nEpoch 169 loss is 0.4230460352811647\nEpoch 179 loss is 0.4250528931966397\nEpoch 189 loss is 0.4252225452870114\nEpoch 199 loss is 0.42203162628683427\nTrain Acc.:  0.7921464738180187\n              precision    recall  f1-score   support\n\n           0       0.95      0.88      0.92      1064\n           1       0.85      0.66      0.75      1064\n           2       0.92      0.80      0.86      1064\n           3       0.88      0.83      0.86      1063\n           4       0.71      0.83      0.77      1064\n           5       0.78      0.88      0.83      1064\n           6       0.79      0.72      0.75      1063\n           7       1.00      0.97      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.84      0.79      0.81      1064\n          10       0.84      0.82      0.83      1064\n\n   micro avg       0.87      0.84      0.85     11702\n   macro avg       0.87      0.84      0.85     11702\nweighted avg       0.87      0.84      0.85     11702\n samples avg       0.81      0.84      0.82     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2795195632281697\nEpoch 19 loss is 0.3375428780831629\nEpoch 29 loss is 0.33256474318305307\nEpoch 39 loss is 0.33185831863511417\nEpoch 49 loss is 0.3295307025439074\nEpoch 59 loss is 0.3210891922650079\nEpoch 69 loss is 0.32034934104938245\nEpoch 79 loss is 0.3163362890807787\nEpoch 89 loss is 0.315736489078309\nEpoch 99 loss is 0.3155383143134653\nEpoch 109 loss is 0.31553327463043673\nEpoch 119 loss is 0.31600845927522986\nEpoch 129 loss is 0.3150536779122853\nEpoch 139 loss is 0.3150594287707738\nEpoch 149 loss is 0.31524582921067934\nEpoch 159 loss is 0.3153440366348357\nEpoch 169 loss is 0.31513214139197515\nEpoch 179 loss is 0.3152703075228755\nEpoch 189 loss is 0.3215461540110862\nEpoch 199 loss is 0.31620214863132784\nTrain Acc.:  0.7109833999188155\n              precision    recall  f1-score   support\n\n           0       0.87      0.95      0.91      1063\n           1       0.84      0.76      0.80      1064\n           2       0.82      0.71      0.76      1064\n           3       0.87      0.84      0.85      1063\n           4       0.82      0.53      0.64      1064\n           5       0.86      0.74      0.80      1064\n           6       0.90      0.02      0.04      1064\n           7       1.00      0.97      0.98      1064\n           8       0.99      1.00      1.00      1064\n           9       0.63      0.93      0.75      1064\n          10       0.91      0.84      0.87      1064\n\n   micro avg       0.85      0.75      0.80     11702\n   macro avg       0.87      0.75      0.76     11702\nweighted avg       0.87      0.75      0.76     11702\n samples avg       0.73      0.75      0.74     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.39677247203032356\nEpoch 19 loss is 0.35580593764225016\nEpoch 29 loss is 0.42506711288150867\nEpoch 39 loss is 0.4258715088317278\nEpoch 49 loss is 0.42243423247066575\nEpoch 59 loss is 0.4237868170495762\nEpoch 69 loss is 0.41925568150605863\nEpoch 79 loss is 0.439859722214063\nEpoch 89 loss is 0.4537169174860011\nEpoch 99 loss is 0.4516644332667944\nEpoch 109 loss is 0.4505254539904648\nEpoch 119 loss is 0.44988270158956056\nEpoch 129 loss is 0.4494370519612547\nEpoch 139 loss is 0.449528904207753\nEpoch 149 loss is 0.44943834853116904\nEpoch 159 loss is 0.449371091649806\nEpoch 169 loss is 0.4491654165153552\nEpoch 179 loss is 0.4491931206111527\nEpoch 189 loss is 0.44876226191301677\nEpoch 199 loss is 0.44890430281994864\nTrain Acc.:  0.6543465720939176\n              precision    recall  f1-score   support\n\n           0       0.70      0.86      0.77      1064\n           1       0.00      0.00      0.00      1064\n           2       0.95      0.68      0.79      1064\n           3       0.88      0.71      0.78      1064\n           4       0.69      0.60      0.64      1064\n           5       0.63      0.84      0.72      1064\n           6       0.79      0.65      0.71      1063\n           7       0.97      0.98      0.98      1064\n           8       1.00      1.00      1.00      1063\n           9       0.75      0.75      0.75      1064\n          10       0.91      0.83      0.87      1064\n\n   micro avg       0.81      0.72      0.76     11702\n   macro avg       0.75      0.72      0.73     11702\nweighted avg       0.75      0.72      0.73     11702\n samples avg       0.69      0.72      0.70     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=47852b7bd30c475a9fbb69e5802aea36\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/47852b7bd30c475a9fbb69e5802aea36/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 14:42:43,693 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.15395791001477882\nEpoch 19 loss is 0.15815942352420895\nEpoch 29 loss is 0.1521475958209485\nEpoch 39 loss is 0.22264992876002446\nEpoch 49 loss is 0.2434950175800985\nEpoch 59 loss is 0.3000683165649394\nEpoch 69 loss is 0.2986102485389353\nEpoch 79 loss is 0.2978965348099431\nEpoch 89 loss is 0.2976461313968712\nEpoch 99 loss is 0.297615187920047\nEpoch 109 loss is 0.2975814279553468\nEpoch 119 loss is 0.2973692060082367\nEpoch 129 loss is 0.2971646093759705\nEpoch 139 loss is 0.2969823604569539\nEpoch 149 loss is 0.2968226945940804\nEpoch 159 loss is 0.29669339181528953\nEpoch 169 loss is 0.2965860987914452\nEpoch 179 loss is 0.2964960589856526\nEpoch 189 loss is 0.296397997745145\nEpoch 199 loss is 0.2963285508675669\nTrain Acc.:  0.8742068494028671\n              precision    recall  f1-score   support\n\n           0       0.94      0.90      0.92      1063\n           1       0.91      0.89      0.90      1064\n           2       0.97      0.95      0.96      1064\n           3       0.95      0.93      0.94      1064\n           4       0.79      0.76      0.78      1064\n           5       0.89      0.90      0.90      1063\n           6       0.84      0.84      0.84      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      0.99      1.00      1064\n           9       0.89      0.87      0.88      1064\n          10       0.92      0.96      0.94      1064\n\n   micro avg       0.92      0.91      0.91     11702\n   macro avg       0.92      0.91      0.91     11702\nweighted avg       0.92      0.91      0.91     11702\n samples avg       0.89      0.91      0.89     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23371587049756218\nEpoch 19 loss is 0.2171053793431578\nEpoch 29 loss is 0.2117481726413607\nEpoch 39 loss is 0.20548662762905423\nEpoch 49 loss is 0.20055165801015304\nEpoch 59 loss is 0.19926006465921117\nEpoch 69 loss is 0.20121967169245297\nEpoch 79 loss is 0.2012524994483403\nEpoch 89 loss is 0.2089158789663104\nEpoch 99 loss is 0.2069847307533938\nEpoch 109 loss is 0.20158804575222558\nEpoch 119 loss is 0.19908199121184547\nEpoch 129 loss is 0.1983263363973267\nEpoch 139 loss is 0.1981246632369139\nEpoch 149 loss is 0.19843244794655257\nEpoch 159 loss is 0.19990613128620335\nEpoch 169 loss is 0.20346991661240654\nEpoch 179 loss is 0.20220646457032931\nEpoch 189 loss is 0.20162040661533578\nEpoch 199 loss is 0.20261586962160896\nTrain Acc.:  0.7864421988164163\n              precision    recall  f1-score   support\n\n           0       0.95      0.91      0.93      1063\n           1       0.90      0.78      0.84      1064\n           2       0.98      0.66      0.79      1064\n           3       0.88      0.91      0.90      1063\n           4       0.73      0.60      0.66      1064\n           5       0.81      0.92      0.86      1064\n           6       0.75      0.77      0.76      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.85      0.87      0.86      1064\n          10       0.84      0.82      0.83      1064\n\n   micro avg       0.88      0.84      0.86     11702\n   macro avg       0.88      0.84      0.86     11702\nweighted avg       0.88      0.84      0.86     11702\n samples avg       0.81      0.84      0.82     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.17359070471532762\nEpoch 19 loss is 0.19923888591987557\nEpoch 29 loss is 0.22116873452047683\nEpoch 39 loss is 0.2029670658449249\nEpoch 49 loss is 0.20419508592690006\nEpoch 59 loss is 0.19794629333818395\nEpoch 69 loss is 0.1952743740865991\nEpoch 79 loss is 0.19510413114878267\nEpoch 89 loss is 0.19646997157487037\nEpoch 99 loss is 0.19606822933983284\nEpoch 109 loss is 0.19653955121411834\nEpoch 119 loss is 0.19678779104367372\nEpoch 129 loss is 0.19675274303142729\nEpoch 139 loss is 0.19700162446357028\nEpoch 149 loss is 0.1971595217735474\nEpoch 159 loss is 0.19742537431793827\nEpoch 169 loss is 0.19757406298027266\nEpoch 179 loss is 0.1976568692427292\nEpoch 189 loss is 0.19808705126678153\nEpoch 199 loss is 0.19829968689615954\nTrain Acc.:  0.8086610976990621\n              precision    recall  f1-score   support\n\n           0       0.93      0.90      0.91      1064\n           1       0.87      0.82      0.85      1064\n           2       0.97      0.91      0.94      1064\n           3       0.96      0.92      0.94      1063\n           4       0.65      0.71      0.68      1064\n           5       0.86      0.87      0.87      1064\n           6       0.78      0.75      0.77      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.82      0.80      0.81      1064\n          10       0.95      0.90      0.92      1064\n\n   micro avg       0.89      0.87      0.88     11702\n   macro avg       0.89      0.87      0.88     11702\nweighted avg       0.89      0.87      0.88     11702\n samples avg       0.84      0.87      0.85     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.16241861081194287\nEpoch 19 loss is 0.142375697260239\nEpoch 29 loss is 0.15385840834181844\nEpoch 39 loss is 0.2138264016307598\nEpoch 49 loss is 0.2365240586587247\nEpoch 59 loss is 0.2929398953187324\nEpoch 69 loss is 0.2761381170166951\nEpoch 79 loss is 0.3456885180336249\nEpoch 89 loss is 0.34346247018042997\nEpoch 99 loss is 0.3458485532239313\nEpoch 109 loss is 0.3444044925757802\nEpoch 119 loss is 0.3440097919960899\nEpoch 129 loss is 0.34428306482555965\nEpoch 139 loss is 0.3449518206799149\nEpoch 149 loss is 0.3461375016053481\nEpoch 159 loss is 0.3476872522585145\nEpoch 169 loss is 0.34796445004321847\nEpoch 179 loss is 0.3482418109346652\nEpoch 189 loss is 0.34871055688690134\nEpoch 199 loss is 0.3490575960919955\nTrain Acc.:  0.8840558036191168\n              precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94      1063\n           1       0.92      0.90      0.91      1064\n           2       0.97      0.93      0.95      1064\n           3       0.96      0.96      0.96      1063\n           4       0.93      0.78      0.85      1064\n           5       0.94      0.90      0.92      1064\n           6       0.90      0.77      0.83      1064\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.85      0.87      0.86      1064\n          10       0.94      0.92      0.93      1064\n\n   micro avg       0.94      0.91      0.92     11702\n   macro avg       0.94      0.91      0.92     11702\nweighted avg       0.94      0.91      0.92     11702\n samples avg       0.89      0.91      0.90     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.15193523247470855\nEpoch 19 loss is 0.1408373010379282\nEpoch 29 loss is 0.18472082419796915\nEpoch 39 loss is 0.1959035029241411\nEpoch 49 loss is 0.21619177741693082\nEpoch 59 loss is 0.21511564006011144\nEpoch 69 loss is 0.20399205995454375\nEpoch 79 loss is 0.20022756204760384\nEpoch 89 loss is 0.20033638712455112\nEpoch 99 loss is 0.19893451606025606\nEpoch 109 loss is 0.19786037468191467\nEpoch 119 loss is 0.19890569036312059\nEpoch 129 loss is 0.1990134917790664\nEpoch 139 loss is 0.19872859569206974\nEpoch 149 loss is 0.19751614804725498\nEpoch 159 loss is 0.19739665962479105\nEpoch 169 loss is 0.19784372444886006\nEpoch 179 loss is 0.19111926939075988\nEpoch 189 loss is 0.19073564886846614\nEpoch 199 loss is 0.18976225840289762\nTrain Acc.:  0.8771764906958361\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95      1064\n           1       0.89      0.79      0.84      1064\n           2       0.97      0.93      0.95      1064\n           3       0.99      0.94      0.96      1064\n           4       0.90      0.85      0.88      1064\n           5       0.87      0.89      0.88      1064\n           6       0.89      0.77      0.83      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.86      0.87      0.87      1064\n          10       0.97      0.90      0.93      1064\n\n   micro avg       0.93      0.90      0.92     11702\n   macro avg       0.93      0.90      0.92     11702\nweighted avg       0.93      0.90      0.92     11702\n samples avg       0.89      0.90      0.89     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=0e2885b9c6934f79849153fe8c7149d5\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/0e2885b9c6934f79849153fe8c7149d5/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 14:56:58,474 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.11677723785531834\nEpoch 19 loss is 0.08727784719556735\nEpoch 29 loss is 0.09157292581128637\nEpoch 39 loss is 0.07421681373324716\nEpoch 49 loss is 0.07697537707769428\nEpoch 59 loss is 0.06723158685548768\nEpoch 69 loss is 0.06324904770714807\nEpoch 79 loss is 0.0635766536591303\nEpoch 89 loss is 0.0689652033514836\nEpoch 99 loss is 0.09650615274908997\nEpoch 109 loss is 0.07453832805905608\nEpoch 119 loss is 0.07929171747710538\nEpoch 129 loss is 0.07981405980555767\nEpoch 139 loss is 0.09314429308579714\nEpoch 149 loss is 0.1007599468837455\nEpoch 159 loss is 0.1594258365172202\nEpoch 169 loss is 0.16438126926324473\nEpoch 179 loss is 0.1535026421169871\nEpoch 189 loss is 0.15247239398711698\nEpoch 199 loss is 0.1501105166766776\nTrain Acc.:  0.9464609994231632\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97      1063\n           1       0.95      0.92      0.93      1064\n           2       0.98      0.99      0.99      1064\n           3       0.98      0.98      0.98      1064\n           4       0.96      0.91      0.94      1064\n           5       0.94      0.93      0.93      1063\n           6       0.94      0.91      0.92      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.96      0.93      0.94      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.97      0.95      0.96     11702\n   macro avg       0.97      0.95      0.96     11702\nweighted avg       0.97      0.95      0.96     11702\n samples avg       0.95      0.95      0.95     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.10269231647718571\nEpoch 19 loss is 0.0828424365293163\nEpoch 29 loss is 0.0755315495091804\nEpoch 39 loss is 0.07214960396708685\nEpoch 49 loss is 0.06101909084585404\nEpoch 59 loss is 0.06839280821097835\nEpoch 69 loss is 0.0599125453507835\nEpoch 79 loss is 0.07335060013207284\nEpoch 89 loss is 0.07783843407293703\nEpoch 99 loss is 0.07579400460243982\nEpoch 109 loss is 0.08774453313883374\nEpoch 119 loss is 0.07845111177806056\nEpoch 129 loss is 0.08304869556239905\nEpoch 139 loss is 0.09663094311768118\nEpoch 149 loss is 0.09574537896135903\nEpoch 159 loss is 0.10745339520496501\nEpoch 169 loss is 0.12221249713581007\nEpoch 179 loss is 0.1238504916702643\nEpoch 189 loss is 0.12257097729943099\nEpoch 199 loss is 0.12221594344523189\nTrain Acc.:  0.9447732176811161\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1063\n           1       0.96      0.94      0.95      1064\n           2       0.98      0.98      0.98      1064\n           3       0.99      0.97      0.98      1063\n           4       0.92      0.90      0.91      1064\n           5       0.94      0.93      0.93      1064\n           6       0.94      0.92      0.93      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.96      1064\n          10       0.99      0.96      0.98      1064\n\n   micro avg       0.97      0.96      0.96     11702\n   macro avg       0.97      0.96      0.96     11702\nweighted avg       0.97      0.96      0.96     11702\n samples avg       0.95      0.96      0.95     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.11407119840787741\nEpoch 19 loss is 0.0782730350170469\nEpoch 29 loss is 0.07717022981852589\nEpoch 39 loss is 0.07464963948878549\nEpoch 49 loss is 0.07757185254979934\nEpoch 59 loss is 0.06594024636591442\nEpoch 69 loss is 0.06721012903498128\nEpoch 79 loss is 0.06679680268106901\nEpoch 89 loss is 0.06374956514109469\nEpoch 99 loss is 0.06762114545661016\nEpoch 109 loss is 0.06522387791331746\nEpoch 119 loss is 0.0730944025786924\nEpoch 129 loss is 0.07782279538990577\nEpoch 139 loss is 0.08467630408643206\nEpoch 149 loss is 0.09103032341738909\nEpoch 159 loss is 0.09966637200152119\nEpoch 169 loss is 0.09320824764012735\nEpoch 179 loss is 0.12129874941876952\nEpoch 189 loss is 0.14117765266074325\nEpoch 199 loss is 0.1418275397130545\nTrain Acc.:  0.9381716409938684\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      1064\n           1       0.94      0.92      0.93      1064\n           2       0.99      0.96      0.98      1064\n           3       0.98      0.97      0.97      1063\n           4       0.92      0.88      0.90      1064\n           5       0.94      0.95      0.95      1064\n           6       0.91      0.88      0.89      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.93      0.92      0.92      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.96      0.95      0.95     11702\n   macro avg       0.96      0.95      0.95     11702\nweighted avg       0.96      0.95      0.95     11702\n samples avg       0.94      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.11413860654953449\nEpoch 19 loss is 0.09837500329198573\nEpoch 29 loss is 0.09358380064606914\nEpoch 39 loss is 0.09880923285039081\nEpoch 49 loss is 0.0952057750942828\nEpoch 59 loss is 0.08008669991948515\nEpoch 69 loss is 0.08619386331155282\nEpoch 79 loss is 0.07746320343579995\nEpoch 89 loss is 0.0841977986411612\nEpoch 99 loss is 0.08913268603148011\nEpoch 109 loss is 0.08841460231699025\nEpoch 119 loss is 0.08445271805732564\nEpoch 129 loss is 0.09042461018240787\nEpoch 139 loss is 0.08969012531848465\nEpoch 149 loss is 0.10162866889312593\nEpoch 159 loss is 0.09608768953639517\nEpoch 169 loss is 0.11210143946301826\nEpoch 179 loss is 0.11975461444394792\nEpoch 189 loss is 0.11591744085113626\nEpoch 199 loss is 0.1207325082889982\nTrain Acc.:  0.92321661289978\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.96      1063\n           1       0.93      0.93      0.93      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.96      0.97      1063\n           4       0.92      0.86      0.89      1064\n           5       0.96      0.94      0.95      1064\n           6       0.90      0.83      0.86      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.92      0.92      0.92      1064\n          10       0.97      0.96      0.96      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.10585059616812989\nEpoch 19 loss is 0.08375488261089946\nEpoch 29 loss is 0.0842819689311398\nEpoch 39 loss is 0.07327244133203635\nEpoch 49 loss is 0.06713747510860289\nEpoch 59 loss is 0.06443358422771639\nEpoch 69 loss is 0.061379027698900954\nEpoch 79 loss is 0.06024192039466105\nEpoch 89 loss is 0.06170128209518335\nEpoch 99 loss is 0.05731691560047303\nEpoch 109 loss is 0.06712788845175123\nEpoch 119 loss is 0.06511834431841233\nEpoch 129 loss is 0.06352879035121482\nEpoch 139 loss is 0.06082366798673906\nEpoch 149 loss is 0.07462425566574347\nEpoch 159 loss is 0.07087942434442313\nEpoch 169 loss is 0.09351090855570425\nEpoch 179 loss is 0.08463663381504655\nEpoch 189 loss is 0.09110312651864295\nEpoch 199 loss is 0.09962031710031462\nTrain Acc.:  0.951139786784028\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1064\n           1       0.96      0.91      0.93      1064\n           2       0.98      0.98      0.98      1064\n           3       0.98      0.97      0.97      1064\n           4       0.95      0.92      0.94      1064\n           5       0.94      0.95      0.95      1064\n           6       0.93      0.90      0.91      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.96      0.94      0.95      1064\n          10       0.97      0.97      0.97      1064\n\n   micro avg       0.97      0.96      0.96     11702\n   macro avg       0.97      0.96      0.96     11702\nweighted avg       0.97      0.96      0.96     11702\n samples avg       0.95      0.96      0.95     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=02f388d5c7e34bd6b473b117ea6509ff\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/02f388d5c7e34bd6b473b117ea6509ff/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 15:16:17,894 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.08736984969525521\nEpoch 19 loss is 0.07819826650816623\nEpoch 29 loss is 0.06782916854065665\nEpoch 39 loss is 0.04816816897126411\nEpoch 49 loss is 0.05163560574118214\nEpoch 59 loss is 0.04627741363342768\nEpoch 69 loss is 0.04440861685817132\nEpoch 79 loss is 0.03960527436632251\nEpoch 89 loss is 0.040994377833916226\nEpoch 99 loss is 0.03771855142496925\nEpoch 109 loss is 0.037066781398096174\nEpoch 119 loss is 0.03917588771049437\nEpoch 129 loss is 0.03853848459391027\nEpoch 139 loss is 0.039490334296889244\nEpoch 149 loss is 0.03568498748548023\nEpoch 159 loss is 0.037056578648648794\nEpoch 169 loss is 0.03705010800859619\nEpoch 179 loss is 0.03907089468945371\nEpoch 189 loss is 0.0396113535151825\nEpoch 199 loss is 0.04512131274939002\nTrain Acc.:  0.9723759266776337\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.95      0.95      0.95      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.98      0.99      1064\n           4       0.97      0.95      0.96      1064\n           5       0.96      0.94      0.95      1063\n           6       0.96      0.95      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.95      0.96      1064\n          10       0.98      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.10348894731392981\nEpoch 19 loss is 0.06993990700555251\nEpoch 29 loss is 0.06444708635591645\nEpoch 39 loss is 0.06056610582579465\nEpoch 49 loss is 0.04929898078856764\nEpoch 59 loss is 0.05208108105377289\nEpoch 69 loss is 0.055182737210477444\nEpoch 79 loss is 0.053781560456890025\nEpoch 89 loss is 0.041558242582498066\nEpoch 99 loss is 0.04481064647571699\nEpoch 109 loss is 0.04177463603363158\nEpoch 119 loss is 0.03897786006229341\nEpoch 129 loss is 0.042479025262170365\nEpoch 139 loss is 0.04130484690661847\nEpoch 149 loss is 0.043655684630288405\nEpoch 159 loss is 0.044328333606525876\nEpoch 169 loss is 0.04163741938029155\nEpoch 179 loss is 0.03787117794917678\nEpoch 189 loss is 0.04063373131211509\nEpoch 199 loss is 0.045135088320266366\nTrain Acc.:  0.9656247997094451\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.98      0.99      1064\n           3       0.98      0.97      0.98      1063\n           4       0.94      0.94      0.94      1064\n           5       0.96      0.97      0.96      1064\n           6       0.95      0.95      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.93      0.95      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.97      0.97      0.97     11702\n   macro avg       0.97      0.97      0.97     11702\nweighted avg       0.97      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09721067817250643\nEpoch 19 loss is 0.0659285953883801\nEpoch 29 loss is 0.05613079095748313\nEpoch 39 loss is 0.053254430131788696\nEpoch 49 loss is 0.050631655931046286\nEpoch 59 loss is 0.043587857389422176\nEpoch 69 loss is 0.04570086010022542\nEpoch 79 loss is 0.039479428673811066\nEpoch 89 loss is 0.03776955057557153\nEpoch 99 loss is 0.04120841258334559\nEpoch 109 loss is 0.05813870002363725\nEpoch 119 loss is 0.04555968065778827\nEpoch 129 loss is 0.03607066082124637\nEpoch 139 loss is 0.03758934199215175\nEpoch 149 loss is 0.033209230217276724\nEpoch 159 loss is 0.02871578698680781\nEpoch 169 loss is 0.029964328920410958\nEpoch 179 loss is 0.03051551401962286\nEpoch 189 loss is 0.031670732550214034\nEpoch 199 loss is 0.03487335515855483\nTrain Acc.:  0.9769478924092551\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1064\n           1       0.97      0.97      0.97      1064\n           2       0.99      0.98      0.99      1064\n           3       0.98      0.98      0.98      1063\n           4       0.97      0.96      0.97      1064\n           5       0.97      0.97      0.97      1064\n           6       0.96      0.94      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.95      0.96      1064\n          10       0.98      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09506221192044154\nEpoch 19 loss is 0.06567186224064278\nEpoch 29 loss is 0.05404001884322595\nEpoch 39 loss is 0.05249845497810033\nEpoch 49 loss is 0.04073885316623995\nEpoch 59 loss is 0.040838370071218995\nEpoch 69 loss is 0.03615846274442404\nEpoch 79 loss is 0.030689715662826193\nEpoch 89 loss is 0.03348275354964435\nEpoch 99 loss is 0.04012077295457501\nEpoch 109 loss is 0.034105905516408\nEpoch 119 loss is 0.032124319295064915\nEpoch 129 loss is 0.03206602824675486\nEpoch 139 loss is 0.0336496624931189\nEpoch 149 loss is 0.030495164592908134\nEpoch 159 loss is 0.0336733646851268\nEpoch 169 loss is 0.03706312464641834\nEpoch 179 loss is 0.034242405249922364\nEpoch 189 loss is 0.03487617796112035\nEpoch 199 loss is 0.031548788032238144\nTrain Acc.:  0.9773965432520777\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.99      1063\n           1       0.95      0.96      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.98      0.98      1063\n           4       0.97      0.97      0.97      1064\n           5       0.96      0.94      0.95      1064\n           6       0.96      0.95      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.98      0.97      0.97      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09597225538158832\nEpoch 19 loss is 0.07305103076821956\nEpoch 29 loss is 0.06610023701716355\nEpoch 39 loss is 0.058439643653002006\nEpoch 49 loss is 0.047609421255355604\nEpoch 59 loss is 0.050110073609477744\nEpoch 69 loss is 0.044684631060736904\nEpoch 79 loss is 0.041153442952646245\nEpoch 89 loss is 0.03982352230327579\nEpoch 99 loss is 0.036985448655858864\nEpoch 109 loss is 0.04060346388820211\nEpoch 119 loss is 0.0447469210668093\nEpoch 129 loss is 0.03394044590851765\nEpoch 139 loss is 0.03358597226108189\nEpoch 149 loss is 0.027238770870941995\nEpoch 159 loss is 0.029254705460162258\nEpoch 169 loss is 0.05193828537228054\nEpoch 179 loss is 0.030730149318143475\nEpoch 189 loss is 0.03086909795063045\nEpoch 199 loss is 0.03218570903393944\nTrain Acc.:  0.9781229303309334\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1064\n           1       0.95      0.97      0.96      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.98      0.99      1064\n           4       0.97      0.96      0.97      1064\n           5       0.97      0.96      0.97      1064\n           6       0.96      0.95      0.96      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.98      0.97      0.97      1064\n          10       0.98      0.98      0.98      1064\n\n   micro avg       0.98      0.98      0.98     11702\n   macro avg       0.98      0.98      0.98     11702\nweighted avg       0.98      0.98      0.98     11702\n samples avg       0.97      0.98      0.97     11702\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip.html#multi-layer",
    "href": "examples/autass/autass_multiple_run_angle_clip.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=d09b15b39d9d46629a9dd600951fbd03\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d09b15b39d9d46629a9dd600951fbd03/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 15:46:41,689 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.36936323919586267\nEpoch 19 loss is 0.4417360333030594\nEpoch 29 loss is 0.4639713755873071\nEpoch 39 loss is 0.47806268155361914\nEpoch 49 loss is 0.46751608711861314\nEpoch 59 loss is 0.4455429524911341\nEpoch 69 loss is 0.44326834924033726\nEpoch 79 loss is 0.4069394754230474\nEpoch 89 loss is 0.41501924604142476\nEpoch 99 loss is 0.4060409507954031\nEpoch 109 loss is 0.4149785299989848\nEpoch 119 loss is 0.4198482804118088\nEpoch 129 loss is 0.4451440663334436\nEpoch 139 loss is 0.48754778360762985\nEpoch 149 loss is 0.512868387767623\nEpoch 159 loss is 0.4912468525778975\nEpoch 169 loss is 0.4819266422257403\nEpoch 179 loss is 0.503401190978166\nEpoch 189 loss is 0.475582148562929\nEpoch 199 loss is 0.47280443258388355\nTrain Acc.:  0.6744931313692396\n              precision    recall  f1-score   support\n\n           0       0.86      0.75      0.80      1063\n           1       0.85      0.75      0.80      1064\n           2       0.81      0.84      0.82      1064\n           3       0.85      0.79      0.82      1064\n           4       0.67      0.40      0.50      1064\n           5       0.77      0.86      0.82      1063\n           6       0.67      0.43      0.53      1064\n           7       0.98      0.96      0.97      1064\n           8       1.00      1.00      1.00      1064\n           9       0.68      0.41      0.51      1064\n          10       0.81      0.86      0.83      1064\n\n   micro avg       0.83      0.73      0.78     11702\n   macro avg       0.81      0.73      0.76     11702\nweighted avg       0.81      0.73      0.76     11702\n samples avg       0.70      0.73      0.71     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3120843412146201\nEpoch 19 loss is 0.338843757118815\nEpoch 29 loss is 0.29489357968814983\nEpoch 39 loss is 0.3142865816273397\nEpoch 49 loss is 0.33635124551797807\nEpoch 59 loss is 0.37067740493614026\nEpoch 69 loss is 0.336927943126264\nEpoch 79 loss is 0.30800579708110587\nEpoch 89 loss is 0.30535664010364083\nEpoch 99 loss is 0.2919258725294692\nEpoch 109 loss is 0.2816264724044177\nEpoch 119 loss is 0.310759231961488\nEpoch 129 loss is 0.31188458260685364\nEpoch 139 loss is 0.3267504583368058\nEpoch 149 loss is 0.32820414042444124\nEpoch 159 loss is 0.3265853992645182\nEpoch 169 loss is 0.30502072256656815\nEpoch 179 loss is 0.31455467220775174\nEpoch 189 loss is 0.2898721081722409\nEpoch 199 loss is 0.31343407820932145\nTrain Acc.:  0.6747922319311214\n              precision    recall  f1-score   support\n\n           0       0.91      0.87      0.89      1063\n           1       0.86      0.73      0.79      1064\n           2       0.92      0.57      0.70      1064\n           3       0.88      0.81      0.85      1063\n           4       0.72      0.70      0.71      1064\n           5       0.83      0.80      0.81      1064\n           6       0.39      0.97      0.56      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.83      0.83      0.83      1064\n          10       0.77      0.78      0.77      1064\n\n   micro avg       0.77      0.82      0.80     11702\n   macro avg       0.83      0.82      0.81     11702\nweighted avg       0.83      0.82      0.81     11702\n samples avg       0.75      0.82      0.77     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5730218218958721\nEpoch 19 loss is 0.5197506032800867\nEpoch 29 loss is 0.5308330161877794\nEpoch 39 loss is 0.5457173549411902\nEpoch 49 loss is 0.4674426359824832\nEpoch 59 loss is 0.48413304547490904\nEpoch 69 loss is 0.5169540288793292\nEpoch 79 loss is 0.46700861763673146\nEpoch 89 loss is 0.42243541114495425\nEpoch 99 loss is 0.441465108668188\nEpoch 109 loss is 0.420622401289878\nEpoch 119 loss is 0.48287951270030366\nEpoch 129 loss is 0.4898719847934528\nEpoch 139 loss is 0.511257985603324\nEpoch 149 loss is 0.5320171382337141\nEpoch 159 loss is 0.482352293969556\nEpoch 169 loss is 0.5216527805105815\nEpoch 179 loss is 0.5349811485542204\nEpoch 189 loss is 0.5128652697548396\nEpoch 199 loss is 0.5348841235805307\nTrain Acc.:  0.4734120964812955\n              precision    recall  f1-score   support\n\n           0       0.47      0.97      0.63      1064\n           1       0.52      0.46      0.49      1064\n           2       0.74      0.67      0.71      1064\n           3       0.74      0.56      0.63      1063\n           4       0.55      0.25      0.34      1064\n           5       0.68      0.79      0.73      1064\n           6       0.31      0.02      0.03      1063\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.45      0.01      0.02      1064\n          10       0.83      0.63      0.72      1064\n\n   micro avg       0.70      0.58      0.63     11702\n   macro avg       0.66      0.58      0.57     11702\nweighted avg       0.66      0.58      0.57     11702\n samples avg       0.52      0.58      0.54     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.4130547185457221\nEpoch 19 loss is 0.28443910819247403\nEpoch 29 loss is 0.2840811952740083\nEpoch 39 loss is 0.3201059445456724\nEpoch 49 loss is 0.28015486618309876\nEpoch 59 loss is 0.31602913086881074\nEpoch 69 loss is 0.27090680930739386\nEpoch 79 loss is 0.2440252615249398\nEpoch 89 loss is 0.2710016949262824\nEpoch 99 loss is 0.27676315756716174\nEpoch 109 loss is 0.26114457901684446\nEpoch 119 loss is 0.3384359963182098\nEpoch 129 loss is 0.3536610052551853\nEpoch 139 loss is 0.32877881416825233\nEpoch 149 loss is 0.3546631938786416\nEpoch 159 loss is 0.3654947728190782\nEpoch 169 loss is 0.34189606918074955\nEpoch 179 loss is 0.34633550078607206\nEpoch 189 loss is 0.3490021138615275\nEpoch 199 loss is 0.3347815447852944\nTrain Acc.:  0.7526438353237763\n              precision    recall  f1-score   support\n\n           0       0.73      0.98      0.84      1063\n           1       0.90      0.66      0.76      1064\n           2       0.93      0.83      0.88      1064\n           3       0.94      0.82      0.87      1063\n           4       0.66      0.60      0.63      1064\n           5       0.80      0.89      0.84      1064\n           6       0.89      0.43      0.58      1064\n           7       1.00      0.97      0.98      1064\n           8       1.00      0.97      0.99      1064\n           9       0.75      0.94      0.83      1064\n          10       0.84      0.71      0.77      1064\n\n   micro avg       0.85      0.80      0.82     11702\n   macro avg       0.86      0.80      0.81     11702\nweighted avg       0.86      0.80      0.81     11702\n samples avg       0.78      0.80      0.78     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28021247108809677\nEpoch 19 loss is 0.24448510212480346\nEpoch 29 loss is 0.25997805330578505\nEpoch 39 loss is 0.2997642359869558\nEpoch 49 loss is 0.284409333111343\nEpoch 59 loss is 0.29594549671485526\nEpoch 69 loss is 0.27488304189669654\nEpoch 79 loss is 0.3148477242705397\nEpoch 89 loss is 0.30100965217864945\nEpoch 99 loss is 0.34245337392046377\nEpoch 109 loss is 0.3656904773209293\nEpoch 119 loss is 0.3708399227898303\nEpoch 129 loss is 0.3596192533165072\nEpoch 139 loss is 0.321385041998874\nEpoch 149 loss is 0.3122428807215139\nEpoch 159 loss is 0.33298053660486926\nEpoch 169 loss is 0.2991883369131804\nEpoch 179 loss is 0.31971790791167104\nEpoch 189 loss is 0.299987522555066\nEpoch 199 loss is 0.31500240064659873\nTrain Acc.:  0.7926378533125388\n              precision    recall  f1-score   support\n\n           0       0.86      0.89      0.88      1064\n           1       0.87      0.85      0.86      1064\n           2       0.92      0.93      0.92      1064\n           3       0.90      0.89      0.90      1064\n           4       0.77      0.46      0.58      1064\n           5       0.78      0.91      0.84      1064\n           6       0.77      0.76      0.77      1063\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1063\n           9       0.85      0.85      0.85      1064\n          10       0.87      0.75      0.81      1064\n\n   micro avg       0.87      0.84      0.86     11702\n   macro avg       0.87      0.84      0.85     11702\nweighted avg       0.87      0.84      0.85     11702\n samples avg       0.82      0.84      0.83     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=1ffe99f9f8014fe59562e6e70ac4e4ac\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1ffe99f9f8014fe59562e6e70ac4e4ac/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 16:04:21,821 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.30027549864319536\nEpoch 19 loss is 0.2913620808041314\nEpoch 29 loss is 0.29192267719386994\nEpoch 39 loss is 0.29982285425986316\nEpoch 49 loss is 0.2991453145584028\nEpoch 59 loss is 0.2806252701793078\nEpoch 69 loss is 0.29399948094396766\nEpoch 79 loss is 0.2848964264304165\nEpoch 89 loss is 0.2640974185080014\nEpoch 99 loss is 0.2781391938018297\nEpoch 109 loss is 0.29882929257076446\nEpoch 119 loss is 0.2896457279584684\nEpoch 129 loss is 0.30861868284987515\nEpoch 139 loss is 0.30446866634588193\nEpoch 149 loss is 0.3117817060575804\nEpoch 159 loss is 0.2748762158827661\nEpoch 169 loss is 0.2728797345000005\nEpoch 179 loss is 0.2798542257381176\nEpoch 189 loss is 0.282661688657906\nEpoch 199 loss is 0.30204731380523975\nTrain Acc.:  0.7394193176234324\n              precision    recall  f1-score   support\n\n           0       0.92      0.84      0.88      1063\n           1       0.75      0.74      0.75      1064\n           2       0.93      0.75      0.83      1064\n           3       0.89      0.88      0.88      1064\n           4       0.84      0.62      0.72      1064\n           5       0.79      0.83      0.81      1063\n           6       0.62      0.66      0.64      1064\n           7       0.99      1.00      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.81      0.67      0.73      1064\n          10       0.85      0.83      0.84      1064\n\n   micro avg       0.85      0.80      0.83     11702\n   macro avg       0.85      0.80      0.82     11702\nweighted avg       0.85      0.80      0.82     11702\n samples avg       0.77      0.80      0.78     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3553981755313085\nEpoch 19 loss is 0.3506370108940286\nEpoch 29 loss is 0.30082422517534047\nEpoch 39 loss is 0.30063860033973894\nEpoch 49 loss is 0.25131238597560024\nEpoch 59 loss is 0.23531957531915973\nEpoch 69 loss is 0.2380401779400625\nEpoch 79 loss is 0.2335633736232218\nEpoch 89 loss is 0.23700447810534878\nEpoch 99 loss is 0.21088957250172105\nEpoch 109 loss is 0.23098805169700698\nEpoch 119 loss is 0.2506731403676874\nEpoch 129 loss is 0.24334308375596303\nEpoch 139 loss is 0.25569175507335257\nEpoch 149 loss is 0.2508974958920864\nEpoch 159 loss is 0.23111012958349472\nEpoch 169 loss is 0.22934085708672713\nEpoch 179 loss is 0.23431740304916213\nEpoch 189 loss is 0.2142045345957332\nEpoch 199 loss is 0.220009767135271\nTrain Acc.:  0.7865703847715085\n              precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      1063\n           1       0.86      0.81      0.83      1064\n           2       0.96      0.91      0.94      1064\n           3       0.95      0.84      0.90      1063\n           4       0.88      0.50      0.63      1064\n           5       0.80      0.93      0.86      1064\n           6       0.61      0.88      0.72      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.87      0.80      0.83      1064\n          10       0.90      0.81      0.85      1064\n\n   micro avg       0.88      0.85      0.87     11702\n   macro avg       0.89      0.85      0.86     11702\nweighted avg       0.89      0.85      0.86     11702\n samples avg       0.82      0.85      0.83     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.25758739764160093\nEpoch 19 loss is 0.266251132975017\nEpoch 29 loss is 0.2533447710924989\nEpoch 39 loss is 0.24690942194464932\nEpoch 49 loss is 0.231170000858992\nEpoch 59 loss is 0.22355611411128506\nEpoch 69 loss is 0.21875239712216873\nEpoch 79 loss is 0.22903089863599008\nEpoch 89 loss is 0.2373035179400735\nEpoch 99 loss is 0.23382820577734512\nEpoch 109 loss is 0.20544294051074055\nEpoch 119 loss is 0.20787382558825876\nEpoch 129 loss is 0.20653071272151727\nEpoch 139 loss is 0.22629611145400377\nEpoch 149 loss is 0.2440747135803093\nEpoch 159 loss is 0.22601217778289143\nEpoch 169 loss is 0.2584220808089135\nEpoch 179 loss is 0.27634828629385144\nEpoch 189 loss is 0.2721481887397433\nEpoch 199 loss is 0.2826596067736297\nTrain Acc.:  0.8100070502275301\n              precision    recall  f1-score   support\n\n           0       0.85      0.95      0.90      1064\n           1       0.86      0.81      0.84      1064\n           2       0.90      0.90      0.90      1064\n           3       0.96      0.94      0.95      1063\n           4       0.82      0.49      0.62      1064\n           5       0.85      0.84      0.85      1064\n           6       0.86      0.58      0.69      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.80      0.94      0.86      1064\n          10       0.90      0.77      0.83      1064\n\n   micro avg       0.89      0.84      0.86     11702\n   macro avg       0.89      0.84      0.86     11702\nweighted avg       0.89      0.84      0.86     11702\n samples avg       0.82      0.84      0.83     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28775937804643187\nEpoch 19 loss is 0.23007369599769445\nEpoch 29 loss is 0.25889260081970894\nEpoch 39 loss is 0.25902661986905157\nEpoch 49 loss is 0.2292498016389515\nEpoch 59 loss is 0.22306006992961439\nEpoch 69 loss is 0.1976271504421206\nEpoch 79 loss is 0.20343043020192228\nEpoch 89 loss is 0.21934100686580518\nEpoch 99 loss is 0.22369418242070047\nEpoch 109 loss is 0.21545978081993952\nEpoch 119 loss is 0.21148563517815785\nEpoch 129 loss is 0.21415086358216098\nEpoch 139 loss is 0.23069838988169805\nEpoch 149 loss is 0.2332487083323042\nEpoch 159 loss is 0.2051832755068869\nEpoch 169 loss is 0.22123131840723095\nEpoch 179 loss is 0.20724936091666757\nEpoch 189 loss is 0.1936030315379404\nEpoch 199 loss is 0.2227826737601992\nTrain Acc.:  0.8283376418057128\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90      1063\n           1       0.82      0.90      0.86      1064\n           2       0.95      0.92      0.93      1064\n           3       0.92      0.96      0.94      1063\n           4       0.81      0.73      0.77      1064\n           5       0.90      0.81      0.85      1064\n           6       0.89      0.43      0.58      1064\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.78      0.96      0.86      1064\n          10       0.82      0.88      0.85      1064\n\n   micro avg       0.89      0.86      0.88     11702\n   macro avg       0.89      0.86      0.87     11702\nweighted avg       0.89      0.86      0.87     11702\n samples avg       0.85      0.86      0.85     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.16656616157130508\nEpoch 19 loss is 0.1656779172497797\nEpoch 29 loss is 0.22550141041854313\nEpoch 39 loss is 0.20679155828053714\nEpoch 49 loss is 0.21150408955747108\nEpoch 59 loss is 0.23130154015187063\nEpoch 69 loss is 0.2393321476323322\nEpoch 79 loss is 0.22455397225477222\nEpoch 89 loss is 0.25369244062104573\nEpoch 99 loss is 0.2299914278176482\nEpoch 109 loss is 0.19713104532598436\nEpoch 119 loss is 0.196544944395429\nEpoch 129 loss is 0.17342359035545055\nEpoch 139 loss is 0.18163042589732023\nEpoch 149 loss is 0.20009191348732322\nEpoch 159 loss is 0.18232177900944924\nEpoch 169 loss is 0.1977332400942652\nEpoch 179 loss is 0.1948407359503548\nEpoch 189 loss is 0.2035039176614566\nEpoch 199 loss is 0.1989608963906182\nTrain Acc.:  0.8589099920951995\n              precision    recall  f1-score   support\n\n           0       0.92      0.96      0.94      1064\n           1       0.88      0.77      0.82      1064\n           2       0.89      0.95      0.92      1064\n           3       0.98      0.96      0.97      1064\n           4       0.85      0.80      0.82      1064\n           5       0.77      0.93      0.84      1064\n           6       0.90      0.78      0.84      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.87      0.89      0.88      1064\n          10       0.93      0.83      0.88      1064\n\n   micro avg       0.91      0.90      0.90     11702\n   macro avg       0.91      0.90      0.90     11702\nweighted avg       0.91      0.90      0.90     11702\n samples avg       0.88      0.90      0.88     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=66b39d66d7654fa1ad879e1cd8e1a4ce\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/66b39d66d7654fa1ad879e1cd8e1a4ce/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 16:23:01,246 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.1499046038395931\nEpoch 19 loss is 0.1297322766785629\nEpoch 29 loss is 0.13019643237124603\nEpoch 39 loss is 0.12789428760818733\nEpoch 49 loss is 0.11103197831648269\nEpoch 59 loss is 0.11020350228682631\nEpoch 69 loss is 0.11147977013764811\nEpoch 79 loss is 0.10846879683535147\nEpoch 89 loss is 0.12197619828830582\nEpoch 99 loss is 0.11589422750814764\nEpoch 109 loss is 0.12233447725951128\nEpoch 119 loss is 0.11728783170849341\nEpoch 129 loss is 0.11600955988839287\nEpoch 139 loss is 0.10777309333314188\nEpoch 149 loss is 0.11286425886149831\nEpoch 159 loss is 0.10997786349122186\nEpoch 169 loss is 0.10343363161044343\nEpoch 179 loss is 0.10918495130351621\nEpoch 189 loss is 0.10805207773255567\nEpoch 199 loss is 0.1042582369224553\nTrain Acc.:  0.9155681842459461\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96      1063\n           1       0.92      0.89      0.90      1064\n           2       0.98      0.93      0.96      1064\n           3       0.96      0.97      0.97      1064\n           4       0.93      0.84      0.88      1064\n           5       0.90      0.91      0.91      1063\n           6       0.88      0.85      0.87      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.90      0.92      1064\n          10       0.94      0.94      0.94      1064\n\n   micro avg       0.95      0.93      0.94     11702\n   macro avg       0.95      0.93      0.94     11702\nweighted avg       0.95      0.93      0.94     11702\n samples avg       0.92      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.16444180091313235\nEpoch 19 loss is 0.13450935301533853\nEpoch 29 loss is 0.12395481547756365\nEpoch 39 loss is 0.11403245044394655\nEpoch 49 loss is 0.10585260064483076\nEpoch 59 loss is 0.12085044614478288\nEpoch 69 loss is 0.12072065235425643\nEpoch 79 loss is 0.12322402368413449\nEpoch 89 loss is 0.13057146826320154\nEpoch 99 loss is 0.1284516260088808\nEpoch 109 loss is 0.1310643245563845\nEpoch 119 loss is 0.15000504618170252\nEpoch 129 loss is 0.15256414701505086\nEpoch 139 loss is 0.13475608804617148\nEpoch 149 loss is 0.12786701654120483\nEpoch 159 loss is 0.12325916810132656\nEpoch 169 loss is 0.12089844310402888\nEpoch 179 loss is 0.123651544885496\nEpoch 189 loss is 0.12969231060336694\nEpoch 199 loss is 0.15106089017416613\nTrain Acc.:  0.912256713739398\n              precision    recall  f1-score   support\n\n           0       0.96      0.95      0.95      1063\n           1       0.91      0.86      0.88      1064\n           2       0.99      0.96      0.97      1064\n           3       0.97      0.95      0.96      1063\n           4       0.92      0.89      0.90      1064\n           5       0.89      0.92      0.91      1064\n           6       0.86      0.86      0.86      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.90      0.90      0.90      1064\n          10       0.97      0.94      0.96      1064\n\n   micro avg       0.94      0.93      0.94     11702\n   macro avg       0.94      0.93      0.94     11702\nweighted avg       0.94      0.93      0.94     11702\n samples avg       0.92      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13140804343389728\nEpoch 19 loss is 0.11568335987799334\nEpoch 29 loss is 0.10454780419932769\nEpoch 39 loss is 0.10599905002509148\nEpoch 49 loss is 0.10790101651025631\nEpoch 59 loss is 0.10838345710268503\nEpoch 69 loss is 0.1039850904931613\nEpoch 79 loss is 0.10940135407457686\nEpoch 89 loss is 0.09866982557423697\nEpoch 99 loss is 0.10053318491512914\nEpoch 109 loss is 0.09320339956264681\nEpoch 119 loss is 0.10104087870858815\nEpoch 129 loss is 0.10796066144525626\nEpoch 139 loss is 0.10673830550703585\nEpoch 149 loss is 0.10363054506764556\nEpoch 159 loss is 0.10652278300728972\nEpoch 169 loss is 0.11524796425634735\nEpoch 179 loss is 0.1187950656937112\nEpoch 189 loss is 0.11531818662457277\nEpoch 199 loss is 0.10933889640581886\nTrain Acc.:  0.916145021043861\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95      1064\n           1       0.91      0.91      0.91      1064\n           2       1.00      0.98      0.99      1064\n           3       0.97      0.95      0.96      1063\n           4       0.87      0.84      0.86      1064\n           5       0.92      0.91      0.91      1064\n           6       0.86      0.88      0.87      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.88      0.93      0.90      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.94      0.93      0.94     11702\n   macro avg       0.94      0.93      0.94     11702\nweighted avg       0.94      0.93      0.94     11702\n samples avg       0.92      0.93      0.93     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.15382963120659784\nEpoch 19 loss is 0.10859517485079796\nEpoch 29 loss is 0.1039033885767897\nEpoch 39 loss is 0.10722984283457825\nEpoch 49 loss is 0.09191558609732126\nEpoch 59 loss is 0.10290934605597563\nEpoch 69 loss is 0.0952635279995302\nEpoch 79 loss is 0.0910337109495511\nEpoch 89 loss is 0.08792022893121838\nEpoch 99 loss is 0.09991178895421955\nEpoch 109 loss is 0.10163613127338143\nEpoch 119 loss is 0.10929553506790932\nEpoch 129 loss is 0.11517537118674706\nEpoch 139 loss is 0.10527693350535264\nEpoch 149 loss is 0.10425300772978957\nEpoch 159 loss is 0.10921202865336493\nEpoch 169 loss is 0.1063591963798746\nEpoch 179 loss is 0.09805513885860391\nEpoch 189 loss is 0.09630969560112046\nEpoch 199 loss is 0.11513547873782298\nTrain Acc.:  0.9290277095306257\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      1063\n           1       0.91      0.93      0.92      1064\n           2       0.99      0.98      0.99      1064\n           3       0.98      0.98      0.98      1063\n           4       0.92      0.92      0.92      1064\n           5       0.94      0.90      0.92      1064\n           6       0.88      0.82      0.85      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.91      0.95      0.93      1064\n          10       0.98      0.93      0.95      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.93      0.94      0.94     11702\n\n\n\n/tmp/ipykernel_10107/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13468610442686887\nEpoch 19 loss is 0.11356436751635222\nEpoch 29 loss is 0.10511056101321942\nEpoch 39 loss is 0.1061999656253616\nEpoch 49 loss is 0.10663321393686137\nEpoch 59 loss is 0.10928515764258159\nEpoch 69 loss is 0.11259141153569381\nEpoch 79 loss is 0.09192517041510463\nEpoch 89 loss is 0.09801434421544775\nEpoch 99 loss is 0.1031168064577551\nEpoch 109 loss is 0.09404308404121978\nEpoch 119 loss is 0.09329897373453444\nEpoch 129 loss is 0.09098583010381506\nEpoch 139 loss is 0.08366240511713254\nEpoch 149 loss is 0.07989489898176606\nEpoch 159 loss is 0.0764208793328456\nEpoch 169 loss is 0.0826377881570616\nEpoch 179 loss is 0.08229154744932386\nEpoch 189 loss is 0.09376729251789975\nEpoch 199 loss is 0.09184004778612281\nTrain Acc.:  0.9369752387463414\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      1064\n           1       0.96      0.89      0.92      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.98      0.99      1064\n           4       0.95      0.88      0.92      1064\n           5       0.89      0.96      0.92      1064\n           6       0.92      0.88      0.90      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.93      0.95      0.94      1064\n          10       0.96      0.96      0.96      1064\n\n   micro avg       0.96      0.95      0.95     11702\n   macro avg       0.96      0.95      0.95     11702\nweighted avg       0.96      0.95      0.95     11702\n samples avg       0.94      0.95      0.94     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=6782b5dd458348fd9dde2a4046e3d09b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6782b5dd458348fd9dde2a4046e3d09b/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_15152/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 20:10:34,834 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.13682720639443957\nEpoch 19 loss is 0.10102070636348458\nEpoch 29 loss is 0.08727009396955031\nEpoch 39 loss is 0.07671863965077798\nEpoch 49 loss is 0.06848380096450109\nEpoch 59 loss is 0.0635398093390423\nEpoch 69 loss is 0.0628619211282882\nEpoch 79 loss is 0.06982675314397956\nEpoch 89 loss is 0.06102933808512002\nEpoch 99 loss is 0.06000732133720239\nEpoch 109 loss is 0.060890425387314366\nEpoch 119 loss is 0.05301149732799192\nEpoch 129 loss is 0.0522579315878903\nEpoch 139 loss is 0.05275362570989821\nEpoch 149 loss is 0.04787591243938674\nEpoch 159 loss is 0.0456566457444692\nEpoch 169 loss is 0.04486260011549171\nEpoch 179 loss is 0.0447318709239749\nEpoch 189 loss is 0.04379179251941935\nEpoch 199 loss is 0.04489066304454168\nTrain Acc.:  0.9654752494285043\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.94      0.94      0.94      1064\n           2       0.99      0.97      0.98      1064\n           3       1.00      0.99      0.99      1064\n           4       0.96      0.93      0.94      1064\n           5       0.95      0.94      0.95      1063\n           6       0.95      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.94      0.95      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.97      0.96      0.97     11702\n   macro avg       0.97      0.96      0.97     11702\nweighted avg       0.97      0.96      0.97     11702\n samples avg       0.96      0.96      0.96     11702\n\n\n\n/tmp/ipykernel_15152/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.14070723384058115\nEpoch 19 loss is 0.09901262946414933\nEpoch 29 loss is 0.1163871561542634\nEpoch 39 loss is 0.0928615501126676\nEpoch 49 loss is 0.09462188473736881\nEpoch 59 loss is 0.09051752364025904\nEpoch 69 loss is 0.08058040002663806\nEpoch 79 loss is 0.07118650187737878\nEpoch 89 loss is 0.05725321689636519\nEpoch 99 loss is 0.05587193962924077\nEpoch 109 loss is 0.053161680819638193\nEpoch 119 loss is 0.052564399732753064\nEpoch 129 loss is 0.04833477461407704\nEpoch 139 loss is 0.046382782052641146\nEpoch 149 loss is 0.05173067099999342\nEpoch 159 loss is 0.04534002150442024\nEpoch 169 loss is 0.046009179389923965\nEpoch 179 loss is 0.04352159747685905\nEpoch 189 loss is 0.041239578438454445\nEpoch 199 loss is 0.040097920362887236\nTrain Acc.:  0.9692994637554212\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.94      0.93      0.94      1064\n           2       1.00      0.99      0.99      1064\n           3       0.99      0.98      0.99      1063\n           4       0.96      0.95      0.95      1064\n           5       0.96      0.94      0.95      1064\n           6       0.96      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.98      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_15152/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.1371787810722252\nEpoch 19 loss is 0.09978662945393381\nEpoch 29 loss is 0.08858285073706741\nEpoch 39 loss is 0.0944868673915956\nEpoch 49 loss is 0.0822929725506483\nEpoch 59 loss is 0.06799231767158967\nEpoch 69 loss is 0.06335458838516671\nEpoch 79 loss is 0.06147011599416563\nEpoch 89 loss is 0.06221068190118905\nEpoch 99 loss is 0.05993916873581913\nEpoch 109 loss is 0.054406843221859624\nEpoch 119 loss is 0.045617025331166426\nEpoch 129 loss is 0.046012246996210246\nEpoch 139 loss is 0.044311349952736896\nEpoch 149 loss is 0.04082695991627327\nEpoch 159 loss is 0.039679863425516555\nEpoch 169 loss is 0.040999900860178046\nEpoch 179 loss is 0.04099159740351322\nEpoch 189 loss is 0.03939206668245912\nEpoch 199 loss is 0.039607182730482776\nTrain Acc.:  0.9713718033627449\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1064\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.98      0.99      1064\n           3       1.00      0.99      0.99      1063\n           4       0.97      0.95      0.96      1064\n           5       0.96      0.95      0.95      1064\n           6       0.96      0.92      0.94      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      0.99      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_15152/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13303833512235225\nEpoch 19 loss is 0.11102950469889049\nEpoch 29 loss is 0.1033785956422809\nEpoch 39 loss is 0.10035753702045348\nEpoch 49 loss is 0.08026176993296501\nEpoch 59 loss is 0.08006638138825455\nEpoch 69 loss is 0.07315596454862489\nEpoch 79 loss is 0.06695363560418462\nEpoch 89 loss is 0.07812911516053243\nEpoch 99 loss is 0.07286186109129712\nEpoch 109 loss is 0.06823154445600114\nEpoch 119 loss is 0.0748763280247656\nEpoch 129 loss is 0.06759634080194282\nEpoch 139 loss is 0.06537460445723998\nEpoch 149 loss is 0.07214919456239974\nEpoch 159 loss is 0.06770210388031259\nEpoch 169 loss is 0.05968848252020946\nEpoch 179 loss is 0.0640390788375198\nEpoch 189 loss is 0.06062085278669487\nEpoch 199 loss is 0.053366963455588425\nTrain Acc.:  0.9539812421219048\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.93      0.89      0.91      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.99      0.99      1063\n           4       0.95      0.93      0.94      1064\n           5       0.94      0.93      0.94      1064\n           6       0.93      0.93      0.93      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.96      0.96      0.96      1064\n          10       0.99      0.96      0.98      1064\n\n   micro avg       0.97      0.96      0.96     11702\n   macro avg       0.97      0.96      0.96     11702\nweighted avg       0.97      0.96      0.96     11702\n samples avg       0.95      0.96      0.95     11702\n\n\n\n/tmp/ipykernel_15152/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13915089234034914\nEpoch 19 loss is 0.097373618826446\nEpoch 29 loss is 0.08775521526507787\nEpoch 39 loss is 0.07581347006628077\nEpoch 49 loss is 0.06707371275141155\nEpoch 59 loss is 0.07044893252214587\nEpoch 69 loss is 0.06647936146342898\nEpoch 79 loss is 0.06290139006666541\nEpoch 89 loss is 0.07122418431326744\nEpoch 99 loss is 0.06815365861695168\nEpoch 109 loss is 0.07112648692035674\nEpoch 119 loss is 0.06887879623935313\nEpoch 129 loss is 0.065383156901932\nEpoch 139 loss is 0.06278148365408386\nEpoch 149 loss is 0.05971489880293905\nEpoch 159 loss is 0.05826395838084222\nEpoch 169 loss is 0.059517928423252736\nEpoch 179 loss is 0.05508918404760044\nEpoch 189 loss is 0.05508463686628998\nEpoch 199 loss is 0.056739337282013\nTrain Acc.:  0.9532121263913517\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1064\n           1       0.93      0.94      0.93      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.96      0.98      1064\n           4       0.96      0.94      0.95      1064\n           5       0.93      0.92      0.93      1064\n           6       0.94      0.92      0.93      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.94      0.93      0.94      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.97      0.96      0.96     11702\n   macro avg       0.97      0.96      0.96     11702\nweighted avg       0.97      0.96      0.96     11702\n samples avg       0.95      0.96      0.95     11702\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_multiple_runs.html",
    "href": "examples/autass/autass_multiple_runs.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_multiple_runs.html#config",
    "href": "examples/autass/autass_multiple_runs.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1"
  },
  {
    "objectID": "examples/autass/autass_multiple_runs.html#single-layer",
    "href": "examples/autass/autass_multiple_runs.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=4c6da71cfc4c498f8da7e410e130c443\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/4c6da71cfc4c498f8da7e410e130c443/output/log\n======> WARNING! Git diff to large to store (3944kb), skipping uncommitted changes <======\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2798053516728018\nEpoch 19 loss is 0.28693026311742914\nEpoch 29 loss is 0.2874780130965781\nEpoch 39 loss is 0.2984541579734532\nEpoch 49 loss is 0.3193280469886697\nEpoch 59 loss is 0.33707626521445716\nEpoch 69 loss is 0.30040521139203175\nEpoch 79 loss is 0.28762346204797534\nEpoch 89 loss is 0.31152799377307844\nEpoch 99 loss is 0.2576630936111671\nEpoch 109 loss is 0.291178549427303\nEpoch 119 loss is 0.2490802764713475\nEpoch 129 loss is 0.23373190527990692\nEpoch 139 loss is 0.28033977577880903\nEpoch 149 loss is 0.2837003647392515\nEpoch 159 loss is 0.2538141131717079\nEpoch 169 loss is 0.3233458041492201\nEpoch 179 loss is 0.34501202353519583\nEpoch 189 loss is 0.34266169941405217\nEpoch 199 loss is 0.3550294292895104\nTrain Acc.:  0.8201337406798128\n              precision    recall  f1-score   support\n\n           0       0.93      0.93      0.93      1063\n           1       0.85      0.74      0.79      1064\n           2       0.96      0.93      0.94      1064\n           3       0.86      0.93      0.90      1064\n           4       0.84      0.83      0.83      1064\n           5       0.90      0.75      0.82      1063\n           6       0.77      0.71      0.74      1064\n           7       0.99      0.97      0.98      1064\n           8       0.98      1.00      0.99      1064\n           9       0.83      0.74      0.78      1064\n          10       0.92      0.83      0.87      1064\n\n   micro avg       0.90      0.85      0.87     11702\n   macro avg       0.89      0.85      0.87     11702\nweighted avg       0.89      0.85      0.87     11702\n samples avg       0.83      0.85      0.84     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3103850582404919\nEpoch 19 loss is 0.2418774839044744\nEpoch 29 loss is 0.265908605107625\nEpoch 39 loss is 0.2943059382354039\nEpoch 49 loss is 0.276436161067747\nEpoch 59 loss is 0.32590297383744593\nEpoch 69 loss is 0.2767146737255772\nEpoch 79 loss is 0.287858186740984\nEpoch 89 loss is 0.35907828087177\nEpoch 99 loss is 0.4186029191773882\nEpoch 109 loss is 0.33285225361643794\nEpoch 119 loss is 0.3228767984459741\nEpoch 129 loss is 0.35609662567173445\nEpoch 139 loss is 0.2398215116127861\nEpoch 149 loss is 0.37533755882336206\nEpoch 159 loss is 0.3230767278001795\nEpoch 169 loss is 0.32038495509949566\nEpoch 179 loss is 0.2728121593992897\nEpoch 189 loss is 0.23113384043537294\nEpoch 199 loss is 0.23697167827133853\nTrain Acc.:  0.817505928600423\n              precision    recall  f1-score   support\n\n           0       0.94      0.89      0.91      1063\n           1       0.85      0.72      0.78      1064\n           2       0.96      0.92      0.94      1064\n           3       0.92      0.91      0.92      1063\n           4       0.79      0.69      0.74      1064\n           5       0.84      0.91      0.87      1064\n           6       0.70      0.70      0.70      1064\n           7       0.98      0.99      0.98      1064\n           8       1.00      0.99      0.99      1064\n           9       0.85      0.91      0.88      1064\n          10       0.93      0.84      0.88      1064\n\n   micro avg       0.89      0.86      0.87     11702\n   macro avg       0.89      0.86      0.87     11702\nweighted avg       0.89      0.86      0.87     11702\n samples avg       0.84      0.86      0.84     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.234435066899149\nEpoch 19 loss is 0.23500500804411487\nEpoch 29 loss is 0.2704866989198225\nEpoch 39 loss is 0.22928027331865544\nEpoch 49 loss is 0.24259319125743486\nEpoch 59 loss is 0.23993151432124912\nEpoch 69 loss is 0.2355228313899169\nEpoch 79 loss is 0.29352466112392755\nEpoch 89 loss is 0.34534633221051547\nEpoch 99 loss is 0.28059194333336157\nEpoch 109 loss is 0.3367117756837938\nEpoch 119 loss is 0.35539835849532864\nEpoch 129 loss is 0.29370534618128263\nEpoch 139 loss is 0.2897315933896303\nEpoch 149 loss is 0.23595774051726134\nEpoch 159 loss is 0.3217572035914341\nEpoch 169 loss is 0.3612773694004577\nEpoch 179 loss is 0.29780505445589145\nEpoch 189 loss is 0.3884432736077841\nEpoch 199 loss is 0.5057752075140738\nTrain Acc.:  0.82641485247933\n              precision    recall  f1-score   support\n\n           0       0.74      0.98      0.84      1064\n           1       0.87      0.69      0.77      1064\n           2       0.96      0.92      0.94      1064\n           3       0.93      0.93      0.93      1063\n           4       0.84      0.36      0.50      1064\n           5       0.79      0.94      0.86      1064\n           6       0.87      0.67      0.76      1063\n           7       0.96      0.99      0.97      1064\n           8       1.00      1.00      1.00      1064\n           9       0.84      0.92      0.88      1064\n          10       0.94      0.83      0.88      1064\n\n   micro avg       0.88      0.84      0.86     11702\n   macro avg       0.89      0.84      0.85     11702\nweighted avg       0.89      0.84      0.85     11702\n samples avg       0.83      0.84      0.83     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.24826095378107285\nEpoch 19 loss is 0.22183934458525567\nEpoch 29 loss is 0.2376939787540323\nEpoch 39 loss is 0.23677890969663506\nEpoch 49 loss is 0.19782310665792108\nEpoch 59 loss is 0.18985340247155252\nEpoch 69 loss is 0.1916335525972637\nEpoch 79 loss is 0.1843041258845089\nEpoch 89 loss is 0.20625673516565524\nEpoch 99 loss is 0.2462787903644629\nEpoch 109 loss is 0.3130027202975933\nEpoch 119 loss is 0.30493362345149605\nEpoch 129 loss is 0.299187534389669\nEpoch 139 loss is 0.2979426610025583\nEpoch 149 loss is 0.3395869082665093\nEpoch 159 loss is 0.3224462941593307\nEpoch 169 loss is 0.3119062898221137\nEpoch 179 loss is 0.36908871562997664\nEpoch 189 loss is 0.36701326872569157\nEpoch 199 loss is 0.2913419812224387\nTrain Acc.:  0.8227829170850514\n              precision    recall  f1-score   support\n\n           0       0.92      0.95      0.94      1063\n           1       0.84      0.91      0.87      1064\n           2       0.95      0.91      0.93      1064\n           3       0.97      0.93      0.95      1063\n           4       0.85      0.57      0.68      1064\n           5       0.91      0.87      0.89      1064\n           6       0.95      0.30      0.45      1064\n           7       0.99      1.00      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.77      0.98      0.86      1064\n          10       0.89      0.83      0.86      1064\n\n   micro avg       0.91      0.84      0.87     11702\n   macro avg       0.91      0.84      0.86     11702\nweighted avg       0.91      0.84      0.86     11702\n samples avg       0.83      0.84      0.83     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23893833129461026\nEpoch 19 loss is 0.35352674061561473\nEpoch 29 loss is 0.31082288381730555\nEpoch 39 loss is 0.28511622276983273\nEpoch 49 loss is 0.23868927247854704\nEpoch 59 loss is 0.2485950896878176\nEpoch 69 loss is 0.41070443044607735\nEpoch 79 loss is 0.3084329791560098\nEpoch 89 loss is 0.30589607100727806\nEpoch 99 loss is 0.3070161200848258\nEpoch 109 loss is 0.2976518308637186\nEpoch 119 loss is 0.2824108441539807\nEpoch 129 loss is 0.3116600160978614\nEpoch 139 loss is 0.31757349912205246\nEpoch 149 loss is 0.30672799590632993\nEpoch 159 loss is 0.40314944724027096\nEpoch 169 loss is 0.4096136636855462\nEpoch 179 loss is 0.4105623334879551\nEpoch 189 loss is 0.44534144032836126\nEpoch 199 loss is 0.46514225549527893\nTrain Acc.:  0.82690623197385\n              precision    recall  f1-score   support\n\n           0       0.96      0.93      0.94      1064\n           1       0.93      0.48      0.64      1064\n           2       0.91      0.91      0.91      1064\n           3       0.94      0.94      0.94      1064\n           4       0.84      0.63      0.72      1064\n           5       0.75      0.94      0.84      1064\n           6       0.85      0.73      0.78      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.86      0.95      0.90      1064\n          10       0.91      0.84      0.87      1064\n\n   micro avg       0.90      0.85      0.87     11702\n   macro avg       0.90      0.85      0.87     11702\nweighted avg       0.90      0.85      0.87     11702\n samples avg       0.84      0.85      0.84     11702\n\n\n\n\ntitle = \"MLMVN 48-10-11\"\nimage_name = \"results/MLMVN_48-10-11.png\"\nplot_loss_acc_list(title, list_losses, list_scores, image_name)\n\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=e9f13dacf8d142619b65b8316ca90780\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e9f13dacf8d142619b65b8316ca90780/output/log\n======> WARNING! Git diff to large to store (3944kb), skipping uncommitted changes <======\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 10:34:58,198 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.16476403157413338\nEpoch 19 loss is 0.13633803332426933\nEpoch 29 loss is 0.143033463264053\nEpoch 39 loss is 0.17849334688199742\nEpoch 49 loss is 0.21700776681915424\nEpoch 59 loss is 0.2034918992282111\nEpoch 69 loss is 0.1712665802112372\nEpoch 79 loss is 0.20085534461565652\nEpoch 89 loss is 0.2502123033688426\nEpoch 99 loss is 0.30439372755517824\nEpoch 109 loss is 0.3093228314198623\nEpoch 119 loss is 0.27491323067701007\nEpoch 129 loss is 0.28011414414391217\nEpoch 139 loss is 0.3747490415384792\nEpoch 149 loss is 0.27613471357266345\nEpoch 159 loss is 0.23981115046878665\nEpoch 169 loss is 0.2038788837831156\nEpoch 179 loss is 0.18847714759522705\nEpoch 189 loss is 0.2114042358851922\nEpoch 199 loss is 0.20061396634221004\nTrain Acc.:  0.8892473348003503\n              precision    recall  f1-score   support\n\n           0       0.94      0.90      0.92      1063\n           1       0.87      0.86      0.86      1064\n           2       0.99      0.94      0.96      1064\n           3       0.97      0.97      0.97      1064\n           4       0.87      0.76      0.81      1064\n           5       0.87      0.91      0.89      1063\n           6       0.87      0.85      0.86      1064\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.90      0.93      0.91      1064\n          10       0.95      0.92      0.94      1064\n\n   micro avg       0.93      0.91      0.92     11702\n   macro avg       0.93      0.91      0.92     11702\nweighted avg       0.93      0.91      0.92     11702\n samples avg       0.90      0.91      0.90     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.16358843200535636\nEpoch 19 loss is 0.2041445672195862\nEpoch 29 loss is 0.24642286260837112\nEpoch 39 loss is 0.19917514925837854\nEpoch 49 loss is 0.25327855214060685\nEpoch 59 loss is 0.21884971568181918\nEpoch 69 loss is 0.22624078805323974\nEpoch 79 loss is 0.26295991330810126\nEpoch 89 loss is 0.22422218505461025\nEpoch 99 loss is 0.2339717792959955\nEpoch 109 loss is 0.19874802368569566\nEpoch 119 loss is 0.23704211230309438\nEpoch 129 loss is 0.24084612126041485\nEpoch 139 loss is 0.20031062393464874\nEpoch 149 loss is 0.18938521056672464\nEpoch 159 loss is 0.20100549875098014\nEpoch 169 loss is 0.20972725948272172\nEpoch 179 loss is 0.20063395868672673\nEpoch 189 loss is 0.183422525811785\nEpoch 199 loss is 0.1669537319356945\nTrain Acc.:  0.8925374409810498\n              precision    recall  f1-score   support\n\n           0       0.95      0.92      0.93      1063\n           1       0.90      0.87      0.88      1064\n           2       0.99      0.97      0.98      1064\n           3       0.98      0.95      0.96      1063\n           4       0.89      0.76      0.82      1064\n           5       0.86      0.94      0.90      1064\n           6       0.89      0.89      0.89      1064\n           7       1.00      0.99      1.00      1064\n           8       1.00      0.99      1.00      1064\n           9       0.91      0.93      0.92      1064\n          10       0.96      0.94      0.95      1064\n\n   micro avg       0.94      0.92      0.93     11702\n   macro avg       0.94      0.92      0.93     11702\nweighted avg       0.94      0.92      0.93     11702\n samples avg       0.91      0.92      0.91     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.19744827084414995\nEpoch 19 loss is 0.20472359266774093\nEpoch 29 loss is 0.2544472813758418\nEpoch 39 loss is 0.19045720603945143\nEpoch 49 loss is 0.20343276143970507\nEpoch 59 loss is 0.22297812880413134\nEpoch 69 loss is 0.22178893715622944\nEpoch 79 loss is 0.2498799044945237\nEpoch 89 loss is 0.22153046921128713\nEpoch 99 loss is 0.23420279826211265\nEpoch 109 loss is 0.22040926507985742\nEpoch 119 loss is 0.21762960224465708\nEpoch 129 loss is 0.2605731742439022\nEpoch 139 loss is 0.2775670812423578\nEpoch 149 loss is 0.19603989313909037\nEpoch 159 loss is 0.20545349137968197\nEpoch 169 loss is 0.20606501781890182\nEpoch 179 loss is 0.18431993443444303\nEpoch 189 loss is 0.1741126129084759\nEpoch 199 loss is 0.1724068336751827\nTrain Acc.:  0.8685025744012648\n              precision    recall  f1-score   support\n\n           0       0.88      0.94      0.91      1064\n           1       0.92      0.75      0.82      1064\n           2       0.96      0.94      0.95      1064\n           3       0.91      0.97      0.94      1063\n           4       0.83      0.81      0.82      1064\n           5       0.83      0.93      0.88      1064\n           6       0.86      0.72      0.79      1063\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.83      0.94      0.88      1064\n          10       0.96      0.78      0.86      1064\n\n   micro avg       0.90      0.89      0.90     11702\n   macro avg       0.91      0.89      0.89     11702\nweighted avg       0.91      0.89      0.89     11702\n samples avg       0.88      0.89      0.88     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.18983226445452947\nEpoch 19 loss is 0.20989331423949678\nEpoch 29 loss is 0.20457408459425253\nEpoch 39 loss is 0.21686740636756055\nEpoch 49 loss is 0.1686385065915988\nEpoch 59 loss is 0.15638486085080838\nEpoch 69 loss is 0.1741827037910428\nEpoch 79 loss is 0.1640841394764191\nEpoch 89 loss is 0.17007600609779142\nEpoch 99 loss is 0.19284076037560408\nEpoch 109 loss is 0.19138844241490113\nEpoch 119 loss is 0.20478322070718535\nEpoch 129 loss is 0.21176829993640264\nEpoch 139 loss is 0.22802495808894305\nEpoch 149 loss is 0.2164955049311736\nEpoch 159 loss is 0.2058771283183833\nEpoch 169 loss is 0.17439026342796043\nEpoch 179 loss is 0.19134460117948338\nEpoch 189 loss is 0.17625964681553905\nEpoch 199 loss is 0.21366730862193598\nTrain Acc.:  0.8870895378896319\n              precision    recall  f1-score   support\n\n           0       0.90      0.94      0.92      1063\n           1       0.87      0.89      0.88      1064\n           2       0.93      0.97      0.95      1064\n           3       0.97      0.98      0.98      1063\n           4       0.90      0.74      0.81      1064\n           5       0.94      0.86      0.89      1064\n           6       0.86      0.76      0.81      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.83      0.92      0.88      1064\n          10       0.93      0.90      0.91      1064\n\n   micro avg       0.92      0.90      0.91     11702\n   macro avg       0.92      0.90      0.91     11702\nweighted avg       0.92      0.90      0.91     11702\n samples avg       0.89      0.90      0.90     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.17402481484483856\nEpoch 19 loss is 0.1944497671217645\nEpoch 29 loss is 0.14842624541281338\nEpoch 39 loss is 0.15502097794515227\nEpoch 49 loss is 0.15759596208770407\nEpoch 59 loss is 0.19170838004354154\nEpoch 69 loss is 0.1871364349827201\nEpoch 79 loss is 0.18722396261041316\nEpoch 89 loss is 0.1876739674177174\nEpoch 99 loss is 0.2193864859799957\nEpoch 109 loss is 0.19671043294002857\nEpoch 119 loss is 0.22702499457486658\nEpoch 129 loss is 0.21129339398637728\nEpoch 139 loss is 0.2182856933389895\nEpoch 149 loss is 0.21264683162737122\nEpoch 159 loss is 0.20315929596125387\nEpoch 169 loss is 0.18449427099611365\nEpoch 179 loss is 0.1783611846088383\nEpoch 189 loss is 0.18641298905492973\nEpoch 199 loss is 0.17783021613349564\nTrain Acc.:  0.8899737218792061\n              precision    recall  f1-score   support\n\n           0       0.91      0.95      0.93      1064\n           1       0.88      0.80      0.84      1064\n           2       0.96      0.97      0.97      1064\n           3       0.96      0.98      0.97      1064\n           4       0.93      0.77      0.84      1064\n           5       0.85      0.92      0.88      1064\n           6       0.90      0.74      0.81      1063\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1063\n           9       0.88      0.92      0.90      1064\n          10       0.95      0.91      0.93      1064\n\n   micro avg       0.93      0.90      0.92     11702\n   macro avg       0.93      0.90      0.91     11702\nweighted avg       0.93      0.90      0.91     11702\n samples avg       0.90      0.90      0.90     11702\n\n\n\n\ntitle = \"MLMVN 48-20-11\"\nimage_name = \"results/MLMVN_48-20-11.png\"\nplot_loss_acc_list(title, list_losses, list_scores, image_name)\n\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=3560d2ef7d5a41718f94259373e7966c\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/3560d2ef7d5a41718f94259373e7966c/output/log\n======> WARNING! Git diff to large to store (3944kb), skipping uncommitted changes <======\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 10:50:33,334 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.09829269534206735\nEpoch 19 loss is 0.09728068709222959\nEpoch 29 loss is 0.07600427848917563\nEpoch 39 loss is 0.060900669434152684\nEpoch 49 loss is 0.06557708991894179\nEpoch 59 loss is 0.0620218198349882\nEpoch 69 loss is 0.055180916741809694\nEpoch 79 loss is 0.05770168026537519\nEpoch 89 loss is 0.06312691609980167\nEpoch 99 loss is 0.06258158094408227\nEpoch 109 loss is 0.06726164969189814\nEpoch 119 loss is 0.053113842805941974\nEpoch 129 loss is 0.05192851680713503\nEpoch 139 loss is 0.07066245038442918\nEpoch 149 loss is 0.0635699498573305\nEpoch 159 loss is 0.05401376535508686\nEpoch 169 loss is 0.06267989696299203\nEpoch 179 loss is 0.08033388429254887\nEpoch 189 loss is 0.06335794221812717\nEpoch 199 loss is 0.0672287838247473\nTrain Acc.:  0.9651975131924712\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      1063\n           1       0.97      0.94      0.95      1064\n           2       0.99      0.98      0.99      1064\n           3       0.99      0.99      0.99      1064\n           4       0.96      0.92      0.94      1064\n           5       0.96      0.96      0.96      1063\n           6       0.95      0.94      0.94      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.96      0.96      0.96      1064\n          10       0.98      0.97      0.97      1064\n\n   micro avg       0.97      0.97      0.97     11702\n   macro avg       0.97      0.97      0.97     11702\nweighted avg       0.97      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.0493437117112904\nEpoch 19 loss is 0.056332634788183775\nEpoch 29 loss is 0.04848900109983579\nEpoch 39 loss is 0.052226300307049216\nEpoch 49 loss is 0.06732437820930277\nEpoch 59 loss is 0.06013609383791088\nEpoch 69 loss is 0.06488484996680653\nEpoch 79 loss is 0.07402341604512623\nEpoch 89 loss is 0.06415823828871538\nEpoch 99 loss is 0.07401829229000033\nEpoch 109 loss is 0.07514582485574416\nEpoch 119 loss is 0.08362585630122568\nEpoch 129 loss is 0.08486246263407854\nEpoch 139 loss is 0.07425523240837605\nEpoch 149 loss is 0.08443371962698622\nEpoch 159 loss is 0.09720449556099098\nEpoch 169 loss is 0.10067659587005852\nEpoch 179 loss is 0.1193294871778844\nEpoch 189 loss is 0.10106213514554797\nEpoch 199 loss is 0.09858312506276698\nTrain Acc.:  0.9670134808896105\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1063\n           1       0.96      0.96      0.96      1064\n           2       1.00      0.99      0.99      1064\n           3       0.99      0.98      0.99      1063\n           4       0.96      0.95      0.95      1064\n           5       0.98      0.97      0.97      1064\n           6       0.97      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.98      0.97      0.97      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.048120356670683234\nEpoch 19 loss is 0.054593459310181086\nEpoch 29 loss is 0.05499350411994715\nEpoch 39 loss is 0.0559697642856175\nEpoch 49 loss is 0.05754442725054534\nEpoch 59 loss is 0.061489513417114956\nEpoch 69 loss is 0.07086944463691781\nEpoch 79 loss is 0.07706030743617291\nEpoch 89 loss is 0.07593725247272468\nEpoch 99 loss is 0.0732947676897344\nEpoch 109 loss is 0.07531350286253687\nEpoch 119 loss is 0.0911661580282024\nEpoch 129 loss is 0.11443896325586743\nEpoch 139 loss is 0.10335307976202589\nEpoch 149 loss is 0.10366860029550481\nEpoch 159 loss is 0.09899129316663502\nEpoch 169 loss is 0.10486127629475518\nEpoch 179 loss is 0.11867640283355274\nEpoch 189 loss is 0.12511503382191627\nEpoch 199 loss is 0.11799590483737314\nTrain Acc.:  0.9631679022368449\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1064\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.98      0.99      1064\n           3       0.99      0.98      0.99      1063\n           4       0.97      0.96      0.96      1064\n           5       0.96      0.95      0.96      1064\n           6       0.93      0.92      0.93      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      0.99      1.00      1064\n           9       0.96      0.95      0.95      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.97      0.97      0.97     11702\n   macro avg       0.97      0.97      0.97     11702\nweighted avg       0.97      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.05517547633181337\nEpoch 19 loss is 0.055728762308260765\nEpoch 29 loss is 0.06259614482605945\nEpoch 39 loss is 0.0581895142248359\nEpoch 49 loss is 0.06561329385021032\nEpoch 59 loss is 0.07032996307126822\nEpoch 69 loss is 0.08136375973696794\nEpoch 79 loss is 0.11977187777366366\nEpoch 89 loss is 0.084233777680467\nEpoch 99 loss is 0.09336862232729168\nEpoch 109 loss is 0.10234290900702865\nEpoch 119 loss is 0.14733771738094092\nEpoch 129 loss is 0.13079876845970226\nEpoch 139 loss is 0.11238334142687469\nEpoch 149 loss is 0.12448816318563458\nEpoch 159 loss is 0.11167881321193102\nEpoch 169 loss is 0.10511961659141926\nEpoch 179 loss is 0.10364262851145377\nEpoch 189 loss is 0.10639918512951829\nEpoch 199 loss is 0.09649024979717642\nTrain Acc.:  0.9571431623475122\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      1063\n           1       0.95      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.98      0.98      1063\n           4       0.96      0.94      0.95      1064\n           5       0.95      0.95      0.95      1064\n           6       0.93      0.93      0.93      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.96      0.94      0.95      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.97      0.96      0.97     11702\n   macro avg       0.97      0.96      0.97     11702\nweighted avg       0.97      0.96      0.97     11702\n samples avg       0.96      0.96      0.96     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.0682420963425835\nEpoch 19 loss is 0.06717379428891208\nEpoch 29 loss is 0.059015318733788666\nEpoch 39 loss is 0.06176190170409206\nEpoch 49 loss is 0.07237386654932479\nEpoch 59 loss is 0.06911019201668694\nEpoch 69 loss is 0.06392903645561016\nEpoch 79 loss is 0.0758716126534257\nEpoch 89 loss is 0.07730068602143042\nEpoch 99 loss is 0.09665151482480443\nEpoch 109 loss is 0.09222695319517936\nEpoch 119 loss is 0.08182469168154197\nEpoch 129 loss is 0.08335947978406187\nEpoch 139 loss is 0.09159492862302886\nEpoch 149 loss is 0.0906091966868739\nEpoch 159 loss is 0.0885280304984252\nEpoch 169 loss is 0.09157913406478384\nEpoch 179 loss is 0.09323288036652193\nEpoch 189 loss is 0.08298648299531121\nEpoch 199 loss is 0.09408725998949831\nTrain Acc.:  0.95340440532399\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97      1064\n           1       0.98      0.87      0.92      1064\n           2       1.00      0.99      0.99      1064\n           3       1.00      0.98      0.99      1064\n           4       0.97      0.92      0.94      1064\n           5       0.93      0.97      0.95      1064\n           6       0.96      0.88      0.92      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.95      0.95      0.95      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.95      0.97     11702\n   macro avg       0.98      0.95      0.96     11702\nweighted avg       0.98      0.95      0.96     11702\n samples avg       0.95      0.95      0.95     11702\n\n\n\n\ntitle = \"MLMVN 48-50-11\"\nimage_name = \"results/MLMVN_48-50-11.png\"\nplot_loss_acc_list(title, list_losses, list_scores, image_name)\n\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=054b29d2a004437ca390c9553dc79f1d\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/054b29d2a004437ca390c9553dc79f1d/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-29 15:23:48,722 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.09289930875531222\nEpoch 19 loss is 0.07704454008271648\nEpoch 29 loss is 0.05779679460027949\nEpoch 39 loss is 0.054986268708860524\nEpoch 49 loss is 0.04733543763708861\nEpoch 59 loss is 0.042054025328116786\nEpoch 69 loss is 0.0412137783292874\nEpoch 79 loss is 0.038980292259018084\nEpoch 89 loss is 0.04195052638688717\nEpoch 99 loss is 0.04400554969643017\nEpoch 109 loss is 0.03583445803540681\nEpoch 119 loss is 0.031936028648656224\nEpoch 129 loss is 0.03988531487611844\nEpoch 139 loss is 0.03480796749345809\nEpoch 149 loss is 0.02863502808161461\nEpoch 159 loss is 0.03285262207237886\nEpoch 169 loss is 0.03205184485479631\nEpoch 179 loss is 0.030309476964466488\nEpoch 189 loss is 0.031384972778109824\nEpoch 199 loss is 0.03278481486374329\nTrain Acc.:  0.9800884483090136\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.99      1063\n           1       0.95      0.95      0.95      1064\n           2       1.00      0.99      0.99      1064\n           3       0.99      0.99      0.99      1064\n           4       0.98      0.95      0.96      1064\n           5       0.98      0.96      0.97      1063\n           6       0.97      0.94      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.97      0.97      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\nClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09651948221217745\nEpoch 19 loss is 0.08151637139831774\nEpoch 29 loss is 0.06691473724981084\nEpoch 39 loss is 0.06491206984382683\nEpoch 49 loss is 0.052847569881330464\nEpoch 59 loss is 0.06549010836655457\nEpoch 69 loss is 0.049922240791715526\nEpoch 79 loss is 0.04560603635225597\nEpoch 89 loss is 0.04709434644222514\nEpoch 99 loss is 0.09155044545100574\nEpoch 109 loss is 0.05177482526370294\nEpoch 119 loss is 0.045173095736625105\nEpoch 129 loss is 0.041602230912906296\nEpoch 139 loss is 0.04224502944542365\nEpoch 149 loss is 0.03876666575995179\nEpoch 159 loss is 0.04185715320046077\nEpoch 169 loss is 0.03916762878300975\nEpoch 179 loss is 0.04057138104183288\nEpoch 189 loss is 0.036145847549370606\nEpoch 199 loss is 0.034255525449209644\nTrain Acc.:  0.9715640822953832\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1063\n           1       0.95      0.94      0.95      1064\n           2       1.00      0.99      0.99      1064\n           3       0.98      0.97      0.97      1063\n           4       0.97      0.96      0.96      1064\n           5       0.96      0.96      0.96      1064\n           6       0.98      0.95      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.96      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.10197848648032753\nEpoch 19 loss is 0.06620522870230666\nEpoch 29 loss is 0.060791586014725764\nEpoch 39 loss is 0.044058532288178125\nEpoch 49 loss is 0.049066377845850113\nEpoch 59 loss is 0.041483171210737656\nEpoch 69 loss is 0.041699789178138276\nEpoch 79 loss is 0.04010221614370895\nEpoch 89 loss is 0.04200853150707565\nEpoch 99 loss is 0.03748572558613685\nEpoch 109 loss is 0.03339820400178514\nEpoch 119 loss is 0.03418882536913687\nEpoch 129 loss is 0.0396406334017026\nEpoch 139 loss is 0.034145767822064424\nEpoch 149 loss is 0.031575263631362274\nEpoch 159 loss is 0.03257143025928933\nEpoch 169 loss is 0.02900946434349241\nEpoch 179 loss is 0.02646721908453089\nEpoch 189 loss is 0.0283171769401771\nEpoch 199 loss is 0.02924505333332722\nTrain Acc.:  0.982673531736706\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.99      1064\n           1       0.96      0.94      0.95      1064\n           2       1.00      0.99      0.99      1064\n           3       0.99      0.98      0.99      1063\n           4       0.99      0.96      0.98      1064\n           5       0.96      0.96      0.96      1064\n           6       0.97      0.96      0.96      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.98      0.97      0.97      1064\n          10       0.99      0.98      0.99      1064\n\n   micro avg       0.98      0.98      0.98     11702\n   macro avg       0.98      0.98      0.98     11702\nweighted avg       0.98      0.98      0.98     11702\n samples avg       0.97      0.98      0.97     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.0941163479264211\nEpoch 19 loss is 0.07211157489541835\nEpoch 29 loss is 0.054892876686113645\nEpoch 39 loss is 0.04960960956649523\nEpoch 49 loss is 0.05018771907397448\nEpoch 59 loss is 0.042637427155138234\nEpoch 69 loss is 0.04489222136725094\nEpoch 79 loss is 0.04308418388475778\nEpoch 89 loss is 0.036878224414511725\nEpoch 99 loss is 0.040199719304018\nEpoch 109 loss is 0.036902204839538835\nEpoch 119 loss is 0.0691947571347315\nEpoch 129 loss is 0.030887174296426335\nEpoch 139 loss is 0.03259241320497548\nEpoch 149 loss is 0.028693328512752206\nEpoch 159 loss is 0.031290255170440935\nEpoch 169 loss is 0.03944383013459841\nEpoch 179 loss is 0.028000841225022752\nEpoch 189 loss is 0.02779464459963016\nEpoch 199 loss is 0.02589195430212821\nTrain Acc.:  0.9802807272416519\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1063\n           1       0.96      0.96      0.96      1064\n           2       1.00      0.99      0.99      1064\n           3       0.98      0.99      0.99      1063\n           4       0.98      0.97      0.97      1064\n           5       0.97      0.95      0.96      1064\n           6       0.97      0.96      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.99      0.98      0.99      1064\n\n   micro avg       0.98      0.98      0.98     11702\n   macro avg       0.98      0.98      0.98     11702\nweighted avg       0.98      0.98      0.98     11702\n samples avg       0.97      0.98      0.97     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09346566112894616\nEpoch 19 loss is 0.06853978841650973\nEpoch 29 loss is 0.06052018883336245\nEpoch 39 loss is 0.05080707795295913\nEpoch 49 loss is 0.04300807335716183\nEpoch 59 loss is 0.04929946212015129\nEpoch 69 loss is 0.03806456857102248\nEpoch 79 loss is 0.038455738981116754\nEpoch 89 loss is 0.03478891861672422\nEpoch 99 loss is 0.03865558608219994\nEpoch 109 loss is 0.03655600548813174\nEpoch 119 loss is 0.03847341857153493\nEpoch 129 loss is 0.03406350920763785\nEpoch 139 loss is 0.028289964899613603\nEpoch 149 loss is 0.03063547244197927\nEpoch 159 loss is 0.03108327055907574\nEpoch 169 loss is 0.02898037821549324\nEpoch 179 loss is 0.033139572848924596\nEpoch 189 loss is 0.03204601005634074\nEpoch 199 loss is 0.036471306994475566\nTrain Acc.:  0.9812207575789946\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.99      1064\n           1       0.96      0.95      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       1.00      0.99      0.99      1064\n           4       0.97      0.96      0.96      1064\n           5       0.97      0.95      0.96      1064\n           6       0.96      0.97      0.96      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.98      0.96      0.97      1064\n          10       0.99      0.98      0.99      1064\n\n   micro avg       0.98      0.98      0.98     11702\n   macro avg       0.98      0.98      0.98     11702\nweighted avg       0.98      0.98      0.98     11702\n samples avg       0.97      0.98      0.97     11702\n\n\n\n\n# title = \"MLMVN 48-100-11\"\n# image_name = \"results/MLMVN_48-100-11.png\"\n# plot_loss_acc_list(title, list_losses, list_scores, image_name)\nplot_loss_acc_list(\n    \"$\\mathbb{C}$: [48-100-11] \", list_losses, list_scores, \"mlmvn-48-100-11.png\"\n)\n\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_multiple_runs.html#multi-layer",
    "href": "examples/autass/autass_multiple_runs.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=e9ee3195207b4491b042a3263013a77e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e9ee3195207b4491b042a3263013a77e/output/log\n======> WARNING! Git diff to large to store (3945kb), skipping uncommitted changes <======\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 11:47:55,893 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.6826655212453996\nEpoch 19 loss is 0.6434948379370677\nEpoch 29 loss is 1.682729382474014\nEpoch 39 loss is 0.8484829522237753\nEpoch 49 loss is 0.9924725228800226\nEpoch 59 loss is 1.225485459978534\nEpoch 69 loss is 1.2029846038520795\nEpoch 79 loss is 1.099226063781701\nEpoch 89 loss is 1.0669221436146312\nEpoch 99 loss is 1.107601797709253\nEpoch 109 loss is 1.154826187850244\nEpoch 119 loss is 1.1937538569152097\nEpoch 129 loss is 1.326074848158841\nEpoch 139 loss is 1.3625349669490228\nEpoch 149 loss is 1.416390194581627\nEpoch 159 loss is 1.4488717111937812\nEpoch 169 loss is 1.4282188851979989\nEpoch 179 loss is 1.3428309118204484\nEpoch 189 loss is 1.3047422414686045\nEpoch 199 loss is 1.2980417729564269\nTrain Acc.:  0.2878629264853548\n              precision    recall  f1-score   support\n\n           0       0.49      0.68      0.57      1063\n           1       0.53      0.09      0.15      1064\n           2       0.59      0.17      0.26      1064\n           3       0.70      0.02      0.03      1064\n           4       0.44      0.21      0.28      1064\n           5       0.67      0.74      0.70      1063\n           6       0.41      0.04      0.07      1064\n           7       0.36      0.80      0.50      1064\n           8       0.86      0.94      0.90      1064\n           9       0.45      0.13      0.20      1064\n          10       0.60      0.01      0.01      1064\n\n   micro avg       0.53      0.35      0.42     11702\n   macro avg       0.55      0.35      0.33     11702\nweighted avg       0.55      0.35      0.33     11702\n samples avg       0.32      0.35      0.33     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.6345554160230358\nEpoch 19 loss is 0.6341311608846562\nEpoch 29 loss is 0.6275105257557847\nEpoch 39 loss is 0.5689082943180627\nEpoch 49 loss is 0.6202519709362332\nEpoch 59 loss is 0.5899542082300644\nEpoch 69 loss is 0.6059699475133334\nEpoch 79 loss is 0.6356775981652049\nEpoch 89 loss is 0.6037438777306993\nEpoch 99 loss is 0.6510460628543737\nEpoch 109 loss is 0.6542612130467913\nEpoch 119 loss is 0.6918248563732031\nEpoch 129 loss is 0.7057371972704083\nEpoch 139 loss is 0.6756918212281602\nEpoch 149 loss is 0.7112479285104816\nEpoch 159 loss is 0.7220244566123118\nEpoch 169 loss is 0.6584324293919726\nEpoch 179 loss is 0.677293022265238\nEpoch 189 loss is 0.732903885474247\nEpoch 199 loss is 0.6873554666623318\nTrain Acc.:  0.19435127224560428\n              precision    recall  f1-score   support\n\n           0       0.65      0.86      0.74      1063\n           1       0.60      0.11      0.18      1064\n           2       1.00      0.00      0.00      1064\n           3       0.71      0.07      0.13      1063\n           4       0.53      0.38      0.44      1064\n           5       0.51      0.43      0.47      1064\n           6       0.12      0.99      0.22      1064\n           7       0.83      0.63      0.72      1064\n           8       0.86      0.95      0.90      1064\n           9       0.60      0.20      0.30      1064\n          10       0.49      0.41      0.45      1064\n\n   micro avg       0.36      0.46      0.40     11702\n   macro avg       0.63      0.46      0.41     11702\nweighted avg       0.63      0.46      0.41     11702\n samples avg       0.32      0.46      0.37     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5564281727099429\nEpoch 19 loss is 0.6418290258673861\nEpoch 29 loss is 0.5871009681556589\nEpoch 39 loss is 0.5831930390706477\nEpoch 49 loss is 0.5565574083864082\nEpoch 59 loss is 0.564178441950088\nEpoch 69 loss is 0.6094082608646254\nEpoch 79 loss is 0.6089586009663431\nEpoch 89 loss is 0.5983078625392015\nEpoch 99 loss is 0.5945027639119206\nEpoch 109 loss is 0.6506962952883656\nEpoch 119 loss is 0.7257264045269749\nEpoch 129 loss is 0.678212085055888\nEpoch 139 loss is 0.6776558524296338\nEpoch 149 loss is 0.7104895346845802\nEpoch 159 loss is 0.6882588813485881\nEpoch 169 loss is 0.7179067223460094\nEpoch 179 loss is 0.7732332268676302\nEpoch 189 loss is 0.6688145327155556\nEpoch 199 loss is 0.7576797799143649\nTrain Acc.:  0.35227636891917874\n              precision    recall  f1-score   support\n\n           0       0.69      0.91      0.78      1064\n           1       0.63      0.14      0.23      1064\n           2       0.54      0.05      0.08      1064\n           3       0.73      0.17      0.27      1063\n           4       0.60      0.15      0.24      1064\n           5       0.37      0.55      0.45      1064\n           6       0.58      0.39      0.47      1063\n           7       0.83      0.88      0.85      1064\n           8       0.96      0.87      0.91      1064\n           9       0.40      0.49      0.44      1064\n          10       0.52      0.22      0.31      1064\n\n   micro avg       0.61      0.44      0.51     11702\n   macro avg       0.62      0.44      0.46     11702\nweighted avg       0.62      0.44      0.46     11702\n samples avg       0.39      0.44      0.41     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5662067563710496\nEpoch 19 loss is 0.6213271730355293\nEpoch 29 loss is 0.654203641457264\nEpoch 39 loss is 0.6337888713462974\nEpoch 49 loss is 0.6457664152151374\nEpoch 59 loss is 0.6597888498208774\nEpoch 69 loss is 0.6427117237528076\nEpoch 79 loss is 0.6644485126309694\nEpoch 89 loss is 0.6497885471919859\nEpoch 99 loss is 0.664900713379054\nEpoch 109 loss is 0.7335350312946143\nEpoch 119 loss is 0.6744735994675508\nEpoch 129 loss is 0.6019557783170132\nEpoch 139 loss is 0.6643925186955931\nEpoch 149 loss is 0.6978280461548153\nEpoch 159 loss is 0.7079139499893651\nEpoch 169 loss is 0.7004879680163926\nEpoch 179 loss is 0.6955539259637074\nEpoch 189 loss is 0.7227160645393367\nEpoch 199 loss is 0.7013177086430019\nTrain Acc.:  0.30412117845621384\n              precision    recall  f1-score   support\n\n           0       0.89      0.70      0.78      1063\n           1       0.69      0.03      0.05      1064\n           2       0.40      0.00      0.01      1064\n           3       0.29      0.75      0.41      1063\n           4       0.00      0.00      0.00      1064\n           5       0.36      0.40      0.38      1064\n           6       0.00      0.00      0.00      1064\n           7       0.59      0.90      0.71      1064\n           8       0.90      0.95      0.92      1064\n           9       0.23      0.91      0.37      1064\n          10       0.00      0.00      0.00      1064\n\n   micro avg       0.42      0.42      0.42     11702\n   macro avg       0.40      0.42      0.33     11702\nweighted avg       0.40      0.42      0.33     11702\n samples avg       0.36      0.42      0.38     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.6462013213971773\nEpoch 19 loss is 0.6389570948507705\nEpoch 29 loss is 0.6302553856908272\nEpoch 39 loss is 0.6686538271036292\nEpoch 49 loss is 0.6319803249955375\nEpoch 59 loss is 0.6704890164284341\nEpoch 69 loss is 0.6675763220336224\nEpoch 79 loss is 0.6127046239510358\nEpoch 89 loss is 0.6306565278537534\nEpoch 99 loss is 0.6080596561205306\nEpoch 109 loss is 0.6252069135156932\nEpoch 119 loss is 0.6703968429372862\nEpoch 129 loss is 0.6282122259810444\nEpoch 139 loss is 0.6578433907832637\nEpoch 149 loss is 0.6300546160158725\nEpoch 159 loss is 0.6349844848961109\nEpoch 169 loss is 0.660426835785601\nEpoch 179 loss is 0.6781952575179441\nEpoch 189 loss is 0.7636298837766232\nEpoch 199 loss is 0.7090256706323586\nTrain Acc.:  0.26226846411861476\n              precision    recall  f1-score   support\n\n           0       0.83      0.79      0.81      1064\n           1       0.00      0.00      0.00      1064\n           2       0.40      0.00      0.00      1064\n           3       0.00      0.00      0.00      1064\n           4       0.44      0.06      0.10      1064\n           5       0.32      0.87      0.46      1064\n           6       0.00      0.00      0.00      1063\n           7       0.77      0.59      0.67      1064\n           8       0.89      0.66      0.76      1063\n           9       0.35      0.33      0.34      1064\n          10       0.33      0.00      0.01      1064\n\n   micro avg       0.52      0.30      0.38     11702\n   macro avg       0.39      0.30      0.29     11702\nweighted avg       0.39      0.30      0.29     11702\n samples avg       0.28      0.30      0.29     11702\n\n\n\n\ntitle = \"MLMVN 48-10-10-11\"\nimage_name = \"results/MLMVN_48-10-10-11.png\"\nplot_loss_acc_list(title, list_losses, list_scores, image_name)\n\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=a3eff1625a2248499f654e9c4acbc249\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/a3eff1625a2248499f654e9c4acbc249/output/log\n======> WARNING! Git diff to large to store (3945kb), skipping uncommitted changes <======\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 12:08:56,112 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.45985028386046645\nEpoch 19 loss is 0.46694517354238074\nEpoch 29 loss is 0.5138218233961965\nEpoch 39 loss is 0.489275320767695\nEpoch 49 loss is 0.46309303163233473\nEpoch 59 loss is 0.5043632157724234\nEpoch 69 loss is 0.5141429366918676\nEpoch 79 loss is 0.5218683561772139\nEpoch 89 loss is 0.5066106861905705\nEpoch 99 loss is 0.4761282653118995\nEpoch 109 loss is 0.49827079538493996\nEpoch 119 loss is 0.48505905692880175\nEpoch 129 loss is 0.5079328540396203\nEpoch 139 loss is 0.5163228861017134\nEpoch 149 loss is 0.5224325401679479\nEpoch 159 loss is 0.5032478966871081\nEpoch 169 loss is 0.5706603788963629\nEpoch 179 loss is 0.5139070999296187\nEpoch 189 loss is 0.5066333485208855\nEpoch 199 loss is 0.5263176922877412\nTrain Acc.:  0.6204200226461855\n              precision    recall  f1-score   support\n\n           0       0.87      0.80      0.83      1063\n           1       0.83      0.71      0.76      1064\n           2       0.77      0.75      0.76      1064\n           3       0.86      0.74      0.80      1064\n           4       0.68      0.54      0.60      1064\n           5       0.83      0.71      0.77      1063\n           6       0.56      0.65      0.60      1064\n           7       0.94      0.90      0.92      1064\n           8       0.99      0.94      0.97      1064\n           9       0.74      0.65      0.69      1064\n          10       0.80      0.55      0.65      1064\n\n   micro avg       0.80      0.72      0.76     11702\n   macro avg       0.81      0.72      0.76     11702\nweighted avg       0.81      0.72      0.76     11702\n samples avg       0.67      0.72      0.69     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5688516497621854\nEpoch 19 loss is 0.5853694274411174\nEpoch 29 loss is 0.5336314268039691\nEpoch 39 loss is 0.5259366272492032\nEpoch 49 loss is 0.4406968322599601\nEpoch 59 loss is 0.4602583709539878\nEpoch 69 loss is 0.422908938931125\nEpoch 79 loss is 0.38922475710926\nEpoch 89 loss is 0.37434202443270037\nEpoch 99 loss is 0.41176429432307965\nEpoch 109 loss is 0.4118744091051351\nEpoch 119 loss is 0.4045098062961893\nEpoch 129 loss is 0.3924032459478811\nEpoch 139 loss is 0.3417262166598632\nEpoch 149 loss is 0.3713361415147345\nEpoch 159 loss is 0.41062360966720174\nEpoch 169 loss is 0.4235746380622734\nEpoch 179 loss is 0.440063917933553\nEpoch 189 loss is 0.4052802512719287\nEpoch 199 loss is 0.42583715747994355\nTrain Acc.:  0.5227636891917875\n              precision    recall  f1-score   support\n\n           0       0.92      0.67      0.78      1063\n           1       0.77      0.70      0.73      1064\n           2       0.80      0.54      0.64      1064\n           3       0.71      0.54      0.62      1063\n           4       0.59      0.45      0.51      1064\n           5       0.79      0.72      0.75      1064\n           6       0.46      0.79      0.58      1064\n           7       0.80      0.69      0.74      1064\n           8       0.99      0.96      0.97      1064\n           9       0.00      0.00      0.00      1064\n          10       0.65      0.49      0.56      1064\n\n   micro avg       0.72      0.60      0.65     11702\n   macro avg       0.68      0.60      0.63     11702\nweighted avg       0.68      0.60      0.63     11702\n samples avg       0.56      0.60      0.57     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.47377377477567517\nEpoch 19 loss is 0.5818123642246609\nEpoch 29 loss is 0.5524255425898911\nEpoch 39 loss is 0.5331408636790176\nEpoch 49 loss is 0.5266223084583397\nEpoch 59 loss is 0.5084204272973452\nEpoch 69 loss is 0.5221864123225712\nEpoch 79 loss is 0.5321368323514395\nEpoch 89 loss is 0.514046891010893\nEpoch 99 loss is 0.5044625116486476\nEpoch 109 loss is 0.511035881480326\nEpoch 119 loss is 0.4235200404283216\nEpoch 129 loss is 0.4491360122023273\nEpoch 139 loss is 0.4383623463237392\nEpoch 149 loss is 0.4002990811088816\nEpoch 159 loss is 0.4750563005024748\nEpoch 169 loss is 0.49327760079269994\nEpoch 179 loss is 0.4378667111784329\nEpoch 189 loss is 0.4289851294534545\nEpoch 199 loss is 0.44141732628824326\nTrain Acc.:  0.6061486529792552\n              precision    recall  f1-score   support\n\n           0       0.74      0.96      0.83      1064\n           1       0.83      0.75      0.79      1064\n           2       0.86      0.61      0.71      1064\n           3       0.72      0.75      0.73      1063\n           4       0.93      0.09      0.16      1064\n           5       0.84      0.79      0.81      1064\n           6       0.90      0.22      0.36      1063\n           7       0.79      0.81      0.80      1064\n           8       0.98      0.98      0.98      1064\n           9       0.67      0.80      0.73      1064\n          10       0.84      0.37      0.51      1064\n\n   micro avg       0.80      0.65      0.72     11702\n   macro avg       0.83      0.65      0.68     11702\nweighted avg       0.83      0.65      0.68     11702\n samples avg       0.62      0.65      0.63     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5850991051083132\nEpoch 19 loss is 0.5139111042576086\nEpoch 29 loss is 0.5051983122342294\nEpoch 39 loss is 0.4890419324490173\nEpoch 49 loss is 0.4791715259888635\nEpoch 59 loss is 0.47829466650556235\nEpoch 69 loss is 0.5754105911410774\nEpoch 79 loss is 0.5353490414692079\nEpoch 89 loss is 0.5049187065218432\nEpoch 99 loss is 0.4971644238685353\nEpoch 109 loss is 0.5064778615772576\nEpoch 119 loss is 0.4903276163818602\nEpoch 129 loss is 0.5342514048885157\nEpoch 139 loss is 0.5082254582992605\nEpoch 149 loss is 0.5033399215026231\nEpoch 159 loss is 0.5054516411753327\nEpoch 169 loss is 0.49747021993098295\nEpoch 179 loss is 0.5080253523201269\nEpoch 189 loss is 0.5288063903208304\nEpoch 199 loss is 0.5138201494084611\nTrain Acc.:  0.4667250624906531\n              precision    recall  f1-score   support\n\n           0       0.87      0.89      0.88      1063\n           1       0.72      0.57      0.64      1064\n           2       0.92      0.27      0.42      1064\n           3       0.79      0.73      0.76      1063\n           4       0.72      0.46      0.56      1064\n           5       0.89      0.33      0.48      1064\n           6       0.00      0.00      0.00      1064\n           7       0.96      0.83      0.89      1064\n           8       1.00      0.98      0.99      1064\n           9       0.28      0.97      0.44      1064\n          10       0.72      0.53      0.61      1064\n\n   micro avg       0.65      0.60      0.62     11702\n   macro avg       0.72      0.60      0.60     11702\nweighted avg       0.71      0.60      0.60     11702\n samples avg       0.53      0.60      0.55     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.43746739830786185\nEpoch 19 loss is 0.5063128960291862\nEpoch 29 loss is 0.5443279274534242\nEpoch 39 loss is 0.5534543733837946\nEpoch 49 loss is 0.5142911469330737\nEpoch 59 loss is 0.48123787249991384\nEpoch 69 loss is 0.4897241118216021\nEpoch 79 loss is 0.486641422076389\nEpoch 89 loss is 0.4709927596896531\nEpoch 99 loss is 0.5305603540484453\nEpoch 109 loss is 0.5086426096428968\nEpoch 119 loss is 0.5261937886370114\nEpoch 129 loss is 0.5249747488613029\nEpoch 139 loss is 0.5480216689726441\nEpoch 149 loss is 0.5228087067398941\nEpoch 159 loss is 0.4817364265776811\nEpoch 169 loss is 0.4622485672858562\nEpoch 179 loss is 0.49610587677871815\nEpoch 189 loss is 0.4783234726880176\nEpoch 199 loss is 0.4741022823045306\nTrain Acc.:  0.5234900762706433\n              precision    recall  f1-score   support\n\n           0       0.87      0.88      0.88      1064\n           1       1.00      0.01      0.01      1064\n           2       0.82      0.16      0.26      1064\n           3       0.81      0.67      0.74      1064\n           4       0.76      0.28      0.41      1064\n           5       0.44      0.96      0.60      1064\n           6       0.69      0.06      0.12      1063\n           7       0.96      0.84      0.90      1064\n           8       1.00      0.96      0.98      1063\n           9       0.60      0.75      0.67      1064\n          10       0.69      0.70      0.69      1064\n\n   micro avg       0.71      0.57      0.63     11702\n   macro avg       0.79      0.57      0.57     11702\nweighted avg       0.79      0.57      0.57     11702\n samples avg       0.55      0.57      0.55     11702\n\n\n\n\ntitle = \"MLMVN 48-20-20-11\"\nimage_name = \"results/MLMVN_48-20-20-11.png\"\nplot_loss_acc_list(title, list_losses, list_scores, image_name)\n\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=2e5cbed78f624d038cdcebb083f902fd\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2e5cbed78f624d038cdcebb083f902fd/output/log\n======> WARNING! Git diff to large to store (3945kb), skipping uncommitted changes <======\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 12:31:47,473 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.18509080590032817\nEpoch 19 loss is 0.29851992221797385\nEpoch 29 loss is 0.38003938826666434\nEpoch 39 loss is 0.35283929485604354\nEpoch 49 loss is 0.36427067316960327\nEpoch 59 loss is 0.31956071771618505\nEpoch 69 loss is 0.2919854808615466\nEpoch 79 loss is 0.26728589118038387\nEpoch 89 loss is 0.2505744858820967\nEpoch 99 loss is 0.24366257508408273\nEpoch 109 loss is 0.2526296646400297\nEpoch 119 loss is 0.2543207117758536\nEpoch 129 loss is 0.24703290146744958\nEpoch 139 loss is 0.24847459500962163\nEpoch 149 loss is 0.24224065429443592\nEpoch 159 loss is 0.2280505035935983\nEpoch 169 loss is 0.22783374165114506\nEpoch 179 loss is 0.22949014620485136\nEpoch 189 loss is 0.2128073209289787\nEpoch 199 loss is 0.21034013310932373\nTrain Acc.:  0.8228683743884462\n              precision    recall  f1-score   support\n\n           0       0.94      0.90      0.92      1063\n           1       0.87      0.80      0.83      1064\n           2       0.95      0.87      0.91      1064\n           3       0.92      0.91      0.91      1064\n           4       0.86      0.73      0.79      1064\n           5       0.83      0.88      0.86      1063\n           6       0.81      0.76      0.78      1064\n           7       0.99      0.97      0.98      1064\n           8       1.00      0.99      0.99      1064\n           9       0.84      0.81      0.82      1064\n          10       0.89      0.86      0.88      1064\n\n   micro avg       0.90      0.86      0.88     11702\n   macro avg       0.90      0.86      0.88     11702\nweighted avg       0.90      0.86      0.88     11702\n samples avg       0.84      0.86      0.85     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2861229127408851\nEpoch 19 loss is 0.3707694820754066\nEpoch 29 loss is 0.4135629910389318\nEpoch 39 loss is 0.4015215646610006\nEpoch 49 loss is 0.3934166022651121\nEpoch 59 loss is 0.37682299911311523\nEpoch 69 loss is 0.32850649921225733\nEpoch 79 loss is 0.3117090487146507\nEpoch 89 loss is 0.3013186492740078\nEpoch 99 loss is 0.2815918980042734\nEpoch 109 loss is 0.3025606700072974\nEpoch 119 loss is 0.29640244765806867\nEpoch 129 loss is 0.2886136706741845\nEpoch 139 loss is 0.27816065918712707\nEpoch 149 loss is 0.27148853781337934\nEpoch 159 loss is 0.26770085066452964\nEpoch 169 loss is 0.2697538316258019\nEpoch 179 loss is 0.27578019618131644\nEpoch 189 loss is 0.26646575962929053\nEpoch 199 loss is 0.2633851934702109\nTrain Acc.:  0.7693934667891554\n              precision    recall  f1-score   support\n\n           0       0.92      0.87      0.90      1063\n           1       0.84      0.83      0.83      1064\n           2       0.92      0.84      0.88      1064\n           3       0.89      0.83      0.86      1063\n           4       0.76      0.71      0.73      1064\n           5       0.84      0.81      0.83      1064\n           6       0.73      0.75      0.74      1064\n           7       0.99      0.93      0.96      1064\n           8       1.00      0.99      0.99      1064\n           9       0.78      0.77      0.78      1064\n          10       0.86      0.85      0.86      1064\n\n   micro avg       0.87      0.83      0.85     11702\n   macro avg       0.87      0.83      0.85     11702\nweighted avg       0.87      0.83      0.85     11702\n samples avg       0.80      0.83      0.81     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2636942418729698\nEpoch 19 loss is 0.38913279833920067\nEpoch 29 loss is 0.41660029638624246\nEpoch 39 loss is 0.35837348257730234\nEpoch 49 loss is 0.32704220072461\nEpoch 59 loss is 0.3238192812282917\nEpoch 69 loss is 0.324375133254903\nEpoch 79 loss is 0.283519484376465\nEpoch 89 loss is 0.290315689826698\nEpoch 99 loss is 0.29113917867086464\nEpoch 109 loss is 0.3312690777595432\nEpoch 119 loss is 0.28558159575996994\nEpoch 129 loss is 0.27814503072941504\nEpoch 139 loss is 0.2974450427403483\nEpoch 149 loss is 0.27829655088323624\nEpoch 159 loss is 0.27569508470039195\nEpoch 169 loss is 0.25619125382733604\nEpoch 179 loss is 0.25419745505951113\nEpoch 189 loss is 0.2643537804062262\nEpoch 199 loss is 0.2618419861302584\nTrain Acc.:  0.788728181682227\n              precision    recall  f1-score   support\n\n           0       0.93      0.90      0.91      1064\n           1       0.87      0.77      0.82      1064\n           2       0.93      0.89      0.91      1064\n           3       0.88      0.86      0.87      1063\n           4       0.75      0.73      0.74      1064\n           5       0.83      0.85      0.84      1064\n           6       0.82      0.71      0.76      1063\n           7       0.98      0.97      0.97      1064\n           8       1.00      0.98      0.99      1064\n           9       0.81      0.79      0.80      1064\n          10       0.89      0.84      0.86      1064\n\n   micro avg       0.88      0.84      0.86     11702\n   macro avg       0.88      0.84      0.86     11702\nweighted avg       0.88      0.84      0.86     11702\n samples avg       0.81      0.84      0.82     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.24772645202023053\nEpoch 19 loss is 0.35750288399056496\nEpoch 29 loss is 0.3701326898119261\nEpoch 39 loss is 0.33407303916772696\nEpoch 49 loss is 0.3462574832072576\nEpoch 59 loss is 0.3339426384355099\nEpoch 69 loss is 0.33922922079491125\nEpoch 79 loss is 0.3259204646015445\nEpoch 89 loss is 0.3115701407476312\nEpoch 99 loss is 0.32205575641217443\nEpoch 109 loss is 0.29186489277176464\nEpoch 119 loss is 0.2695312388966853\nEpoch 129 loss is 0.2950951697843554\nEpoch 139 loss is 0.27228947845328294\nEpoch 149 loss is 0.27533878935326495\nEpoch 159 loss is 0.256029731536393\nEpoch 169 loss is 0.2520228613461403\nEpoch 179 loss is 0.2517227649665734\nEpoch 189 loss is 0.23340040093628597\nEpoch 199 loss is 0.2333106974808292\nTrain Acc.:  0.7876386010639435\n              precision    recall  f1-score   support\n\n           0       0.91      0.89      0.90      1063\n           1       0.82      0.84      0.83      1064\n           2       0.91      0.86      0.89      1064\n           3       0.93      0.85      0.89      1063\n           4       0.83      0.78      0.81      1064\n           5       0.84      0.80      0.82      1064\n           6       0.87      0.60      0.71      1064\n           7       0.99      0.96      0.97      1064\n           8       1.00      0.99      0.99      1064\n           9       0.84      0.81      0.82      1064\n          10       0.88      0.83      0.85      1064\n\n   micro avg       0.89      0.84      0.86     11702\n   macro avg       0.89      0.84      0.86     11702\nweighted avg       0.89      0.84      0.86     11702\n samples avg       0.81      0.84      0.82     11702\n\n\n\n/tmp/ipykernel_932/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3290784643476184\nEpoch 19 loss is 0.3882220039257218\nEpoch 29 loss is 0.3997899653239338\nEpoch 39 loss is 0.38339673065514107\nEpoch 49 loss is 0.3914353149120599\nEpoch 59 loss is 0.3413151340879377\nEpoch 69 loss is 0.3134185935040384\nEpoch 79 loss is 0.2860619695979416\nEpoch 89 loss is 0.2603699206305271\nEpoch 99 loss is 0.25138523657636386\nEpoch 109 loss is 0.23567740645017163\nEpoch 119 loss is 0.21597357797692088\nEpoch 129 loss is 0.20476071706230148\nEpoch 139 loss is 0.20329952377044547\nEpoch 149 loss is 0.20430155607077827\nEpoch 159 loss is 0.2098854033635211\nEpoch 169 loss is 0.2101066364317881\nEpoch 179 loss is 0.208051023591462\nEpoch 189 loss is 0.20806564248675852\nEpoch 199 loss is 0.2095600698320772\nTrain Acc.:  0.8154976819706454\n              precision    recall  f1-score   support\n\n           0       0.91      0.93      0.92      1064\n           1       0.92      0.64      0.75      1064\n           2       0.92      0.89      0.91      1064\n           3       0.95      0.87      0.91      1064\n           4       0.86      0.71      0.78      1064\n           5       0.74      0.93      0.82      1064\n           6       0.84      0.64      0.73      1063\n           7       0.99      0.99      0.99      1064\n           8       1.00      0.99      1.00      1063\n           9       0.84      0.82      0.83      1064\n          10       0.92      0.87      0.90      1064\n\n   micro avg       0.90      0.84      0.87     11702\n   macro avg       0.90      0.84      0.87     11702\nweighted avg       0.90      0.84      0.87     11702\n samples avg       0.82      0.84      0.83     11702\n\n\n\n\ntitle = \"MLMVN 48-50-50-11\"\nimage_name = \"results/MLMVN_48-50-50-11.png\"\nplot_loss_acc_list(title, list_losses, list_scores, image_name)\n\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=40045a36abde40619f35b7868e1213d1\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/40045a36abde40619f35b7868e1213d1/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-29 15:56:48,669 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.14245659853440193\nEpoch 19 loss is 0.11926958512560476\nEpoch 29 loss is 0.10088007783397561\nEpoch 39 loss is 0.10160706429771875\nEpoch 49 loss is 0.09256600861419079\nEpoch 59 loss is 0.10744608628004096\nEpoch 69 loss is 0.12774685299619692\nEpoch 79 loss is 0.12405285738088577\nEpoch 89 loss is 0.12148849057961768\nEpoch 99 loss is 0.17548190991717774\nEpoch 109 loss is 0.2519070374055556\nEpoch 119 loss is 0.29390135104918574\nEpoch 129 loss is 0.2631115029064872\nEpoch 139 loss is 0.26468762700088244\nEpoch 149 loss is 0.2426366788530615\nEpoch 159 loss is 0.24249734627332406\nEpoch 169 loss is 0.2180138422574741\nEpoch 179 loss is 0.1942677829861684\nEpoch 189 loss is 0.1656781252246518\nEpoch 199 loss is 0.15678583025914528\nTrain Acc.:  0.9119576131775162\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1063\n           1       0.89      0.91      0.90      1064\n           2       0.99      0.96      0.97      1064\n           3       0.94      0.95      0.95      1064\n           4       0.92      0.90      0.91      1064\n           5       0.91      0.90      0.91      1063\n           6       0.86      0.86      0.86      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.92      0.86      0.89      1064\n          10       0.96      0.93      0.95      1064\n\n   micro avg       0.94      0.93      0.93     11702\n   macro avg       0.94      0.93      0.93     11702\nweighted avg       0.94      0.93      0.93     11702\n samples avg       0.91      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13182902422312057\nEpoch 19 loss is 0.10948094408196307\nEpoch 29 loss is 0.11030647533056807\nEpoch 39 loss is 0.09747520923272845\nEpoch 49 loss is 0.10055963818000929\nEpoch 59 loss is 0.12781840344883313\nEpoch 69 loss is 0.16007342374600603\nEpoch 79 loss is 0.2685125349928008\nEpoch 89 loss is 0.2605283290643327\nEpoch 99 loss is 0.24953134086762976\nEpoch 109 loss is 0.26341047064393114\nEpoch 119 loss is 0.23938208227294375\nEpoch 129 loss is 0.22167797312644905\nEpoch 139 loss is 0.19953444239050008\nEpoch 149 loss is 0.20466815381159612\nEpoch 159 loss is 0.18944607768172028\nEpoch 169 loss is 0.17543960927604327\nEpoch 179 loss is 0.18857073919083642\nEpoch 189 loss is 0.1775085144108209\nEpoch 199 loss is 0.1846037562875263\nTrain Acc.:  0.9090947935137906\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.96      1063\n           1       0.89      0.88      0.89      1064\n           2       0.99      0.96      0.98      1064\n           3       0.98      0.97      0.97      1063\n           4       0.94      0.89      0.92      1064\n           5       0.91      0.87      0.89      1064\n           6       0.87      0.85      0.86      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      0.99      0.99      1064\n           9       0.94      0.88      0.91      1064\n          10       0.96      0.91      0.94      1064\n\n   micro avg       0.95      0.92      0.94     11702\n   macro avg       0.95      0.92      0.94     11702\nweighted avg       0.95      0.92      0.94     11702\n samples avg       0.91      0.92      0.91     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.14836475443880184\nEpoch 19 loss is 0.1144531471986026\nEpoch 29 loss is 0.11196463483836677\nEpoch 39 loss is 0.11829244064335528\nEpoch 49 loss is 0.2990447238807741\nEpoch 59 loss is 0.3416765423666923\nEpoch 69 loss is 0.3760340294767417\nEpoch 79 loss is 0.3602369454833449\nEpoch 89 loss is 0.3415423835663611\nEpoch 99 loss is 0.3017989133006243\nEpoch 109 loss is 0.2706555786076341\nEpoch 119 loss is 0.28536886623172014\nEpoch 129 loss is 0.25318045626204816\nEpoch 139 loss is 0.23099538649480147\nEpoch 149 loss is 0.20753359049453549\nEpoch 159 loss is 0.19738129756691591\nEpoch 169 loss is 0.17773918197037844\nEpoch 179 loss is 0.20320036291560054\nEpoch 189 loss is 0.1805354561882004\nEpoch 199 loss is 0.16590106544112354\nTrain Acc.:  0.889482342384686\n              precision    recall  f1-score   support\n\n           0       0.97      0.90      0.94      1064\n           1       0.91      0.86      0.88      1064\n           2       0.97      0.91      0.94      1064\n           3       0.98      0.96      0.97      1063\n           4       0.89      0.89      0.89      1064\n           5       0.90      0.91      0.90      1064\n           6       0.84      0.84      0.84      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.91      0.85      0.88      1064\n          10       0.97      0.93      0.95      1064\n\n   micro avg       0.94      0.91      0.93     11702\n   macro avg       0.94      0.91      0.93     11702\nweighted avg       0.94      0.91      0.93     11702\n samples avg       0.89      0.91      0.90     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.18644038213906955\nEpoch 19 loss is 0.12830064648067416\nEpoch 29 loss is 0.16682216144940276\nEpoch 39 loss is 0.1759418592172155\nEpoch 49 loss is 0.2644686708855199\nEpoch 59 loss is 0.3123614077032397\nEpoch 69 loss is 0.27872443139031733\nEpoch 79 loss is 0.29207407259361867\nEpoch 89 loss is 0.2838279987711244\nEpoch 99 loss is 0.24346773650683004\nEpoch 109 loss is 0.21714146376339183\nEpoch 119 loss is 0.19293183048183818\nEpoch 129 loss is 0.1947981148247875\nEpoch 139 loss is 0.17726155223340453\nEpoch 149 loss is 0.20668125270168672\nEpoch 159 loss is 0.18338413211908833\nEpoch 169 loss is 0.19277969415818375\nEpoch 179 loss is 0.16932003069773488\nEpoch 189 loss is 0.17114803008451324\nEpoch 199 loss is 0.16234674302244245\nTrain Acc.:  0.872796803896853\n              precision    recall  f1-score   support\n\n           0       0.96      0.95      0.96      1063\n           1       0.87      0.86      0.87      1064\n           2       0.97      0.92      0.95      1064\n           3       0.95      0.90      0.92      1063\n           4       0.94      0.88      0.91      1064\n           5       0.88      0.87      0.88      1064\n           6       0.89      0.74      0.81      1064\n           7       0.99      0.98      0.98      1064\n           8       1.00      0.99      1.00      1064\n           9       0.91      0.85      0.88      1064\n          10       0.95      0.89      0.92      1064\n\n   micro avg       0.94      0.89      0.92     11702\n   macro avg       0.94      0.89      0.92     11702\nweighted avg       0.94      0.89      0.92     11702\n samples avg       0.88      0.89      0.88     11702\n\n\n\n/tmp/ipykernel_1728/726702759.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13195299604718233\nEpoch 19 loss is 0.09760398376517222\nEpoch 29 loss is 0.09333988532269522\nEpoch 39 loss is 0.08521124567492029\nEpoch 49 loss is 0.09132043426181803\nEpoch 59 loss is 0.09214216010663917\nEpoch 69 loss is 0.0896809856068434\nEpoch 79 loss is 0.09808253427637235\nEpoch 89 loss is 0.1009467307297589\nEpoch 99 loss is 0.1341353638423057\nEpoch 109 loss is 0.2061791644495565\nEpoch 119 loss is 0.26550000244909344\nEpoch 129 loss is 0.26717282808322534\nEpoch 139 loss is 0.246903110038921\nEpoch 149 loss is 0.24215059780955484\nEpoch 159 loss is 0.2196847823727955\nEpoch 169 loss is 0.22964239756409188\nEpoch 179 loss is 0.21456982879325057\nEpoch 189 loss is 0.2175676527718779\nEpoch 199 loss is 0.20209335012773158\nTrain Acc.:  0.9208665370564232\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97      1064\n           1       0.93      0.88      0.91      1064\n           2       0.98      0.95      0.96      1064\n           3       0.98      0.96      0.97      1064\n           4       0.92      0.89      0.91      1064\n           5       0.91      0.91      0.91      1064\n           6       0.89      0.88      0.88      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      0.99      1.00      1063\n           9       0.91      0.90      0.91      1064\n          10       0.97      0.94      0.95      1064\n\n   micro avg       0.95      0.93      0.94     11702\n   macro avg       0.95      0.93      0.94     11702\nweighted avg       0.95      0.93      0.94     11702\n samples avg       0.92      0.93      0.92     11702\n\n\n\n\n# title = \"MLMVN 48-100-100-11\"\n# image_name = \"results/MLMVN_48-100-100-11.png\"\n# plot_loss_acc_list(title, list_losses, list_scores, image_name)\nplot_loss_acc_list(\n    \"$\\mathbb{C}$: [48-100-100-11] \",\n    list_losses,\n    list_scores,\n    \"mlmvn-48-100-100-11.png\",\n)\n\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_initializers_with_bias.html",
    "href": "examples/autass/autass_initializers_with_bias.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_initializers_with_bias.html#config",
    "href": "examples/autass/autass_initializers_with_bias.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 50\nbatch_size = 538\nlr = 1"
  },
  {
    "objectID": "examples/autass/autass_initializers_with_bias.html#single-layer",
    "href": "examples/autass/autass_initializers_with_bias.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n    def initialize_weights(self, initilizer=\"uniform\"):\n        if initilizer == \"uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_uniform_independent_(m.bias, -0.5, 0.5)\n        elif initilizer == \"normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_normal_independent_(\n                            m.bias,\n                        )\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_normal_independent_(\n                            m.bias,\n                        )\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_normal_independent_(\n                            m.bias,\n                        )\n        elif initilizer == \"ones\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"zeros\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"kaiming_normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_kaiming_normal_(m.bias)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_kaiming_normal_(m.bias)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_kaiming_normal_(m.bias)\n        elif initilizer == \"kaiming_uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_kaiming_uniform_(m.bias)\n        elif initilizer == \"xavier_normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_xavier_normal_(m.bias)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_xavier_normal_(m.bias)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_xavier_normal_(m.bias)\n        elif initilizer == \"xavier_uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_xavier_uniform_(m.bias)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_xavier_uniform_(m.bias)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_xavier_uniform_(m.bias)\n        elif initilizer == \"trabelsi_standard_glorot\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias)\n        elif initilizer == \"trabelsi_independent_glorot\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias)\n        elif initilizer == \"trabelsi_standard_xavier\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"xavier\")\n        elif initilizer == \"trabelsi_independent_xavier\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"xavier\")\n        elif initilizer == \"trabelsi_standard_kaiming\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"kaiming\")\n        elif initilizer == \"trabelsi_independent_kaiming\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"kaiming\")\n        elif initilizer == \"trabelsi_standard_he\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_standard_(m.bias, kind=\"he\")\n        elif initilizer == \"trabelsi_independent_he\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        # nn.init.constant_(m.bias,0)\n                        cmplx_init.cplx_trabelsi_independent_(m.bias, kind=\"he\")\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\n\nInit\n\ninitilizers = [\n    \"uniform\",\n    \"normal\",\n    # \"zeros\",\n    # \"ones\",\n    \"kaiming_normal\",\n    \"kaiming_uniform\",\n    \"xavier_normal\",\n    \"xavier_uniform\",\n    \"trabelsi_standard_glorot\",\n    \"trabelsi_independent_glorot\",\n    \"trabelsi_standard_xavier\",\n    \"trabelsi_independent_xavier\",\n    \"trabelsi_standard_kaiming\",\n    \"trabelsi_independent_kaiming\",\n    \"trabelsi_standard_he\",\n    \"trabelsi_independent_he\",\n]\n\n\nfor initilizer in initilizers:\n    model = Model(categories=categories, periodicity=periodicity)\n    model.initialize_weights(initilizer=initilizer)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    task = Task.init(\n        project_name=\"mlmvn\",\n        task_name=\"SDD-mlmvn-[48-100-11]\",\n        tags=[\"mlmvn\", \"SDD\", \"initilizer_with_bias\"],\n    )\n    writer = SummaryWriter()\n\n    #  capture a dictionary of hyperparameters with config\n    config_dict = {\n        \"learning_rate\": lr,\n        \"epochs\": epochs,\n        \"batch_size\": batch_size,\n        \"optim\": \"ECL\",\n        \"categories\": categories,\n        \"periodicity\": periodicity,\n        \"layer\": \"[48-100-11]\",\n        \"initilizer\": initilizer,\n    }\n    task.connect(config_dict)\n\n    x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n    Logger.current_logger().report_single_value(\n        name=\"Train Acc.\",\n        value=acc,\n    )\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    print(\"Val Acc.: \", acc)\n    Logger.current_logger().report_single_value(\n        name=\"Val Acc.\",\n        value=acc,\n    )\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n    task.mark_completed()\n    task.close()\n\nClearML Task: created new task id=8c4dc60250654161a4455e9fb49f6061\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/8c4dc60250654161a4455e9fb49f6061/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 20:03:11,212 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.13520533966014758\nEpoch 19 loss is 0.09242117989507001\nEpoch 29 loss is 0.06890200001316225\nEpoch 39 loss is 0.06940312129715898\nEpoch 49 loss is 0.0739187122910833\nTrain Acc.:  0.9514164850660172\nVal Acc.:  0.9457404084422798\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1074\n           1       0.95      0.91      0.93      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.98      0.98      1048\n           4       0.95      0.96      0.96      1057\n           5       0.93      0.94      0.94      1072\n           6       0.93      0.90      0.92      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.93      0.95      1030\n          10       0.99      0.96      0.98      1012\n\n   micro avg       0.97      0.96      0.96     11703\n   macro avg       0.97      0.96      0.96     11703\nweighted avg       0.97      0.96      0.96     11703\n samples avg       0.95      0.96      0.95     11703\n\nClearML Task: created new task id=671fc38623e64b04b2183783d6660d82\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/671fc38623e64b04b2183783d6660d82/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.6364161542642706\nEpoch 19 loss is 0.4933384342885658\nEpoch 29 loss is 0.2978411113611267\nEpoch 39 loss is 0.17510296008646237\nEpoch 49 loss is 0.136787017185157\nTrain Acc.:  0.8634576763662778\nVal Acc.:  0.8490985217465608\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95      1074\n           1       0.87      0.85      0.86      1089\n           2       0.93      0.84      0.88      1044\n           3       0.96      0.92      0.94      1048\n           4       0.90      0.84      0.87      1057\n           5       0.89      0.85      0.87      1072\n           6       0.89      0.83      0.86      1066\n           7       0.99      0.99      0.99      1103\n           8       0.98      1.00      0.99      1108\n           9       0.91      0.91      0.91      1030\n          10       0.89      0.77      0.83      1012\n\n   micro avg       0.92      0.89      0.91     11703\n   macro avg       0.92      0.89      0.90     11703\nweighted avg       0.92      0.89      0.91     11703\n samples avg       0.87      0.89      0.88     11703\n\nClearML Task: created new task id=cbd1148dc06c4bb484e287631dbecfe1\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/cbd1148dc06c4bb484e287631dbecfe1/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13064789010576158\nEpoch 19 loss is 0.08065432691827415\nEpoch 29 loss is 0.07580616387918908\nEpoch 39 loss is 0.06079223146693188\nEpoch 49 loss is 0.05756999806851636\nTrain Acc.:  0.9545784728453617\nVal Acc.:  0.9430915149961548\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1074\n           1       0.95      0.94      0.95      1089\n           2       0.99      0.97      0.98      1044\n           3       0.99      0.98      0.98      1048\n           4       0.94      0.94      0.94      1057\n           5       0.95      0.94      0.94      1072\n           6       0.94      0.92      0.93      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.95      0.93      0.94      1030\n          10       0.99      0.94      0.96      1012\n\n   micro avg       0.97      0.96      0.96     11703\n   macro avg       0.97      0.96      0.96     11703\nweighted avg       0.97      0.96      0.96     11703\n samples avg       0.95      0.96      0.95     11703\n\nClearML Task: created new task id=24fb62f34d6045dc84880b3cdb4ff51e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/24fb62f34d6045dc84880b3cdb4ff51e/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13044388983633648\nEpoch 19 loss is 0.08460253840021804\nEpoch 29 loss is 0.07930393311991951\nEpoch 39 loss is 0.06527583675469127\nEpoch 49 loss is 0.05000301778597409\nTrain Acc.:  0.9599196684185788\nVal Acc.:  0.948645646415449\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1074\n           1       0.95      0.94      0.95      1089\n           2       1.00      0.98      0.99      1044\n           3       0.99      0.98      0.98      1048\n           4       0.95      0.96      0.95      1057\n           5       0.96      0.94      0.95      1072\n           6       0.93      0.92      0.92      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.95      0.94      1030\n          10       0.99      0.96      0.98      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\nClearML Task: created new task id=76f869fd23be4c139728384b8cde4c7a\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/76f869fd23be4c139728384b8cde4c7a/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09251221922659182\nEpoch 19 loss is 0.06227124288757633\nEpoch 29 loss is 0.059843142229535094\nEpoch 39 loss is 0.06101339361081321\nEpoch 49 loss is 0.05014295614441572\nTrain Acc.:  0.9669059522283467\nVal Acc.:  0.9546270187131505\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.93      0.93      0.93      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.98      0.98      1048\n           4       0.97      0.95      0.96      1057\n           5       0.94      0.93      0.93      1072\n           6       0.96      0.94      0.95      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.97      0.96      0.96      1030\n          10       0.99      0.98      0.98      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\nClearML Task: created new task id=2f0b154634c74df4adf05315bed073df\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2f0b154634c74df4adf05315bed073df/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.10127652630034135\nEpoch 19 loss is 0.10470796996730673\nEpoch 29 loss is 0.061063666924023825\nEpoch 39 loss is 0.06377833585016535\nEpoch 49 loss is 0.046801240935354885\nTrain Acc.:  0.9597487501602359\nVal Acc.:  0.9475348201315902\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.95      0.93      0.94      1089\n           2       1.00      0.98      0.99      1044\n           3       0.99      0.98      0.98      1048\n           4       0.96      0.96      0.96      1057\n           5       0.94      0.94      0.94      1072\n           6       0.95      0.93      0.94      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.95      0.93      0.94      1030\n          10       0.99      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.95      0.96      0.96     11703\n\nClearML Task: created new task id=cf0a0472af3947029f29e2ec9a6d83ae\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/cf0a0472af3947029f29e2ec9a6d83ae/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09907775976141676\nEpoch 19 loss is 0.07054048413195367\nEpoch 29 loss is 0.05308170731832958\nEpoch 39 loss is 0.049473569621797074\nEpoch 49 loss is 0.04445638241988443\nTrain Acc.:  0.9670127761398112\nVal Acc.:  0.9555669486456464\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.96      0.94      0.95      1089\n           2       1.00      0.99      0.99      1044\n           3       0.99      0.97      0.98      1048\n           4       0.96      0.95      0.96      1057\n           5       0.96      0.94      0.95      1072\n           6       0.97      0.91      0.94      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.96      0.96      1030\n          10       0.99      0.96      0.98      1012\n\n   micro avg       0.98      0.96      0.97     11703\n   macro avg       0.98      0.96      0.97     11703\nweighted avg       0.98      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\nClearML Task: created new task id=33833dedc47446eeb1262956c279de8b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/33833dedc47446eeb1262956c279de8b/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09330733755305037\nEpoch 19 loss is 0.07534531581462517\nEpoch 29 loss is 0.05881492036979832\nEpoch 39 loss is 0.05424361085964104\nEpoch 49 loss is 0.0515420224426341\nTrain Acc.:  0.9599837627654574\nVal Acc.:  0.9484747500640861\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1074\n           1       0.94      0.94      0.94      1089\n           2       0.99      0.98      0.98      1044\n           3       0.99      0.98      0.98      1048\n           4       0.95      0.95      0.95      1057\n           5       0.94      0.94      0.94      1072\n           6       0.94      0.93      0.93      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.95      0.95      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\nClearML Task: created new task id=d2a3805e299c4aabb89fc97016b870fe\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d2a3805e299c4aabb89fc97016b870fe/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09907775976141676\nEpoch 19 loss is 0.07054048413195367\nEpoch 29 loss is 0.05308170731832958\nEpoch 39 loss is 0.049473569621797074\nEpoch 49 loss is 0.04445638241988443\nTrain Acc.:  0.9670127761398112\nVal Acc.:  0.9555669486456464\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.96      0.94      0.95      1089\n           2       1.00      0.99      0.99      1044\n           3       0.99      0.97      0.98      1048\n           4       0.96      0.95      0.96      1057\n           5       0.96      0.94      0.95      1072\n           6       0.97      0.91      0.94      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.96      0.96      1030\n          10       0.99      0.96      0.98      1012\n\n   micro avg       0.98      0.96      0.97     11703\n   macro avg       0.98      0.96      0.97     11703\nweighted avg       0.98      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\nClearML Task: created new task id=71209c3ecb8f4c288e9ae2fc47316b43\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/71209c3ecb8f4c288e9ae2fc47316b43/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.09330733755305037\nEpoch 19 loss is 0.07534531581462517\nEpoch 29 loss is 0.05881492036979832\nEpoch 39 loss is 0.05424361085964104\nEpoch 49 loss is 0.0515420224426341\nTrain Acc.:  0.9599837627654574\nVal Acc.:  0.9484747500640861\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1074\n           1       0.94      0.94      0.94      1089\n           2       0.99      0.98      0.98      1044\n           3       0.99      0.98      0.98      1048\n           4       0.95      0.95      0.95      1057\n           5       0.94      0.94      0.94      1072\n           6       0.94      0.93      0.93      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.95      0.95      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\nClearML Task: created new task id=0d5788d998d14b34b4db688e7e454039\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/0d5788d998d14b34b4db688e7e454039/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.12687696724868183\nEpoch 19 loss is 0.08206688501021006\nEpoch 29 loss is 0.0666472617605075\nEpoch 39 loss is 0.05526746285190691\nEpoch 49 loss is 0.05272579846101851\nTrain Acc.:  0.9609024484040508\nVal Acc.:  0.9495855763479449\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.96      0.91      0.93      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.97      0.98      1048\n           4       0.95      0.94      0.94      1057\n           5       0.94      0.94      0.94      1072\n           6       0.97      0.93      0.95      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.96      0.96      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.95      0.96      0.96     11703\n\nClearML Task: created new task id=dbd3f157afbf49a89e470c1f289309a7\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/dbd3f157afbf49a89e470c1f289309a7/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.12607126522572562\nEpoch 19 loss is 0.08197840312344751\nEpoch 29 loss is 0.06853510060918785\nEpoch 39 loss is 0.06352552227624415\nEpoch 49 loss is 0.05255373344230446\nTrain Acc.:  0.9568431397684057\nVal Acc.:  0.9483893018884046\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1074\n           1       0.96      0.92      0.94      1089\n           2       1.00      0.98      0.99      1044\n           3       0.98      0.97      0.98      1048\n           4       0.96      0.95      0.95      1057\n           5       0.96      0.94      0.95      1072\n           6       0.96      0.92      0.94      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.94      0.94      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.98      0.96      0.97     11703\n   macro avg       0.98      0.96      0.97     11703\nweighted avg       0.98      0.96      0.97     11703\n samples avg       0.95      0.96      0.96     11703\n\nClearML Task: created new task id=c9f5e2d5eae84023829ccbb07f66f1c6\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/c9f5e2d5eae84023829ccbb07f66f1c6/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.12687696724868183\nEpoch 19 loss is 0.08206688501021006\nEpoch 29 loss is 0.0666472617605075\nEpoch 39 loss is 0.05526746285190691\nEpoch 49 loss is 0.05272579846101851\nTrain Acc.:  0.9609024484040508\nVal Acc.:  0.9495855763479449\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.96      0.91      0.93      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.97      0.98      1048\n           4       0.95      0.94      0.94      1057\n           5       0.94      0.94      0.94      1072\n           6       0.97      0.93      0.95      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.96      0.96      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.95      0.96      0.96     11703\n\nClearML Task: created new task id=18941c1c20824769905082794a13375e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/18941c1c20824769905082794a13375e/output/log\n\n\n/tmp/ipykernel_21707/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.12607126522572562\nEpoch 19 loss is 0.08197840312344751\nEpoch 29 loss is 0.06853510060918785\nEpoch 39 loss is 0.06352552227624415\nEpoch 49 loss is 0.05255373344230446\nTrain Acc.:  0.9568431397684057\nVal Acc.:  0.9483893018884046\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1074\n           1       0.96      0.92      0.94      1089\n           2       1.00      0.98      0.99      1044\n           3       0.98      0.97      0.98      1048\n           4       0.96      0.95      0.95      1057\n           5       0.96      0.94      0.95      1072\n           6       0.96      0.92      0.94      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.94      0.94      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.98      0.96      0.97     11703\n   macro avg       0.98      0.96      0.97     11703\nweighted avg       0.98      0.96      0.97     11703\n samples avg       0.95      0.96      0.96     11703"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip.html",
    "href": "examples/autass/autass_single_run_angle_clip.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip.html#config",
    "href": "examples/autass/autass_single_run_angle_clip.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1\nclip_angle_value = 1000000"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip.html#single-layer",
    "href": "examples/autass/autass_single_run_angle_clip.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=eed877f747d14293941d1d4c59755d31\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/eed877f747d14293941d1d4c59755d31/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 19:16:50,127 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.3667091786018161\nEpoch 19 loss is 0.41173681805768103\nEpoch 29 loss is 0.45076533902987087\nEpoch 39 loss is 0.4466027494390662\nEpoch 49 loss is 0.44763459170465236\nEpoch 59 loss is 0.4533707726301364\nEpoch 69 loss is 0.4501369969579344\nEpoch 79 loss is 0.4513209000941288\nEpoch 89 loss is 0.44964272415565953\nEpoch 99 loss is 0.4472901870680344\nEpoch 109 loss is 0.457084284038417\nEpoch 119 loss is 0.4590235278093013\nEpoch 129 loss is 0.46084049332369004\nEpoch 139 loss is 0.4654733920364203\nEpoch 149 loss is 0.46838660731228465\nEpoch 159 loss is 0.47493699919424265\nEpoch 169 loss is 0.4439981988066279\nEpoch 179 loss is 0.44147288396015333\nEpoch 189 loss is 0.44012757789164497\nEpoch 199 loss is 0.4825500314273581\nTrain Acc.:  0.6814938255779174\nVal Acc.:  0.6834999572759122\n              precision    recall  f1-score   support\n\n           0       0.80      0.96      0.88      1074\n           1       0.83      0.71      0.76      1089\n           2       0.90      0.78      0.84      1044\n           3       0.75      0.81      0.78      1048\n           4       0.95      0.15      0.27      1057\n           5       0.76      0.77      0.76      1072\n           6       0.70      0.40      0.51      1066\n           7       0.99      0.97      0.98      1103\n           8       1.00      1.00      1.00      1108\n           9       0.79      0.68      0.73      1030\n          10       0.84      0.81      0.82      1012\n\n   micro avg       0.84      0.73      0.78     11703\n   macro avg       0.85      0.73      0.76     11703\nweighted avg       0.85      0.73      0.76     11703\n samples avg       0.71      0.73      0.72     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=ff4ac1618d7741d28dce4adf5f657270\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/ff4ac1618d7741d28dce4adf5f657270/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 19:19:59,984 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.15915863978261413\nEpoch 19 loss is 0.1538296456538281\nEpoch 29 loss is 0.16869983855980936\nEpoch 39 loss is 0.20823938446270757\nEpoch 49 loss is 0.19376113452791796\nEpoch 59 loss is 0.1911997860728807\nEpoch 69 loss is 0.20126370426733936\nEpoch 79 loss is 0.1990600257416834\nEpoch 89 loss is 0.1981015378481079\nEpoch 99 loss is 0.19735627565076824\nEpoch 109 loss is 0.1970551747579205\nEpoch 119 loss is 0.19676018172952162\nEpoch 129 loss is 0.19637178458667698\nEpoch 139 loss is 0.1960267613935089\nEpoch 149 loss is 0.1960906102544728\nEpoch 159 loss is 0.19589580950040747\nEpoch 169 loss is 0.19579449202804897\nEpoch 179 loss is 0.19579767062563044\nEpoch 189 loss is 0.19586095329122385\nEpoch 199 loss is 0.19593349538483032\nTrain Acc.:  0.8697602871426741\nVal Acc.:  0.8605485772878749\n              precision    recall  f1-score   support\n\n           0       0.95      0.95      0.95      1074\n           1       0.91      0.80      0.85      1089\n           2       0.94      0.92      0.93      1044\n           3       0.93      0.94      0.93      1048\n           4       0.90      0.80      0.85      1057\n           5       0.84      0.89      0.87      1072\n           6       0.84      0.71      0.77      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.82      0.89      0.85      1030\n          10       0.94      0.90      0.92      1012\n\n   micro avg       0.92      0.89      0.90     11703\n   macro avg       0.92      0.89      0.90     11703\nweighted avg       0.92      0.89      0.90     11703\n samples avg       0.88      0.89      0.88     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=1916af932eaa4601b9a0207100456fb5\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1916af932eaa4601b9a0207100456fb5/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 19:28:37,693 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.10994139911930761\nEpoch 19 loss is 0.08976162337749397\nEpoch 29 loss is 0.07983302634388777\nEpoch 39 loss is 0.07148470195280872\nEpoch 49 loss is 0.06555567246210765\nEpoch 59 loss is 0.07427495906935433\nEpoch 69 loss is 0.06748365287950181\nEpoch 79 loss is 0.06323275339633817\nEpoch 89 loss is 0.061583781833071546\nEpoch 99 loss is 0.07261522340433889\nEpoch 109 loss is 0.09177229207035358\nEpoch 119 loss is 0.09549642858542892\nEpoch 129 loss is 0.104507046857026\nEpoch 139 loss is 0.09946573986720618\nEpoch 149 loss is 0.11588957238736819\nEpoch 159 loss is 0.12785879042298928\nEpoch 169 loss is 0.1736157925207539\nEpoch 179 loss is 0.17378276686611133\nEpoch 189 loss is 0.17053030474324235\nEpoch 199 loss is 0.17043714903871385\nTrain Acc.:  0.9441951886510277\nVal Acc.:  0.9367683499957276\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      1074\n           1       0.94      0.93      0.93      1089\n           2       0.98      0.97      0.97      1044\n           3       0.99      0.97      0.98      1048\n           4       0.95      0.93      0.94      1057\n           5       0.95      0.93      0.94      1072\n           6       0.94      0.92      0.93      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.93      0.93      1030\n          10       0.97      0.97      0.97      1012\n\n   micro avg       0.96      0.96      0.96     11703\n   macro avg       0.96      0.96      0.96     11703\nweighted avg       0.96      0.96      0.96     11703\n samples avg       0.95      0.96      0.95     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=7b3abe9fb989495e892c444570be6067\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/7b3abe9fb989495e892c444570be6067/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 19:35:24,268 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.09364351451345408\nEpoch 19 loss is 0.06047805760727481\nEpoch 29 loss is 0.0578744128569904\nEpoch 39 loss is 0.04534195824524318\nEpoch 49 loss is 0.04307949224899972\nEpoch 59 loss is 0.041017087963280054\nEpoch 69 loss is 0.03752497958328672\nEpoch 79 loss is 0.03929154410570669\nEpoch 89 loss is 0.03530748449555099\nEpoch 99 loss is 0.03361906166904868\nEpoch 109 loss is 0.03500346362768258\nEpoch 119 loss is 0.03421256406705508\nEpoch 129 loss is 0.03967580897383263\nEpoch 139 loss is 0.03983386888202658\nEpoch 149 loss is 0.035099183909890475\nEpoch 159 loss is 0.039366477472969064\nEpoch 169 loss is 0.039393422961427775\nEpoch 179 loss is 0.03634864197734159\nEpoch 189 loss is 0.03297036735253193\nEpoch 199 loss is 0.03543338693745545\nTrain Acc.:  0.9744477203777293\nVal Acc.:  0.9567632231051867\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.97      0.97      0.97      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.98      0.98      1048\n           4       0.95      0.96      0.95      1057\n           5       0.97      0.96      0.96      1072\n           6       0.95      0.95      0.95      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.97      0.95      0.96      1030\n          10       0.99      0.96      0.97      1012\n\n   micro avg       0.98      0.97      0.97     11703\n   macro avg       0.98      0.97      0.97     11703\nweighted avg       0.98      0.97      0.97     11703\n samples avg       0.96      0.97      0.97     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip.html#multi-layer",
    "href": "examples/autass/autass_single_run_angle_clip.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=9f8e1bb1a8ce4055b17fd15ff8f5f96e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/9f8e1bb1a8ce4055b17fd15ff8f5f96e/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 19:47:37,678 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.3190616793468566\nEpoch 19 loss is 0.3316516310882961\nEpoch 29 loss is 0.3159845589401083\nEpoch 39 loss is 0.2897936458677712\nEpoch 49 loss is 0.29438441483606387\nEpoch 59 loss is 0.29755967444288156\nEpoch 69 loss is 0.2976103369922987\nEpoch 79 loss is 0.3521991805852908\nEpoch 89 loss is 0.3175364303332033\nEpoch 99 loss is 0.3012195794930889\nEpoch 109 loss is 0.3188232513425489\nEpoch 119 loss is 0.32362149784936967\nEpoch 129 loss is 0.3005814172908205\nEpoch 139 loss is 0.2813095426634295\nEpoch 149 loss is 0.26490572151648945\nEpoch 159 loss is 0.311720634966309\nEpoch 169 loss is 0.2963948818829981\nEpoch 179 loss is 0.2921389037327001\nEpoch 189 loss is 0.3533133125418626\nEpoch 199 loss is 0.3518870098072943\nTrain Acc.:  0.7671666025723198\nVal Acc.:  0.7652738614030591\n              precision    recall  f1-score   support\n\n           0       0.86      0.90      0.88      1074\n           1       0.86      0.83      0.84      1089\n           2       0.94      0.84      0.89      1044\n           3       0.92      0.76      0.84      1048\n           4       0.88      0.49      0.63      1057\n           5       0.80      0.86      0.83      1072\n           6       0.76      0.53      0.63      1066\n           7       0.99      0.99      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.84      0.75      0.79      1030\n          10       0.70      0.91      0.79      1012\n\n   micro avg       0.87      0.81      0.84     11703\n   macro avg       0.87      0.81      0.83     11703\nweighted avg       0.87      0.81      0.83     11703\n samples avg       0.79      0.81      0.79     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=1b1a786365114896aa361f7567b2a590\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1b1a786365114896aa361f7567b2a590/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 19:56:38,160 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.22423453263455123\nEpoch 19 loss is 0.2417288273187978\nEpoch 29 loss is 0.24098897281491427\nEpoch 39 loss is 0.25785384004717277\nEpoch 49 loss is 0.20411375730867218\nEpoch 59 loss is 0.19907421010282828\nEpoch 69 loss is 0.21536965079441342\nEpoch 79 loss is 0.21283156041274492\nEpoch 89 loss is 0.20860426362519324\nEpoch 99 loss is 0.1986070523113828\nEpoch 109 loss is 0.21692347488143257\nEpoch 119 loss is 0.23349276288092297\nEpoch 129 loss is 0.21625116723365828\nEpoch 139 loss is 0.23410189520133975\nEpoch 149 loss is 0.23888556577670042\nEpoch 159 loss is 0.23277480097426984\nEpoch 169 loss is 0.20190034595150705\nEpoch 179 loss is 0.20968810270215624\nEpoch 189 loss is 0.2009444427805874\nEpoch 199 loss is 0.2018091584845151\nTrain Acc.:  0.8408537367004231\nVal Acc.:  0.8437152866786294\n              precision    recall  f1-score   support\n\n           0       0.89      0.96      0.92      1074\n           1       0.88      0.87      0.88      1089\n           2       0.93      0.88      0.90      1044\n           3       0.94      0.94      0.94      1048\n           4       0.88      0.66      0.75      1057\n           5       0.90      0.87      0.88      1072\n           6       0.81      0.65      0.72      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.86      0.87      0.86      1030\n          10       0.90      0.85      0.87      1012\n\n   micro avg       0.91      0.87      0.89     11703\n   macro avg       0.91      0.87      0.88     11703\nweighted avg       0.91      0.87      0.89     11703\n samples avg       0.86      0.87      0.86     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=88a9a42920894ee480fda889218b6295\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/88a9a42920894ee480fda889218b6295/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 20:06:38,774 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.12497814221423068\nEpoch 19 loss is 0.11329098739263005\nEpoch 29 loss is 0.1134584198279062\nEpoch 39 loss is 0.10747780190630066\nEpoch 49 loss is 0.10495547213762008\nEpoch 59 loss is 0.10659716730495539\nEpoch 69 loss is 0.09905256375984144\nEpoch 79 loss is 0.09258128419659628\nEpoch 89 loss is 0.08945538147457059\nEpoch 99 loss is 0.10534673199439117\nEpoch 109 loss is 0.09873397783198988\nEpoch 119 loss is 0.0957623519177352\nEpoch 129 loss is 0.1089078274844085\nEpoch 139 loss is 0.10761467200030443\nEpoch 149 loss is 0.0974923953101792\nEpoch 159 loss is 0.10399220587872246\nEpoch 169 loss is 0.11563031956286923\nEpoch 179 loss is 0.1356005221774724\nEpoch 189 loss is 0.11880395204472788\nEpoch 199 loss is 0.11979121066603561\nTrain Acc.:  0.934709225312994\nVal Acc.:  0.9241220199948731\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      1074\n           1       0.93      0.90      0.91      1089\n           2       0.99      0.97      0.98      1044\n           3       0.98      0.97      0.97      1048\n           4       0.94      0.89      0.92      1057\n           5       0.90      0.90      0.90      1072\n           6       0.89      0.87      0.88      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.90      0.91      0.91      1030\n          10       0.98      0.95      0.96      1012\n\n   micro avg       0.95      0.94      0.95     11703\n   macro avg       0.95      0.94      0.95     11703\nweighted avg       0.95      0.94      0.95     11703\n samples avg       0.93      0.94      0.93     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=d9b3caa55d294da8b5913a5161413f86\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d9b3caa55d294da8b5913a5161413f86/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_2584/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 20:18:40,249 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.13289887250103694\nEpoch 19 loss is 0.0969205175520467\nEpoch 29 loss is 0.0876257468319937\nEpoch 39 loss is 0.09244551632670642\nEpoch 49 loss is 0.06913963450583839\nEpoch 59 loss is 0.05862448278693276\nEpoch 69 loss is 0.06090114262274447\nEpoch 79 loss is 0.05708236092705054\nEpoch 89 loss is 0.056327734121452935\nEpoch 99 loss is 0.058758856743251306\nEpoch 109 loss is 0.05308146111475717\nEpoch 119 loss is 0.05663969716960362\nEpoch 129 loss is 0.050931966043855297\nEpoch 139 loss is 0.0470478870489573\nEpoch 149 loss is 0.04390343749136136\nEpoch 159 loss is 0.0413619943037058\nEpoch 169 loss is 0.04258073464153097\nEpoch 179 loss is 0.044157359217192584\nEpoch 189 loss is 0.04075450701217185\nEpoch 199 loss is 0.03822650540962055\nTrain Acc.:  0.9713711917275563\nVal Acc.:  0.9542852260104246\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.96      0.95      0.95      1089\n           2       0.99      0.98      0.99      1044\n           3       0.99      0.98      0.98      1048\n           4       0.96      0.95      0.96      1057\n           5       0.95      0.95      0.95      1072\n           6       0.94      0.92      0.93      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.95      0.94      1030\n          10       0.99      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_pa_loss.html",
    "href": "examples/autass/autass_single_run_angle_pa_loss.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_pa_loss.html#config",
    "href": "examples/autass/autass_single_run_angle_pa_loss.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_pa_loss.html#single-layer",
    "href": "examples/autass/autass_single_run_angle_pa_loss.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=24f4db1bc3534991ae833c133d9f8d2d\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/24f4db1bc3534991ae833c133d9f8d2d/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 20:34:31,569 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.24948623262964803\nEpoch 19 loss is 0.255540866164878\nEpoch 29 loss is 0.21773538117269958\nEpoch 39 loss is 0.21244534374484728\nEpoch 49 loss is 0.2473424009802358\nEpoch 59 loss is 0.20695948558149752\nEpoch 69 loss is 0.29476683270052434\nEpoch 79 loss is 0.24103331229757735\nEpoch 89 loss is 0.27724582003427484\nEpoch 99 loss is 0.2970595447560823\nEpoch 109 loss is 0.3243128357279459\nEpoch 119 loss is 0.39155765239889734\nEpoch 129 loss is 0.4004725978900191\nEpoch 139 loss is 0.3923780522292057\nEpoch 149 loss is 0.32839938368714444\nEpoch 159 loss is 0.28364692218207543\nEpoch 169 loss is 0.27329238707230713\nEpoch 179 loss is 0.23872938813414282\nEpoch 189 loss is 0.24975022028484206\nEpoch 199 loss is 0.23256343403758803\nTrain Acc.:  0.8357475537324275\nVal Acc.:  0.8311544048534564\n              precision    recall  f1-score   support\n\n           0       0.93      0.94      0.94      1074\n           1       0.92      0.83      0.87      1089\n           2       0.92      0.97      0.94      1044\n           3       0.95      0.94      0.95      1048\n           4       0.76      0.67      0.71      1057\n           5       0.89      0.89      0.89      1072\n           6       0.73      0.75      0.74      1066\n           7       0.99      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.85      0.78      0.82      1030\n          10       0.85      0.83      0.84      1012\n\n   micro avg       0.89      0.87      0.88     11703\n   macro avg       0.89      0.87      0.88     11703\nweighted avg       0.89      0.87      0.88     11703\n samples avg       0.85      0.87      0.86     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=9dcf5016dc4b4a7498fe8d6d4aeccbc9\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/9dcf5016dc4b4a7498fe8d6d4aeccbc9/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 20:37:36,382 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.24476062344530095\nEpoch 19 loss is 0.17524997121406452\nEpoch 29 loss is 0.17326589625589803\nEpoch 39 loss is 0.1250403376833135\nEpoch 49 loss is 0.10869102016395354\nEpoch 59 loss is 0.12357040169954497\nEpoch 69 loss is 0.1093577461378208\nEpoch 79 loss is 0.12643380921745745\nEpoch 89 loss is 0.11328518219237213\nEpoch 99 loss is 0.10813876603811017\nEpoch 109 loss is 0.10728799886326053\nEpoch 119 loss is 0.09921635073504151\nEpoch 129 loss is 0.1086356377434932\nEpoch 139 loss is 0.10239754285657386\nEpoch 149 loss is 0.09024639101528273\nEpoch 159 loss is 0.10786458972685774\nEpoch 169 loss is 0.10565815323356467\nEpoch 179 loss is 0.10199340886787205\nEpoch 189 loss is 0.10471597717547888\nEpoch 199 loss is 0.10269092835171191\nTrain Acc.:  0.9379994017860958\nVal Acc.:  0.9320687003332478\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1074\n           1       0.95      0.92      0.93      1089\n           2       0.99      0.97      0.98      1044\n           3       0.97      0.97      0.97      1048\n           4       0.96      0.93      0.95      1057\n           5       0.91      0.91      0.91      1072\n           6       0.92      0.89      0.90      1066\n           7       1.00      0.99      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.95      0.92      0.93      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.96      0.95      0.95     11703\n   macro avg       0.96      0.95      0.95     11703\nweighted avg       0.96      0.95      0.95     11703\n samples avg       0.94      0.95      0.94     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=6e46e1426f0545abadae1da64a699266\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6e46e1426f0545abadae1da64a699266/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 20:43:58,494 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.23981090673183392\nEpoch 19 loss is 0.1350937123107344\nEpoch 29 loss is 0.10171720961260702\nEpoch 39 loss is 0.10397829965296723\nEpoch 49 loss is 0.09251700067492151\nEpoch 59 loss is 0.07677589171885311\nEpoch 69 loss is 0.07428011077625409\nEpoch 79 loss is 0.07424593369765878\nEpoch 89 loss is 0.06974386731484813\nEpoch 99 loss is 0.0625939765088816\nEpoch 109 loss is 0.06981443338641498\nEpoch 119 loss is 0.06748007620098263\nEpoch 129 loss is 0.05657005163013897\nEpoch 139 loss is 0.06359593600342729\nEpoch 149 loss is 0.05271322280799125\nEpoch 159 loss is 0.046950683837239236\nEpoch 169 loss is 0.045860277729382115\nEpoch 179 loss is 0.041940594701453865\nEpoch 189 loss is 0.042831541937416555\nEpoch 199 loss is 0.04799205469350261\nTrain Acc.:  0.9769687646882879\nVal Acc.:  0.9669315560112791\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1074\n           1       0.96      0.96      0.96      1089\n           2       1.00      0.99      0.99      1044\n           3       0.99      0.98      0.98      1048\n           4       0.96      0.98      0.97      1057\n           5       0.96      0.96      0.96      1072\n           6       0.97      0.95      0.96      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.97      0.97      0.97      1030\n          10       0.99      0.97      0.98      1012\n\n   micro avg       0.98      0.98      0.98     11703\n   macro avg       0.98      0.98      0.98     11703\nweighted avg       0.98      0.98      0.98     11703\n samples avg       0.97      0.98      0.97     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=e2d5d30ff2454b1db42f1bd3e874b935\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e2d5d30ff2454b1db42f1bd3e874b935/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 20:48:36,824 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.19675702136134435\nEpoch 19 loss is 0.12691860293194868\nEpoch 29 loss is 0.09963694001283371\nEpoch 39 loss is 0.08266232549928246\nEpoch 49 loss is 0.07721942842928552\nEpoch 59 loss is 0.07040153803166502\nEpoch 69 loss is 0.06772988802277076\nEpoch 79 loss is 0.06808096965213609\nEpoch 89 loss is 0.06277160699643843\nEpoch 99 loss is 0.06304586082936255\nEpoch 109 loss is 0.05947888365928943\nEpoch 119 loss is 0.056828984384148414\nEpoch 129 loss is 0.05502314843135909\nEpoch 139 loss is 0.04954769492662839\nEpoch 149 loss is 0.04978222914593748\nEpoch 159 loss is 0.05478295147543594\nEpoch 169 loss is 0.04662054887355213\nEpoch 179 loss is 0.05929304252375296\nEpoch 189 loss is 0.05981274953089023\nEpoch 199 loss is 0.05613814120733131\nTrain Acc.:  0.9749177455881725\nVal Acc.:  0.9561650858754166\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1074\n           1       0.94      0.95      0.94      1089\n           2       0.99      0.98      0.99      1044\n           3       0.99      0.99      0.99      1048\n           4       0.96      0.95      0.96      1057\n           5       0.96      0.93      0.94      1072\n           6       0.96      0.95      0.95      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.96      0.96      1030\n          10       0.99      0.97      0.98      1012\n\n   micro avg       0.97      0.97      0.97     11703\n   macro avg       0.97      0.97      0.97     11703\nweighted avg       0.97      0.97      0.97     11703\n samples avg       0.96      0.97      0.96     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_pa_loss.html#multi-layer",
    "href": "examples/autass/autass_single_run_angle_pa_loss.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=ab30cdc3b6dc4a99bd4878cab67ae651\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/ab30cdc3b6dc4a99bd4878cab67ae651/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 21:00:37,622 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.5352860994082685\nEpoch 19 loss is 0.8573220308220786\nEpoch 29 loss is 0.8846553943619413\nEpoch 39 loss is 0.9282479485173039\nEpoch 49 loss is 0.9304585998948428\nEpoch 59 loss is 0.8896554528724867\nEpoch 69 loss is 0.9020712551731598\nEpoch 79 loss is 0.8780626206812697\nEpoch 89 loss is 0.9192603465644281\nEpoch 99 loss is 0.9080221700448701\nEpoch 109 loss is 0.8979155400653883\nEpoch 119 loss is 0.9165361645534543\nEpoch 129 loss is 0.8875104726098959\nEpoch 139 loss is 0.9474128621314876\nEpoch 149 loss is 0.8798036890266178\nEpoch 159 loss is 0.870499564750111\nEpoch 169 loss is 0.8812218883029092\nEpoch 179 loss is 0.8765249923026144\nEpoch 189 loss is 0.8565456302997135\nEpoch 199 loss is 0.8486741464838758\nTrain Acc.:  0.5802033927274282\nVal Acc.:  0.571990088011621\n              precision    recall  f1-score   support\n\n           0       0.79      0.73      0.76      1074\n           1       0.64      0.35      0.45      1089\n           2       0.92      0.74      0.82      1044\n           3       0.84      0.80      0.82      1048\n           4       0.49      0.32      0.38      1057\n           5       0.70      0.56      0.62      1072\n           6       0.66      0.49      0.57      1066\n           7       0.93      0.83      0.88      1103\n           8       0.91      0.86      0.88      1108\n           9       0.72      0.90      0.80      1030\n          10       0.82      0.59      0.69      1012\n\n   micro avg       0.78      0.65      0.71     11703\n   macro avg       0.77      0.65      0.70     11703\nweighted avg       0.77      0.65      0.70     11703\n samples avg       0.61      0.65      0.62     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=cde3f58739e24fe2bf1c929abc8af866\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/cde3f58739e24fe2bf1c929abc8af866/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 21:07:48,901 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.33310382354735135\nEpoch 19 loss is 0.3045317527345633\nEpoch 29 loss is 0.26208047114462196\nEpoch 39 loss is 0.5633030280355086\nEpoch 49 loss is 0.6604212666659914\nEpoch 59 loss is 0.6350045622908638\nEpoch 69 loss is 0.6371642678164225\nEpoch 79 loss is 0.6625240454923111\nEpoch 89 loss is 0.618989009951792\nEpoch 99 loss is 0.5788566858980941\nEpoch 109 loss is 0.5580994790124282\nEpoch 119 loss is 0.5665000001440373\nEpoch 129 loss is 0.6303235022304811\nEpoch 139 loss is 0.6471938510996637\nEpoch 149 loss is 0.6645542731394183\nEpoch 159 loss is 0.6454662272330662\nEpoch 169 loss is 0.6336318409892531\nEpoch 179 loss is 0.6200715265569947\nEpoch 189 loss is 0.6395423856912091\nEpoch 199 loss is 0.5969744437662491\nTrain Acc.:  0.7452036063752511\nVal Acc.:  0.7395539605229429\n              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.93      1074\n           1       0.87      0.79      0.83      1089\n           2       0.88      0.75      0.81      1044\n           3       0.89      0.84      0.87      1048\n           4       0.79      0.71      0.75      1057\n           5       0.88      0.83      0.86      1072\n           6       0.77      0.69      0.73      1066\n           7       0.96      0.91      0.93      1103\n           8       1.00      0.98      0.99      1108\n           9       0.81      0.73      0.77      1030\n          10       0.81      0.61      0.69      1012\n\n   micro avg       0.88      0.80      0.84     11703\n   macro avg       0.87      0.80      0.83     11703\nweighted avg       0.87      0.80      0.83     11703\n samples avg       0.77      0.80      0.78     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=4910372b55c6431281234703cea5b873\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/4910372b55c6431281234703cea5b873/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 21:12:12,127 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.228445869053326\nEpoch 19 loss is 0.1769025202901553\nEpoch 29 loss is 0.13638551487260145\nEpoch 39 loss is 0.12389236050933781\nEpoch 49 loss is 0.10762237564656339\nEpoch 59 loss is 0.09565055571155039\nEpoch 69 loss is 0.08563549226626824\nEpoch 79 loss is 0.07961991527154336\nEpoch 89 loss is 0.08409453002137111\nEpoch 99 loss is 0.11271795819004764\nEpoch 109 loss is 0.09878662956475265\nEpoch 119 loss is 0.0966812858882719\nEpoch 129 loss is 0.08962629585333381\nEpoch 139 loss is 0.08695483521719193\nEpoch 149 loss is 0.08539091657127361\nEpoch 159 loss is 0.09094220320688623\nEpoch 169 loss is 0.09666773124329389\nEpoch 179 loss is 0.0897644800428595\nEpoch 189 loss is 0.09589186592403481\nEpoch 199 loss is 0.09725851786233339\nTrain Acc.:  0.9344528479254797\nVal Acc.:  0.9253182944544134\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97      1074\n           1       0.93      0.92      0.92      1089\n           2       0.98      0.96      0.97      1044\n           3       0.98      0.96      0.97      1048\n           4       0.93      0.94      0.94      1057\n           5       0.94      0.92      0.93      1072\n           6       0.92      0.89      0.91      1066\n           7       0.99      0.99      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.92      0.93      1030\n          10       0.98      0.94      0.96      1012\n\n   micro avg       0.96      0.95      0.95     11703\n   macro avg       0.96      0.95      0.95     11703\nweighted avg       0.96      0.95      0.95     11703\n samples avg       0.94      0.95      0.94     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=bae1f2339cae4e1880f988c0593eaa4c\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/bae1f2339cae4e1880f988c0593eaa4c/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_3354/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 21:18:41,784 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.3850133302732534\nEpoch 19 loss is 0.22928350502576889\nEpoch 29 loss is 0.18473120489159367\nEpoch 39 loss is 0.151906876717256\nEpoch 49 loss is 0.13306868079012138\nEpoch 59 loss is 0.12176301270741918\nEpoch 69 loss is 0.11425308696387655\nEpoch 79 loss is 0.1074521977072848\nEpoch 89 loss is 0.09860157716737963\nEpoch 99 loss is 0.09525496381491966\nEpoch 109 loss is 0.09652682279150973\nEpoch 119 loss is 0.09278257367639477\nEpoch 129 loss is 0.09085598133404435\nEpoch 139 loss is 0.0890178305214046\nEpoch 149 loss is 0.08561603038010733\nEpoch 159 loss is 0.08194544436834562\nEpoch 169 loss is 0.07805339709847767\nEpoch 179 loss is 0.08145146698926171\nEpoch 189 loss is 0.07395142641579694\nEpoch 199 loss is 0.07222291978897587\nTrain Acc.:  0.945583899500064\nVal Acc.:  0.924292916346236\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.93      0.92      0.92      1089\n           2       0.99      0.96      0.97      1044\n           3       0.98      0.96      0.97      1048\n           4       0.96      0.95      0.96      1057\n           5       0.94      0.92      0.93      1072\n           6       0.93      0.87      0.90      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.91      0.91      0.91      1030\n          10       0.97      0.94      0.96      1012\n\n   micro avg       0.96      0.94      0.95     11703\n   macro avg       0.96      0.94      0.95     11703\nweighted avg       0.96      0.94      0.95     11703\n samples avg       0.93      0.94      0.94     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run.html",
    "href": "examples/autass/autass_single_run.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_single_run.html#config",
    "href": "examples/autass/autass_single_run.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1"
  },
  {
    "objectID": "examples/autass/autass_single_run.html#single-layer",
    "href": "examples/autass/autass_single_run.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=2771750f687848d0b040bf19c0328338\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2771750f687848d0b040bf19c0328338/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 08:29:25,993 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.2837285218377664\nEpoch 19 loss is 0.431940673221039\nEpoch 29 loss is 0.3961146214247004\nEpoch 39 loss is 0.39737014772407764\nEpoch 49 loss is 0.38761378372742683\nEpoch 59 loss is 0.4756636272664083\nEpoch 69 loss is 0.41528194383966116\nEpoch 79 loss is 0.2998848622620864\nEpoch 89 loss is 0.2710656493137349\nEpoch 99 loss is 0.2903374686352884\nEpoch 109 loss is 0.3910996201783306\nEpoch 119 loss is 0.2524314041411569\nEpoch 129 loss is 0.2400397004373593\nEpoch 139 loss is 0.30319441954601506\nEpoch 149 loss is 0.3092964786090183\nEpoch 159 loss is 0.25859828261158435\nEpoch 169 loss is 0.3205783007902051\nEpoch 179 loss is 0.3043531851690775\nEpoch 189 loss is 0.4236513177404459\nEpoch 199 loss is 0.44608120734258866\nTrain Acc.:  0.8010938768533948\nVal Acc.:  0.7966333418781509\n              precision    recall  f1-score   support\n\n           0       0.87      0.96      0.91      1074\n           1       0.91      0.66      0.77      1089\n           2       0.92      0.85      0.88      1044\n           3       0.87      0.80      0.84      1048\n           4       0.88      0.51      0.65      1057\n           5       0.81      0.88      0.84      1072\n           6       0.82      0.72      0.76      1066\n           7       0.98      0.96      0.97      1103\n           8       0.98      1.00      0.99      1108\n           9       0.84      0.87      0.85      1030\n          10       0.89      0.88      0.89      1012\n\n   micro avg       0.89      0.83      0.86     11703\n   macro avg       0.89      0.83      0.85     11703\nweighted avg       0.89      0.83      0.85     11703\n samples avg       0.81      0.83      0.82     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=2937a1c5c09247d3afefe736b93cce93\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2937a1c5c09247d3afefe736b93cce93/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 08:33:02,557 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.1781452279297529\nEpoch 19 loss is 0.17121365507481565\nEpoch 29 loss is 0.23437533994636411\nEpoch 39 loss is 0.249442526042442\nEpoch 49 loss is 0.2709493982290144\nEpoch 59 loss is 0.22188192248227645\nEpoch 69 loss is 0.24606682756213247\nEpoch 79 loss is 0.291044741861424\nEpoch 89 loss is 0.24921788018106592\nEpoch 99 loss is 0.24654104628164147\nEpoch 109 loss is 0.2149965350030757\nEpoch 119 loss is 0.20888710558226523\nEpoch 129 loss is 0.18029340312631476\nEpoch 139 loss is 0.21206527705421888\nEpoch 149 loss is 0.16859814960066902\nEpoch 159 loss is 0.20565043305802141\nEpoch 169 loss is 0.20745351853536964\nEpoch 179 loss is 0.21776656653704443\nEpoch 189 loss is 0.20066393883063724\nEpoch 199 loss is 0.17997682184925615\nTrain Acc.:  0.8812759047985301\nVal Acc.:  0.8766982824916688\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95      1074\n           1       0.90      0.83      0.86      1089\n           2       0.97      0.92      0.95      1044\n           3       0.94      0.90      0.92      1048\n           4       0.89      0.91      0.90      1057\n           5       0.86      0.90      0.88      1072\n           6       0.83      0.80      0.81      1066\n           7       1.00      0.98      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.87      0.89      0.88      1030\n          10       0.94      0.89      0.92      1012\n\n   micro avg       0.92      0.91      0.92     11703\n   macro avg       0.92      0.91      0.91     11703\nweighted avg       0.93      0.91      0.91     11703\n samples avg       0.89      0.91      0.90     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=b9c663cb49f645d3b526ff62b971b678\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b9c663cb49f645d3b526ff62b971b678/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 08:36:28,294 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.12631168614962632\nEpoch 19 loss is 0.08741677563113538\nEpoch 29 loss is 0.07765135114449577\nEpoch 39 loss is 0.07199543979859238\nEpoch 49 loss is 0.05459176181501563\nEpoch 59 loss is 0.054154946830923104\nEpoch 69 loss is 0.0498279498984762\nEpoch 79 loss is 0.04969739252919254\nEpoch 89 loss is 0.05646039586774165\nEpoch 99 loss is 0.05206559631806764\nEpoch 109 loss is 0.04809522582296451\nEpoch 119 loss is 0.0522341919664429\nEpoch 129 loss is 0.0495262401204281\nEpoch 139 loss is 0.04954010092306082\nEpoch 149 loss is 0.061976915229967955\nEpoch 159 loss is 0.05182441637727222\nEpoch 169 loss is 0.05091017378293241\nEpoch 179 loss is 0.0561740020607465\nEpoch 189 loss is 0.06101975489402404\nEpoch 199 loss is 0.06845130197965535\nTrain Acc.:  0.9619706875186942\nVal Acc.:  0.9557378449970093\n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.98      1074\n           1       0.95      0.92      0.93      1089\n           2       0.99      0.98      0.98      1044\n           3       0.99      0.98      0.98      1048\n           4       0.97      0.96      0.97      1057\n           5       0.95      0.94      0.95      1072\n           6       0.95      0.94      0.94      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.95      0.95      1030\n          10       0.99      0.97      0.98      1012\n\n   micro avg       0.98      0.96      0.97     11703\n   macro avg       0.98      0.96      0.97     11703\nweighted avg       0.98      0.96      0.97     11703\n samples avg       0.96      0.96      0.96     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=77bda608f5dd4cbf9fb5bf146c9bcdbe\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/77bda608f5dd4cbf9fb5bf146c9bcdbe/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 08:40:51,411 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.09760215896283031\nEpoch 19 loss is 0.07483332226588477\nEpoch 29 loss is 0.06169817582724667\nEpoch 39 loss is 0.04806632873441776\nEpoch 49 loss is 0.050257943968239405\nEpoch 59 loss is 0.045243461048822334\nEpoch 69 loss is 0.03916302966121763\nEpoch 79 loss is 0.03600947264373011\nEpoch 89 loss is 0.0327954003325785\nEpoch 99 loss is 0.034495561806988345\nEpoch 109 loss is 0.02976081583721455\nEpoch 119 loss is 0.03207888606505858\nEpoch 129 loss is 0.029421696533450108\nEpoch 139 loss is 0.02886477329200674\nEpoch 149 loss is 0.028011108627125445\nEpoch 159 loss is 0.02746762018733399\nEpoch 169 loss is 0.029159449466333746\nEpoch 179 loss is 0.028677871059498893\nEpoch 189 loss is 0.026269004340911915\nEpoch 199 loss is 0.02659348237350445\nTrain Acc.:  0.9820749476562833\nVal Acc.:  0.9690677604033154\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1074\n           1       0.96      0.96      0.96      1089\n           2       1.00      0.99      0.99      1044\n           3       1.00      0.99      0.99      1048\n           4       0.97      0.97      0.97      1057\n           5       0.96      0.96      0.96      1072\n           6       0.97      0.95      0.96      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.97      0.97      0.97      1030\n          10       0.99      0.98      0.98      1012\n\n   micro avg       0.98      0.98      0.98     11703\n   macro avg       0.98      0.98      0.98     11703\nweighted avg       0.98      0.98      0.98     11703\n samples avg       0.97      0.98      0.97     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run.html#multi-layer",
    "href": "examples/autass/autass_single_run.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=7aa1a5aeeab040fda27730a9cb90d8ad\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/7aa1a5aeeab040fda27730a9cb90d8ad/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.6741966148101034\nEpoch 19 loss is 0.6345819678996693\nEpoch 29 loss is 0.6163333298521995\nEpoch 39 loss is 0.5973288617286425\nEpoch 49 loss is 0.5657958380010314\nEpoch 59 loss is 0.5930215843207252\nEpoch 69 loss is 0.614107440452483\nEpoch 79 loss is 0.6680024372385054\nEpoch 89 loss is 0.670419685702236\nEpoch 99 loss is 0.6396620484680041\nEpoch 109 loss is 0.7318713292921706\nEpoch 119 loss is 0.7326499528623525\nEpoch 129 loss is 0.7676124397701742\nEpoch 139 loss is 0.7363896056980302\nEpoch 149 loss is 0.7518392231032288\nEpoch 159 loss is 0.6876373192278051\nEpoch 169 loss is 0.684599693905956\nEpoch 179 loss is 0.6865932359686486\nEpoch 189 loss is 0.6739611875627026\nEpoch 199 loss is 0.6541066273988838\nTrain Acc.:  0.3464940392257403\nVal Acc.:  0.3492266940100829\n              precision    recall  f1-score   support\n\n           0       0.56      0.70      0.63      1074\n           1       0.59      0.43      0.50      1089\n           2       0.59      0.23      0.33      1044\n           3       0.76      0.43      0.55      1048\n           4       0.57      0.27      0.36      1057\n           5       0.59      0.61      0.60      1072\n           6       0.40      0.08      0.13      1066\n           7       0.92      0.80      0.86      1103\n           8       0.97      0.96      0.97      1108\n           9       0.17      0.03      0.05      1030\n          10       0.49      0.28      0.36      1012\n\n   micro avg       0.67      0.44      0.53     11703\n   macro avg       0.60      0.44      0.48     11703\nweighted avg       0.60      0.44      0.49     11703\n samples avg       0.39      0.44      0.41     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=63f6d3900d26435baf5c0451ee2550cf\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/63f6d3900d26435baf5c0451ee2550cf/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.4918386432669506\nEpoch 19 loss is 0.5658443724932558\nEpoch 29 loss is 0.547344653493303\nEpoch 39 loss is 0.5003598398694803\nEpoch 49 loss is 0.45764246002628084\nEpoch 59 loss is 0.48134115218311874\nEpoch 69 loss is 0.46767609185504994\nEpoch 79 loss is 0.47236872848369066\nEpoch 89 loss is 0.45542004995148055\nEpoch 99 loss is 0.4707600822371793\nEpoch 109 loss is 0.4592737186466213\nEpoch 119 loss is 0.44952235101712684\nEpoch 129 loss is 0.4279650487928361\nEpoch 139 loss is 0.4390951851709622\nEpoch 149 loss is 0.4078203854663746\nEpoch 159 loss is 0.4626180199815107\nEpoch 169 loss is 0.4595541584940826\nEpoch 179 loss is 0.41118996909517436\nEpoch 189 loss is 0.4143808650128177\nEpoch 199 loss is 0.42120076453410465\nTrain Acc.:  0.6030850745630902\nVal Acc.:  0.6048876356489788\n              precision    recall  f1-score   support\n\n           0       0.90      0.74      0.81      1074\n           1       0.71      0.70      0.71      1089\n           2       0.79      0.67      0.73      1044\n           3       0.85      0.70      0.77      1048\n           4       0.78      0.54      0.64      1057\n           5       0.72      0.72      0.72      1072\n           6       0.71      0.45      0.55      1066\n           7       0.98      0.91      0.95      1103\n           8       0.98      0.96      0.97      1108\n           9       0.85      0.31      0.46      1030\n          10       0.70      0.63      0.66      1012\n\n   micro avg       0.82      0.67      0.74     11703\n   macro avg       0.82      0.67      0.72     11703\nweighted avg       0.82      0.67      0.73     11703\n samples avg       0.64      0.67      0.65     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=3ec0ba0e69fe4e8781f1f56f68bb4df0\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/3ec0ba0e69fe4e8781f1f56f68bb4df0/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.17595385331554345\nEpoch 19 loss is 0.24443298597993288\nEpoch 29 loss is 0.34861398016646006\nEpoch 39 loss is 0.35088833894092397\nEpoch 49 loss is 0.3184690939966281\nEpoch 59 loss is 0.29159015152507245\nEpoch 69 loss is 0.29664844727356443\nEpoch 79 loss is 0.3148698996217653\nEpoch 89 loss is 0.34718880768462584\nEpoch 99 loss is 0.32786482129724753\nEpoch 109 loss is 0.3249049482370644\nEpoch 119 loss is 0.31320303291619195\nEpoch 129 loss is 0.30243110019968583\nEpoch 139 loss is 0.30238450965742614\nEpoch 149 loss is 0.29449084419928934\nEpoch 159 loss is 0.2935125197293862\nEpoch 169 loss is 0.2872804696457938\nEpoch 179 loss is 0.29204265563094745\nEpoch 189 loss is 0.3058525140885775\nEpoch 199 loss is 0.30025929403593016\nTrain Acc.:  0.827223005597573\nVal Acc.:  0.820986071947364\n              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.94      1074\n           1       0.87      0.82      0.84      1089\n           2       0.94      0.90      0.92      1044\n           3       0.92      0.88      0.90      1048\n           4       0.87      0.78      0.82      1057\n           5       0.86      0.87      0.87      1072\n           6       0.81      0.72      0.77      1066\n           7       0.99      0.96      0.98      1103\n           8       1.00      0.99      0.99      1108\n           9       0.85      0.80      0.82      1030\n          10       0.92      0.87      0.89      1012\n\n   micro avg       0.91      0.87      0.89     11703\n   macro avg       0.91      0.87      0.89     11703\nweighted avg       0.91      0.87      0.89     11703\n samples avg       0.84      0.87      0.85     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=d2c79e60ea7040fd879fc5e297a36346\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/d2c79e60ea7040fd879fc5e297a36346/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]'}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_9096/3249266730.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.14845134117007924\nEpoch 19 loss is 0.1329600740131187\nEpoch 29 loss is 0.1297421221557918\nEpoch 39 loss is 0.11658710957349895\nEpoch 49 loss is 0.1263711960559784\nEpoch 59 loss is 0.15283946197204235\nEpoch 69 loss is 0.20268996749466878\nEpoch 79 loss is 0.26901874406663706\nEpoch 89 loss is 0.3079409409709422\nEpoch 99 loss is 0.28158223191896325\nEpoch 109 loss is 0.27721301741939225\nEpoch 119 loss is 0.23720462062688716\nEpoch 129 loss is 0.23388674674232188\nEpoch 139 loss is 0.22834769155604162\nEpoch 149 loss is 0.22821928822954132\nEpoch 159 loss is 0.1975671042738866\nEpoch 169 loss is 0.18680390982844616\nEpoch 179 loss is 0.18587410530287377\nEpoch 189 loss is 0.19314962951484493\nEpoch 199 loss is 0.1799970473113\nTrain Acc.:  0.888774943383327\nVal Acc.:  0.8714859437751004\n              precision    recall  f1-score   support\n\n           0       0.97      0.94      0.95      1074\n           1       0.92      0.88      0.90      1089\n           2       0.97      0.91      0.94      1044\n           3       0.93      0.92      0.92      1048\n           4       0.90      0.88      0.89      1057\n           5       0.90      0.89      0.90      1072\n           6       0.84      0.79      0.81      1066\n           7       1.00      0.98      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.89      0.88      0.89      1030\n          10       0.95      0.91      0.93      1012\n\n   micro avg       0.93      0.91      0.92     11703\n   macro avg       0.93      0.91      0.92     11703\nweighted avg       0.93      0.91      0.92     11703\n samples avg       0.89      0.91      0.90     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_pa_loss.html",
    "href": "examples/autass/autass_multiple_run_angle_pa_loss.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_pa_loss.html#config",
    "href": "examples/autass/autass_multiple_run_angle_pa_loss.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1\nclip_angle_value = 1000000"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_pa_loss.html#single-layer",
    "href": "examples/autass/autass_multiple_run_angle_pa_loss.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=e06e21b763e247b78c42f9fc54073711\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e06e21b763e247b78c42f9fc54073711/output/log\n\n\nInconsistency detected by ld.so: dl-open.c: 632: _dl_open: Assertion `_dl_debug_initialize (0, args.nsid)->r_state == RT_CONSISTENT' failed!\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 21:03:49,366 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.24486756715526192\nEpoch 19 loss is 0.16472245788019146\nEpoch 29 loss is 0.16197286684855702\nEpoch 39 loss is 0.15777335760350167\nEpoch 49 loss is 0.16362277181734114\nEpoch 59 loss is 0.1599623075779962\nEpoch 69 loss is 0.15505627729450555\nEpoch 79 loss is 0.1516815979779308\nEpoch 89 loss is 0.14756507300426192\nEpoch 99 loss is 0.14990614337584326\nEpoch 109 loss is 0.14464806634677907\nEpoch 119 loss is 0.14414441882839768\nEpoch 129 loss is 0.133414154667594\nEpoch 139 loss is 0.1568121289545272\nEpoch 149 loss is 0.13148384624669937\nEpoch 159 loss is 0.15520714324892884\nEpoch 169 loss is 0.15488150667967812\nEpoch 179 loss is 0.16196303491616978\nEpoch 189 loss is 0.17048013425110728\nEpoch 199 loss is 0.253481823204663\nTrain Acc.:  0.9061037878949729\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95      1063\n           1       0.88      0.91      0.90      1064\n           2       0.97      0.95      0.96      1064\n           3       0.92      0.93      0.93      1064\n           4       0.90      0.87      0.88      1064\n           5       0.92      0.89      0.90      1063\n           6       0.88      0.85      0.86      1064\n           7       0.99      0.97      0.98      1064\n           8       1.00      0.99      1.00      1064\n           9       0.92      0.90      0.91      1064\n          10       0.95      0.97      0.96      1064\n\n   micro avg       0.93      0.93      0.93     11702\n   macro avg       0.93      0.93      0.93     11702\nweighted avg       0.93      0.93      0.93     11702\n samples avg       0.91      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23206087884330276\nEpoch 19 loss is 0.223541648173787\nEpoch 29 loss is 0.19793911944454762\nEpoch 39 loss is 0.20525953919872755\nEpoch 49 loss is 0.19172214725700548\nEpoch 59 loss is 0.180403134615211\nEpoch 69 loss is 0.20454411746859222\nEpoch 79 loss is 0.22207294107387418\nEpoch 89 loss is 0.1823581614612617\nEpoch 99 loss is 0.1984208152000532\nEpoch 109 loss is 0.21770198580729927\nEpoch 119 loss is 0.26321624687915784\nEpoch 129 loss is 0.2366645070320064\nEpoch 139 loss is 0.26308797516611215\nEpoch 149 loss is 0.2796785803565688\nEpoch 159 loss is 0.19642007623052968\nEpoch 169 loss is 0.3310147275301174\nEpoch 179 loss is 0.2874490546335206\nEpoch 189 loss is 0.2595535203540301\nEpoch 199 loss is 0.3217917178123802\nTrain Acc.:  0.8804452325506869\n              precision    recall  f1-score   support\n\n           0       0.97      0.91      0.94      1063\n           1       0.89      0.87      0.88      1064\n           2       0.98      0.96      0.97      1064\n           3       0.98      0.95      0.97      1063\n           4       0.84      0.71      0.77      1064\n           5       0.92      0.86      0.89      1064\n           6       0.84      0.83      0.83      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.87      0.90      0.88      1064\n          10       0.92      0.93      0.93      1064\n\n   micro avg       0.93      0.90      0.91     11702\n   macro avg       0.93      0.90      0.91     11702\nweighted avg       0.93      0.90      0.91     11702\n samples avg       0.89      0.90      0.89     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23712313444477043\nEpoch 19 loss is 0.21309815317807554\nEpoch 29 loss is 0.2151226041409042\nEpoch 39 loss is 0.22794682641424854\nEpoch 49 loss is 0.18590490342052546\nEpoch 59 loss is 0.17194472537314034\nEpoch 69 loss is 0.18656052422193878\nEpoch 79 loss is 0.1756717054349332\nEpoch 89 loss is 0.19215676606052032\nEpoch 99 loss is 0.19421889588535315\nEpoch 109 loss is 0.23849424654511947\nEpoch 119 loss is 0.21172349809503013\nEpoch 129 loss is 0.310951138936457\nEpoch 139 loss is 0.40137440459088775\nEpoch 149 loss is 0.3257708635272683\nEpoch 159 loss is 0.3058673858224232\nEpoch 169 loss is 0.3393692446068119\nEpoch 179 loss is 0.3696819123265627\nEpoch 189 loss is 0.3599098499384412\nEpoch 199 loss is 0.4304120007418079\nTrain Acc.:  0.8721558741213921\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95      1064\n           1       0.84      0.81      0.82      1064\n           2       0.96      0.91      0.93      1064\n           3       0.96      0.92      0.94      1063\n           4       0.89      0.84      0.86      1064\n           5       0.87      0.84      0.86      1064\n           6       0.88      0.76      0.81      1063\n           7       1.00      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.86      0.86      0.86      1064\n          10       0.92      0.91      0.91      1064\n\n   micro avg       0.92      0.89      0.90     11702\n   macro avg       0.92      0.89      0.90     11702\nweighted avg       0.92      0.89      0.90     11702\n samples avg       0.88      0.89      0.88     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.22525142483966729\nEpoch 19 loss is 0.21769357020501426\nEpoch 29 loss is 0.2577133298481112\nEpoch 39 loss is 0.22213481954372305\nEpoch 49 loss is 0.23841117493258804\nEpoch 59 loss is 0.309014874649911\nEpoch 69 loss is 0.35371038376681546\nEpoch 79 loss is 0.3699092508438563\nEpoch 89 loss is 0.36114378942190123\nEpoch 99 loss is 0.3760639842052328\nEpoch 109 loss is 0.289880557557834\nEpoch 119 loss is 0.3542478901455959\nEpoch 129 loss is 0.2415444508855365\nEpoch 139 loss is 0.25469037973175684\nEpoch 149 loss is 0.23138965174619833\nEpoch 159 loss is 0.21245593786661174\nEpoch 169 loss is 0.28983951582336215\nEpoch 179 loss is 0.2624560468178941\nEpoch 189 loss is 0.22517652315304382\nEpoch 199 loss is 0.30955816883738974\nTrain Acc.:  0.830303159783793\n              precision    recall  f1-score   support\n\n           0       0.95      0.94      0.95      1063\n           1       0.89      0.81      0.85      1064\n           2       0.95      0.85      0.90      1064\n           3       0.96      0.94      0.95      1063\n           4       0.89      0.81      0.85      1064\n           5       0.88      0.81      0.85      1064\n           6       0.77      0.70      0.73      1064\n           7       1.00      0.99      1.00      1064\n           8       0.99      0.99      0.99      1064\n           9       0.86      0.89      0.87      1064\n          10       0.91      0.84      0.87      1064\n\n   micro avg       0.92      0.87      0.89     11702\n   macro avg       0.91      0.87      0.89     11702\nweighted avg       0.91      0.87      0.89     11702\n samples avg       0.85      0.87      0.86     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2563851438763014\nEpoch 19 loss is 0.21581594458135714\nEpoch 29 loss is 0.2354525347648366\nEpoch 39 loss is 0.19813069646116785\nEpoch 49 loss is 0.23640784807386372\nEpoch 59 loss is 0.2855586216205039\nEpoch 69 loss is 0.31419042978716966\nEpoch 79 loss is 0.3396274724821675\nEpoch 89 loss is 0.3805541273460192\nEpoch 99 loss is 0.2594859077291225\nEpoch 109 loss is 0.27091326668934435\nEpoch 119 loss is 0.26640856164548926\nEpoch 129 loss is 0.2599984885766787\nEpoch 139 loss is 0.24273455139343672\nEpoch 149 loss is 0.3339575152030492\nEpoch 159 loss is 0.2597825247475023\nEpoch 169 loss is 0.2548200790331675\nEpoch 179 loss is 0.2912208067110613\nEpoch 189 loss is 0.29534861061989254\nEpoch 199 loss is 0.35203137025199627\nTrain Acc.:  0.8437840493943214\n              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.94      1064\n           1       0.88      0.80      0.84      1064\n           2       0.92      0.85      0.88      1064\n           3       0.93      0.81      0.87      1064\n           4       0.89      0.81      0.85      1064\n           5       0.85      0.81      0.83      1064\n           6       0.84      0.81      0.83      1063\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1063\n           9       0.86      0.86      0.86      1064\n          10       0.94      0.85      0.89      1064\n\n   micro avg       0.91      0.86      0.89     11702\n   macro avg       0.91      0.86      0.89     11702\nweighted avg       0.91      0.86      0.89     11702\n samples avg       0.85      0.86      0.86     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=a07a5ceae0cf4b42a7a0a6bcbcf20e9b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/a07a5ceae0cf4b42a7a0a6bcbcf20e9b/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 21:16:31,580 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.2058236759317573\nEpoch 19 loss is 0.1825360529148008\nEpoch 29 loss is 0.13331677743733528\nEpoch 39 loss is 0.11695840664945023\nEpoch 49 loss is 0.11103621118112407\nEpoch 59 loss is 0.12555862590235578\nEpoch 69 loss is 0.12181158857175331\nEpoch 79 loss is 0.09544202089592545\nEpoch 89 loss is 0.11178487401744581\nEpoch 99 loss is 0.12958967446816022\nEpoch 109 loss is 0.11905161904742857\nEpoch 119 loss is 0.10873579923307998\nEpoch 129 loss is 0.09673697400248864\nEpoch 139 loss is 0.09588223402448456\nEpoch 149 loss is 0.11162506774666282\nEpoch 159 loss is 0.09059592728792225\nEpoch 169 loss is 0.09758258176950994\nEpoch 179 loss is 0.10536165971500439\nEpoch 189 loss is 0.10205764671734051\nEpoch 199 loss is 0.14389611907866479\nTrain Acc.:  0.9354583716110838\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      1063\n           1       0.93      0.93      0.93      1064\n           2       0.98      0.95      0.96      1064\n           3       0.96      0.95      0.96      1064\n           4       0.94      0.93      0.94      1064\n           5       0.94      0.94      0.94      1063\n           6       0.88      0.88      0.88      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.92      0.93      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.94      0.94      0.94     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2307281778017605\nEpoch 19 loss is 0.18615664526599873\nEpoch 29 loss is 0.16207854951403944\nEpoch 39 loss is 0.15162521963532127\nEpoch 49 loss is 0.16081494726357065\nEpoch 59 loss is 0.1257449738051931\nEpoch 69 loss is 0.10937442608829037\nEpoch 79 loss is 0.12329823825151687\nEpoch 89 loss is 0.12930987857639764\nEpoch 99 loss is 0.12765360621548777\nEpoch 109 loss is 0.15596338287786382\nEpoch 119 loss is 0.16634696725086548\nEpoch 129 loss is 0.1495299076006498\nEpoch 139 loss is 0.1502898510399668\nEpoch 149 loss is 0.15832158268949942\nEpoch 159 loss is 0.17899469216655411\nEpoch 169 loss is 0.23058719525799198\nEpoch 179 loss is 0.2102188014453179\nEpoch 189 loss is 0.23088950030054886\nEpoch 199 loss is 0.21150270029799098\nTrain Acc.:  0.9071933685132566\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1063\n           1       0.91      0.89      0.90      1064\n           2       0.98      0.97      0.97      1064\n           3       0.94      0.94      0.94      1063\n           4       0.91      0.92      0.92      1064\n           5       0.93      0.93      0.93      1064\n           6       0.90      0.89      0.90      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.92      0.88      0.90      1064\n          10       0.92      0.89      0.90      1064\n\n   micro avg       0.94      0.93      0.94     11702\n   macro avg       0.94      0.93      0.94     11702\nweighted avg       0.94      0.93      0.94     11702\n samples avg       0.92      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.21257633823646344\nEpoch 19 loss is 0.17318814963579834\nEpoch 29 loss is 0.1458617958714956\nEpoch 39 loss is 0.14125668374187184\nEpoch 49 loss is 0.128772752498416\nEpoch 59 loss is 0.1352171863065471\nEpoch 69 loss is 0.10681416437897072\nEpoch 79 loss is 0.13216534508015265\nEpoch 89 loss is 0.11111060725019\nEpoch 99 loss is 0.11440525031960108\nEpoch 109 loss is 0.1094888755315214\nEpoch 119 loss is 0.11533819118199232\nEpoch 129 loss is 0.10736030126824322\nEpoch 139 loss is 0.10061210309481297\nEpoch 149 loss is 0.10068345790438905\nEpoch 159 loss is 0.09078661822690025\nEpoch 169 loss is 0.09375741469930386\nEpoch 179 loss is 0.09420665138442623\nEpoch 189 loss is 0.09327934878515685\nEpoch 199 loss is 0.09550257254442705\nTrain Acc.:  0.9378939047578354\n              precision    recall  f1-score   support\n\n           0       0.97      0.94      0.96      1064\n           1       0.94      0.91      0.92      1064\n           2       0.97      0.96      0.97      1064\n           3       0.99      0.97      0.98      1063\n           4       0.94      0.92      0.93      1064\n           5       0.94      0.93      0.93      1064\n           6       0.91      0.86      0.88      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.92      0.91      0.92      1064\n          10       0.96      0.95      0.96      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.94     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20641349230841313\nEpoch 19 loss is 0.15738830801178577\nEpoch 29 loss is 0.1799417678605428\nEpoch 39 loss is 0.13007205544481562\nEpoch 49 loss is 0.11737783309073208\nEpoch 59 loss is 0.1114466828734566\nEpoch 69 loss is 0.11852908050617135\nEpoch 79 loss is 0.11542474814310127\nEpoch 89 loss is 0.10236658536166438\nEpoch 99 loss is 0.12045357416608558\nEpoch 109 loss is 0.1051365830820148\nEpoch 119 loss is 0.09761249375093389\nEpoch 129 loss is 0.09216019224495184\nEpoch 139 loss is 0.09421224180251014\nEpoch 149 loss is 0.09233837313904565\nEpoch 159 loss is 0.09223922889858933\nEpoch 169 loss is 0.08829034240388713\nEpoch 179 loss is 0.09375694157319493\nEpoch 189 loss is 0.08536116014844801\nEpoch 199 loss is 0.08500084532665717\nTrain Acc.:  0.94060717414062\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1063\n           1       0.93      0.93      0.93      1064\n           2       0.98      0.96      0.97      1064\n           3       0.97      0.95      0.96      1063\n           4       0.96      0.92      0.94      1064\n           5       0.95      0.92      0.93      1064\n           6       0.90      0.90      0.90      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.92      0.93      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.96      0.95      0.95     11702\n   macro avg       0.96      0.95      0.95     11702\nweighted avg       0.96      0.95      0.95     11702\n samples avg       0.94      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20548555472102373\nEpoch 19 loss is 0.15323632791255185\nEpoch 29 loss is 0.1295082539268068\nEpoch 39 loss is 0.12081646040093838\nEpoch 49 loss is 0.10243967732402894\nEpoch 59 loss is 0.10126855908153241\nEpoch 69 loss is 0.11296332021354512\nEpoch 79 loss is 0.09185533808063474\nEpoch 89 loss is 0.09732833351838127\nEpoch 99 loss is 0.09188653614455912\nEpoch 109 loss is 0.10085898797344649\nEpoch 119 loss is 0.10443676031698373\nEpoch 129 loss is 0.1023311360663922\nEpoch 139 loss is 0.10306230769293823\nEpoch 149 loss is 0.11354442989486692\nEpoch 159 loss is 0.10403630260181586\nEpoch 169 loss is 0.09833004415491228\nEpoch 179 loss is 0.14203489341950598\nEpoch 189 loss is 0.10320350983155654\nEpoch 199 loss is 0.11371829480246762\nTrain Acc.:  0.9411626466126861\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96      1064\n           1       0.93      0.92      0.93      1064\n           2       0.98      0.94      0.96      1064\n           3       0.98      0.96      0.97      1064\n           4       0.95      0.91      0.93      1064\n           5       0.94      0.93      0.93      1064\n           6       0.90      0.90      0.90      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.94      0.91      0.93      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.94      0.94      0.94     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=1dd535f82fcf4ad28379c9d07b765068\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/1dd535f82fcf4ad28379c9d07b765068/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 21:30:40,926 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.24396016163019188\nEpoch 19 loss is 0.1408100184642106\nEpoch 29 loss is 0.10649778199752649\nEpoch 39 loss is 0.0854672719392131\nEpoch 49 loss is 0.09677528437761192\nEpoch 59 loss is 0.08491636262762711\nEpoch 69 loss is 0.07073853121933514\nEpoch 79 loss is 0.0623588742935536\nEpoch 89 loss is 0.06493405545145113\nEpoch 99 loss is 0.06570635803032455\nEpoch 109 loss is 0.05703714881850874\nEpoch 119 loss is 0.06868247092465077\nEpoch 129 loss is 0.05820049851283152\nEpoch 139 loss is 0.057654175689656244\nEpoch 149 loss is 0.054585099955762065\nEpoch 159 loss is 0.07306441308956363\nEpoch 169 loss is 0.06311269411213584\nEpoch 179 loss is 0.0785263104691008\nEpoch 189 loss is 0.08227202587346817\nEpoch 199 loss is 0.055809321412454364\nTrain Acc.:  0.9670775738671566\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.98      0.98      1064\n           4       0.96      0.92      0.94      1064\n           5       0.97      0.95      0.96      1063\n           6       0.95      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2661679163999582\nEpoch 19 loss is 0.14625262471743808\nEpoch 29 loss is 0.12002121900326053\nEpoch 39 loss is 0.11372154790877459\nEpoch 49 loss is 0.09963077571846711\nEpoch 59 loss is 0.08699985949055491\nEpoch 69 loss is 0.08908419619519271\nEpoch 79 loss is 0.077337234526038\nEpoch 89 loss is 0.0786930436278074\nEpoch 99 loss is 0.06691134614071381\nEpoch 109 loss is 0.07431928775441791\nEpoch 119 loss is 0.059303313881029024\nEpoch 129 loss is 0.05999768811650241\nEpoch 139 loss is 0.05597988277724091\nEpoch 149 loss is 0.05467993967785231\nEpoch 159 loss is 0.054380003065822906\nEpoch 169 loss is 0.055407265453249616\nEpoch 179 loss is 0.05802883371582689\nEpoch 189 loss is 0.05513524843130078\nEpoch 199 loss is 0.04906545557802202\nTrain Acc.:  0.9664793727433931\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1063\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.98      0.98      1063\n           4       0.94      0.94      0.94      1064\n           5       0.97      0.96      0.97      1064\n           6       0.97      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20502371384654208\nEpoch 19 loss is 0.1600604545972537\nEpoch 29 loss is 0.17638293655801232\nEpoch 39 loss is 0.11561468644927743\nEpoch 49 loss is 0.09013697049000392\nEpoch 59 loss is 0.08748177132875291\nEpoch 69 loss is 0.07197819254783605\nEpoch 79 loss is 0.0702369190086657\nEpoch 89 loss is 0.07234675932731245\nEpoch 99 loss is 0.068253828507892\nEpoch 109 loss is 0.07305692741178821\nEpoch 119 loss is 0.07672600907945662\nEpoch 129 loss is 0.07097353389268429\nEpoch 139 loss is 0.06347196049334779\nEpoch 149 loss is 0.0644738520961493\nEpoch 159 loss is 0.05376314275695037\nEpoch 169 loss is 0.05644687125414726\nEpoch 179 loss is 0.048773048522865105\nEpoch 189 loss is 0.04476576239785017\nEpoch 199 loss is 0.04786218107136711\nTrain Acc.:  0.9728032131946076\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1064\n           1       0.96      0.96      0.96      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.97      0.98      1063\n           4       0.97      0.95      0.96      1064\n           5       0.96      0.96      0.96      1064\n           6       0.96      0.94      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.95      0.95      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.22543501013461756\nEpoch 19 loss is 0.17068695324254124\nEpoch 29 loss is 0.12409391685149763\nEpoch 39 loss is 0.11697496548175042\nEpoch 49 loss is 0.08876241864971807\nEpoch 59 loss is 0.08153087295328582\nEpoch 69 loss is 0.07483510813769084\nEpoch 79 loss is 0.10710111668977772\nEpoch 89 loss is 0.08042401739956408\nEpoch 99 loss is 0.07369310521635002\nEpoch 109 loss is 0.07451935209149846\nEpoch 119 loss is 0.059885478707408256\nEpoch 129 loss is 0.0567862421178645\nEpoch 139 loss is 0.05093684395615367\nEpoch 149 loss is 0.06306265519934738\nEpoch 159 loss is 0.05432974760807225\nEpoch 169 loss is 0.04892963285147231\nEpoch 179 loss is 0.04994840531613505\nEpoch 189 loss is 0.051194054698050454\nEpoch 199 loss is 0.044596025063632284\nTrain Acc.:  0.9714572606661397\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1063\n           1       0.96      0.96      0.96      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.99      0.98      1063\n           4       0.96      0.96      0.96      1064\n           5       0.96      0.95      0.95      1064\n           6       0.95      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.94      0.95      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2782503201838837\nEpoch 19 loss is 0.16120408621727206\nEpoch 29 loss is 0.12418159289722493\nEpoch 39 loss is 0.09063661342210186\nEpoch 49 loss is 0.08274663620794244\nEpoch 59 loss is 0.07859951940472655\nEpoch 69 loss is 0.07256532240445507\nEpoch 79 loss is 0.06916262503726474\nEpoch 89 loss is 0.0630058410204737\nEpoch 99 loss is 0.07026817485377791\nEpoch 109 loss is 0.05949501562367503\nEpoch 119 loss is 0.053246533281885174\nEpoch 129 loss is 0.05572973071008147\nEpoch 139 loss is 0.05570382762746428\nEpoch 149 loss is 0.08662761110902355\nEpoch 159 loss is 0.057920531802435\nEpoch 169 loss is 0.06671310002777765\nEpoch 179 loss is 0.06361666252378624\nEpoch 189 loss is 0.0634835982068112\nEpoch 199 loss is 0.05273022961513594\nTrain Acc.:  0.9670989381930053\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1064\n           1       0.95      0.94      0.95      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.98      0.98      1064\n           4       0.96      0.96      0.96      1064\n           5       0.95      0.95      0.95      1064\n           6       0.94      0.95      0.94      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.98      0.94      0.96      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.97      0.97      0.97     11702\n   macro avg       0.97      0.97      0.97     11702\nweighted avg       0.97      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=5a39cc009dd34b58bf9f7dc26695a102\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/5a39cc009dd34b58bf9f7dc26695a102/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 21:54:30,788 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.2013734097530227\nEpoch 19 loss is 0.13194389245179217\nEpoch 29 loss is 0.10095062501209577\nEpoch 39 loss is 0.09074072104898326\nEpoch 49 loss is 0.08300947225827868\nEpoch 59 loss is 0.0738162527877322\nEpoch 69 loss is 0.07072649592151344\nEpoch 79 loss is 0.07138726478663758\nEpoch 89 loss is 0.06214147198340652\nEpoch 99 loss is 0.07020503698271563\nEpoch 109 loss is 0.074644080701454\nEpoch 119 loss is 0.06734553316012935\nEpoch 129 loss is 0.059474967215059765\nEpoch 139 loss is 0.07335145914140831\nEpoch 149 loss is 0.062376795077677044\nEpoch 159 loss is 0.0526909906751619\nEpoch 169 loss is 0.04941692807072795\nEpoch 179 loss is 0.04210521010111257\nEpoch 189 loss is 0.0447379761754187\nEpoch 199 loss is 0.03882946678160469\nTrain Acc.:  0.97899886769073\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.95      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.99      0.99      1064\n           4       0.97      0.96      0.97      1064\n           5       0.95      0.95      0.95      1063\n           6       0.96      0.95      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.94      0.96      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.21319071325891403\nEpoch 19 loss is 0.1541410855291198\nEpoch 29 loss is 0.12536923443070858\nEpoch 39 loss is 0.09970140587708567\nEpoch 49 loss is 0.08215366375661032\nEpoch 59 loss is 0.07893182432329253\nEpoch 69 loss is 0.07569625143171067\nEpoch 79 loss is 0.07090777313842668\nEpoch 89 loss is 0.06514219463825727\nEpoch 99 loss is 0.06887248604052763\nEpoch 109 loss is 0.057908520270728236\nEpoch 119 loss is 0.057421434582441984\nEpoch 129 loss is 0.04913560375422588\nEpoch 139 loss is 0.04935810215299344\nEpoch 149 loss is 0.04237250334663373\nEpoch 159 loss is 0.040383543448331234\nEpoch 169 loss is 0.036213423181436236\nEpoch 179 loss is 0.03839429011389191\nEpoch 189 loss is 0.03846825102366323\nEpoch 199 loss is 0.037638064394336875\nTrain Acc.:  0.9823317025231268\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.96      0.96      0.96      1064\n           2       1.00      0.99      1.00      1064\n           3       0.98      0.99      0.99      1063\n           4       0.96      0.97      0.96      1064\n           5       0.97      0.96      0.96      1064\n           6       0.96      0.95      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20835717627283434\nEpoch 19 loss is 0.1410003145381981\nEpoch 29 loss is 0.10362160718673499\nEpoch 39 loss is 0.09535519170107241\nEpoch 49 loss is 0.08623970792143622\nEpoch 59 loss is 0.06967393203823516\nEpoch 69 loss is 0.07223157768403805\nEpoch 79 loss is 0.06292363524435325\nEpoch 89 loss is 0.06516889101094622\nEpoch 99 loss is 0.05962646936523845\nEpoch 109 loss is 0.052085959520220305\nEpoch 119 loss is 0.05127448522332268\nEpoch 129 loss is 0.04993565878003458\nEpoch 139 loss is 0.04217621548518707\nEpoch 149 loss is 0.04918457656177097\nEpoch 159 loss is 0.03850822431866483\nEpoch 169 loss is 0.038279677015932684\nEpoch 179 loss is 0.039339300558995774\nEpoch 189 loss is 0.03874658731757177\nEpoch 199 loss is 0.036737692281627654\nTrain Acc.:  0.9827589890401008\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1064\n           1       0.95      0.95      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       1.00      0.98      0.99      1063\n           4       0.97      0.96      0.97      1064\n           5       0.97      0.95      0.96      1064\n           6       0.96      0.94      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.97      0.97      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2047739384042335\nEpoch 19 loss is 0.1240691449665739\nEpoch 29 loss is 0.1250756910889213\nEpoch 39 loss is 0.09980943240036731\nEpoch 49 loss is 0.07931262188990955\nEpoch 59 loss is 0.06880595699322983\nEpoch 69 loss is 0.07295263256108461\nEpoch 79 loss is 0.0657251223690606\nEpoch 89 loss is 0.06288437799940251\nEpoch 99 loss is 0.06078154801969977\nEpoch 109 loss is 0.05630173979958109\nEpoch 119 loss is 0.05059079914071474\nEpoch 129 loss is 0.0519624732081652\nEpoch 139 loss is 0.04989906966215809\nEpoch 149 loss is 0.046105722466105954\nEpoch 159 loss is 0.04479487999819714\nEpoch 169 loss is 0.05169914293629347\nEpoch 179 loss is 0.05468717561123138\nEpoch 189 loss is 0.04472661993607276\nEpoch 199 loss is 0.04167204375351288\nTrain Acc.:  0.9793193325784605\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1063\n           1       0.93      0.96      0.94      1064\n           2       1.00      0.99      1.00      1064\n           3       0.99      0.98      0.99      1063\n           4       0.97      0.97      0.97      1064\n           5       0.97      0.94      0.95      1064\n           6       0.96      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.98      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.24100019695211888\nEpoch 19 loss is 0.15365636763706683\nEpoch 29 loss is 0.11363793386653538\nEpoch 39 loss is 0.09423439262203624\nEpoch 49 loss is 0.08824477344066799\nEpoch 59 loss is 0.07361807474799302\nEpoch 69 loss is 0.07530689777501173\nEpoch 79 loss is 0.06350203232630086\nEpoch 89 loss is 0.061400948333146264\nEpoch 99 loss is 0.05771039868957064\nEpoch 109 loss is 0.057609481716596814\nEpoch 119 loss is 0.05653083207399525\nEpoch 129 loss is 0.05154005805827653\nEpoch 139 loss is 0.05072576182283894\nEpoch 149 loss is 0.046395221236291556\nEpoch 159 loss is 0.04620623101501136\nEpoch 169 loss is 0.042652961928911375\nEpoch 179 loss is 0.0574112161346133\nEpoch 189 loss is 0.044568967625431236\nEpoch 199 loss is 0.03852016601626823\nTrain Acc.:  0.9786570384771508\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1064\n           1       0.95      0.96      0.95      1064\n           2       0.99      0.98      0.99      1064\n           3       0.99      0.98      0.99      1064\n           4       0.98      0.95      0.96      1064\n           5       0.96      0.94      0.95      1064\n           6       0.94      0.95      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.96      0.95      0.96      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_pa_loss.html#multi-layer",
    "href": "examples/autass/autass_multiple_run_angle_pa_loss.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=938ba07947984546b3b8a4b5a774d0d2\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/938ba07947984546b3b8a4b5a774d0d2/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 22:30:29,803 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.541454749663616\nEpoch 19 loss is 0.8791410713849316\nEpoch 29 loss is 0.8839316416940527\nEpoch 39 loss is 0.8966399825316692\nEpoch 49 loss is 0.9264105355661884\nEpoch 59 loss is 0.951371120459427\nEpoch 69 loss is 0.9412028121256291\nEpoch 79 loss is 0.9732946912045937\nEpoch 89 loss is 0.9181100154489218\nEpoch 99 loss is 0.9329806316494657\nEpoch 109 loss is 0.9328285713006949\nEpoch 119 loss is 0.9419897937541761\nEpoch 129 loss is 0.9427600691188952\nEpoch 139 loss is 0.9091315900275359\nEpoch 149 loss is 0.9324776865430842\nEpoch 159 loss is 0.9134668719369948\nEpoch 169 loss is 0.9403598662976411\nEpoch 179 loss is 0.9422566273044115\nEpoch 189 loss is 0.9543148096619407\nEpoch 199 loss is 0.9549645974775849\nTrain Acc.:  0.5539342406050377\n              precision    recall  f1-score   support\n\n           0       0.77      0.60      0.68      1063\n           1       0.65      0.47      0.55      1064\n           2       0.94      0.67      0.78      1064\n           3       0.89      0.62      0.73      1064\n           4       0.32      0.16      0.21      1064\n           5       0.73      0.69      0.71      1063\n           6       0.82      0.44      0.57      1064\n           7       0.91      0.69      0.78      1064\n           8       0.98      0.95      0.97      1064\n           9       0.68      0.90      0.77      1064\n          10       0.76      0.69      0.72      1064\n\n   micro avg       0.78      0.63      0.69     11702\n   macro avg       0.77      0.63      0.68     11702\nweighted avg       0.77      0.63      0.68     11702\n samples avg       0.59      0.63      0.60     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.34781102538165637\nEpoch 19 loss is 0.4926261439071808\nEpoch 29 loss is 0.6328775813367924\nEpoch 39 loss is 0.7170839574504813\nEpoch 49 loss is 0.6934163739177756\nEpoch 59 loss is 0.7141162567964217\nEpoch 69 loss is 0.7691114368900678\nEpoch 79 loss is 0.7562372003321447\nEpoch 89 loss is 0.7766109368260417\nEpoch 99 loss is 0.7075230256227628\nEpoch 109 loss is 0.7393482060014152\nEpoch 119 loss is 0.7458410915657656\nEpoch 129 loss is 0.7585006377982786\nEpoch 139 loss is 0.7711069196763435\nEpoch 149 loss is 0.7341288291259812\nEpoch 159 loss is 0.7801133649364619\nEpoch 169 loss is 0.7263681341448134\nEpoch 179 loss is 0.7519011054649812\nEpoch 189 loss is 0.7048160484621695\nEpoch 199 loss is 0.7475459901191056\nTrain Acc.:  0.7177986198645502\n              precision    recall  f1-score   support\n\n           0       0.89      0.83      0.86      1063\n           1       0.83      0.74      0.78      1064\n           2       0.94      0.84      0.89      1064\n           3       0.82      0.79      0.81      1063\n           4       0.68      0.66      0.67      1064\n           5       0.83      0.83      0.83      1064\n           6       0.70      0.60      0.64      1064\n           7       0.99      0.94      0.96      1064\n           8       0.99      0.97      0.98      1064\n           9       0.83      0.75      0.79      1064\n          10       0.76      0.75      0.76      1064\n\n   micro avg       0.84      0.79      0.82     11702\n   macro avg       0.84      0.79      0.81     11702\nweighted avg       0.84      0.79      0.81     11702\n samples avg       0.75      0.79      0.77     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.608739696487545\nEpoch 19 loss is 0.7696193880760408\nEpoch 29 loss is 0.803278156062957\nEpoch 39 loss is 0.8387883156242925\nEpoch 49 loss is 0.7807021280520069\nEpoch 59 loss is 0.8088241619223006\nEpoch 69 loss is 0.8070463633451938\nEpoch 79 loss is 0.7945587323244073\nEpoch 89 loss is 0.8392897794392442\nEpoch 99 loss is 0.8214029347151293\nEpoch 109 loss is 0.8340895850326423\nEpoch 119 loss is 0.8219225005001145\nEpoch 129 loss is 0.8507321513973675\nEpoch 139 loss is 0.8635722140704808\nEpoch 149 loss is 0.8446035024910838\nEpoch 159 loss is 0.8153800610100258\nEpoch 169 loss is 0.860221271499099\nEpoch 179 loss is 0.8713979258843838\nEpoch 189 loss is 0.8423941483588755\nEpoch 199 loss is 0.7982078285674677\nTrain Acc.:  0.5234687119447946\n              precision    recall  f1-score   support\n\n           0       0.82      0.76      0.79      1064\n           1       0.73      0.59      0.66      1064\n           2       0.59      0.22      0.33      1064\n           3       0.75      0.68      0.71      1063\n           4       0.55      0.52      0.54      1064\n           5       0.49      0.48      0.48      1064\n           6       0.70      0.56      0.62      1063\n           7       0.88      0.77      0.83      1064\n           8       0.99      0.92      0.95      1064\n           9       0.76      0.57      0.65      1064\n          10       0.85      0.73      0.79      1064\n\n   micro avg       0.74      0.62      0.68     11702\n   macro avg       0.74      0.62      0.67     11702\nweighted avg       0.74      0.62      0.67     11702\n samples avg       0.57      0.62      0.58     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2881004625544315\nEpoch 19 loss is 0.257601894880915\nEpoch 29 loss is 0.3070432292199646\nEpoch 39 loss is 1.0114987272533784\nEpoch 49 loss is 1.0626524147227299\nEpoch 59 loss is 0.7220235506940849\nEpoch 69 loss is 0.8701604350827594\nEpoch 79 loss is 1.2108727994663198\nEpoch 89 loss is 1.138239338460986\nEpoch 99 loss is 1.4603121925675742\nEpoch 109 loss is 1.4954496178854135\nEpoch 119 loss is 1.2679945725843622\nEpoch 129 loss is 1.0164146082349543\nEpoch 139 loss is 1.1297298068650685\nEpoch 149 loss is 1.253219169435164\nEpoch 159 loss is 1.1736291050368979\nEpoch 169 loss is 1.1826341111065124\nEpoch 179 loss is 1.4031484346107788\nEpoch 189 loss is 1.2205340600894026\nEpoch 199 loss is 1.3048065655004846\nTrain Acc.:  0.7068600850300168\n              precision    recall  f1-score   support\n\n           0       0.75      0.86      0.80      1063\n           1       0.81      0.79      0.80      1064\n           2       0.94      0.85      0.89      1064\n           3       0.87      0.84      0.85      1063\n           4       0.73      0.48      0.58      1064\n           5       0.85      0.77      0.81      1064\n           6       0.84      0.65      0.73      1064\n           7       0.97      0.97      0.97      1064\n           8       0.99      0.95      0.97      1064\n           9       0.53      0.97      0.69      1064\n          10       0.92      0.76      0.83      1064\n\n   micro avg       0.81      0.81      0.81     11702\n   macro avg       0.84      0.81      0.81     11702\nweighted avg       0.84      0.81      0.81     11702\n samples avg       0.75      0.81      0.77     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5949722022582887\nEpoch 19 loss is 0.8542236273844039\nEpoch 29 loss is 0.8749541100255886\nEpoch 39 loss is 0.9226380212308712\nEpoch 49 loss is 0.9189451568276324\nEpoch 59 loss is 0.9365599144446709\nEpoch 69 loss is 0.9302873031362572\nEpoch 79 loss is 0.9133083164488084\nEpoch 89 loss is 0.867765838931149\nEpoch 99 loss is 0.9202292373715069\nEpoch 109 loss is 0.8840360283963321\nEpoch 119 loss is 0.8900084272652079\nEpoch 129 loss is 0.8624389865444407\nEpoch 139 loss is 0.86264759032102\nEpoch 149 loss is 0.877634036686458\nEpoch 159 loss is 0.8911980209206599\nEpoch 169 loss is 0.8815296026365007\nEpoch 179 loss is 0.8890566057952398\nEpoch 189 loss is 0.8732226294946295\nEpoch 199 loss is 0.9232258855476119\nTrain Acc.:  0.5140470442455188\n              precision    recall  f1-score   support\n\n           0       0.80      0.66      0.72      1064\n           1       0.72      0.55      0.62      1064\n           2       0.75      0.53      0.62      1064\n           3       0.75      0.55      0.64      1064\n           4       0.49      0.36      0.42      1064\n           5       0.76      0.72      0.74      1064\n           6       0.56      0.38      0.45      1063\n           7       0.94      0.90      0.92      1064\n           8       0.98      0.97      0.98      1063\n           9       0.64      0.47      0.54      1064\n          10       0.75      0.50      0.60      1064\n\n   micro avg       0.75      0.60      0.67     11702\n   macro avg       0.74      0.60      0.66     11702\nweighted avg       0.74      0.60      0.66     11702\n samples avg       0.55      0.60      0.57     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=602b39a918d54282b2aaed44b2781349\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/602b39a918d54282b2aaed44b2781349/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 22:48:18,328 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.37705022557280987\nEpoch 19 loss is 0.2740187803883329\nEpoch 29 loss is 0.45516830949325443\nEpoch 39 loss is 0.6148356752471408\nEpoch 49 loss is 0.6222640142110359\nEpoch 59 loss is 0.6982017039915205\nEpoch 69 loss is 0.6373358815772685\nEpoch 79 loss is 0.6311755328631491\nEpoch 89 loss is 0.6082573092923741\nEpoch 99 loss is 0.603828268133041\nEpoch 109 loss is 0.6389025164997437\nEpoch 119 loss is 0.6260305680358405\nEpoch 129 loss is 0.6324748094532601\nEpoch 139 loss is 0.5943924798856283\nEpoch 149 loss is 0.6344508674597645\nEpoch 159 loss is 0.6141789833696603\nEpoch 169 loss is 0.6086970307893493\nEpoch 179 loss is 0.5824319841233837\nEpoch 189 loss is 0.6191231138277477\nEpoch 199 loss is 0.5666659157355745\nTrain Acc.:  0.730681308351315\n              precision    recall  f1-score   support\n\n           0       0.94      0.91      0.92      1063\n           1       0.82      0.70      0.76      1064\n           2       0.86      0.73      0.79      1064\n           3       0.89      0.83      0.86      1064\n           4       0.84      0.81      0.82      1064\n           5       0.86      0.76      0.81      1063\n           6       0.75      0.87      0.81      1064\n           7       0.97      0.91      0.94      1064\n           8       1.00      0.97      0.99      1064\n           9       0.80      0.68      0.73      1064\n          10       0.70      0.53      0.60      1064\n\n   micro avg       0.86      0.79      0.82     11702\n   macro avg       0.86      0.79      0.82     11702\nweighted avg       0.86      0.79      0.82     11702\n samples avg       0.76      0.79      0.77     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28806597272279794\nEpoch 19 loss is 0.22970233591793254\nEpoch 29 loss is 0.28585426998250507\nEpoch 39 loss is 0.38502680568849784\nEpoch 49 loss is 0.7625332656829702\nEpoch 59 loss is 0.6732264557756368\nEpoch 69 loss is 0.6217295541960539\nEpoch 79 loss is 0.6893322196897432\nEpoch 89 loss is 0.6622742723169387\nEpoch 99 loss is 0.6960203626199706\nEpoch 109 loss is 0.6777245937457624\nEpoch 119 loss is 0.7176237176692954\nEpoch 129 loss is 0.742547024761961\nEpoch 139 loss is 0.7291200957471028\nEpoch 149 loss is 0.7470137933371792\nEpoch 159 loss is 0.7220883556014549\nEpoch 169 loss is 0.7159556336891169\nEpoch 179 loss is 0.7108887748361287\nEpoch 189 loss is 0.7436282315327294\nEpoch 199 loss is 0.694954785146677\nTrain Acc.:  0.7865276561198111\n              precision    recall  f1-score   support\n\n           0       0.95      0.94      0.94      1063\n           1       0.80      0.74      0.77      1064\n           2       0.90      0.76      0.82      1064\n           3       0.93      0.90      0.92      1063\n           4       0.85      0.81      0.83      1064\n           5       0.81      0.72      0.76      1064\n           6       0.76      0.70      0.73      1064\n           7       0.99      0.93      0.96      1064\n           8       1.00      0.98      0.99      1064\n           9       0.87      0.88      0.87      1064\n          10       0.91      0.89      0.90      1064\n\n   micro avg       0.89      0.84      0.86     11702\n   macro avg       0.89      0.84      0.86     11702\nweighted avg       0.89      0.84      0.86     11702\n samples avg       0.81      0.84      0.82     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2511134256926335\nEpoch 19 loss is 0.22011394441410292\nEpoch 29 loss is 0.22056851414206183\nEpoch 39 loss is 0.2551644861115903\nEpoch 49 loss is 0.47249132940476685\nEpoch 59 loss is 0.6799162460754719\nEpoch 69 loss is 0.6906088651202789\nEpoch 79 loss is 0.7231533868835432\nEpoch 89 loss is 0.738398383750411\nEpoch 99 loss is 0.7811543296378222\nEpoch 109 loss is 0.7535960303277581\nEpoch 119 loss is 0.7163676381713118\nEpoch 129 loss is 0.7103477423606941\nEpoch 139 loss is 0.7078374401246286\nEpoch 149 loss is 0.7199227568498184\nEpoch 159 loss is 0.7107330170199827\nEpoch 169 loss is 0.7056031770853555\nEpoch 179 loss is 0.7327502640763232\nEpoch 189 loss is 0.7094453790723616\nEpoch 199 loss is 0.7120579963814156\nTrain Acc.:  0.7947956502232572\n              precision    recall  f1-score   support\n\n           0       0.94      0.91      0.93      1064\n           1       0.85      0.80      0.82      1064\n           2       0.87      0.77      0.82      1064\n           3       0.93      0.86      0.89      1063\n           4       0.84      0.82      0.83      1064\n           5       0.85      0.86      0.86      1064\n           6       0.82      0.70      0.75      1063\n           7       0.96      0.90      0.93      1064\n           8       0.99      0.95      0.97      1064\n           9       0.86      0.87      0.86      1064\n          10       0.90      0.87      0.88      1064\n\n   micro avg       0.89      0.85      0.87     11702\n   macro avg       0.89      0.85      0.87     11702\nweighted avg       0.89      0.85      0.87     11702\n samples avg       0.82      0.85      0.83     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28920254058590406\nEpoch 19 loss is 0.20422793527357072\nEpoch 29 loss is 0.23514562707658523\nEpoch 39 loss is 0.30951769319469474\nEpoch 49 loss is 0.802884197886128\nEpoch 59 loss is 0.629426963749978\nEpoch 69 loss is 0.6618189152336303\nEpoch 79 loss is 0.639523350080704\nEpoch 89 loss is 0.6033113390242072\nEpoch 99 loss is 0.6365305132984823\nEpoch 109 loss is 0.6149712207622507\nEpoch 119 loss is 0.6332012921437696\nEpoch 129 loss is 0.6520249413523534\nEpoch 139 loss is 0.6450135768078111\nEpoch 149 loss is 0.6724773114718959\nEpoch 159 loss is 0.637959073743463\nEpoch 169 loss is 0.5887977689032348\nEpoch 179 loss is 0.6275123627575196\nEpoch 189 loss is 0.6157577124370395\nEpoch 199 loss is 0.5571599018372312\nTrain Acc.:  0.7962056957292712\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92      1063\n           1       0.84      0.81      0.83      1064\n           2       0.93      0.86      0.90      1064\n           3       0.89      0.87      0.88      1063\n           4       0.87      0.77      0.82      1064\n           5       0.84      0.81      0.83      1064\n           6       0.76      0.61      0.68      1064\n           7       0.99      0.98      0.98      1064\n           8       1.00      0.99      1.00      1064\n           9       0.83      0.77      0.80      1064\n          10       0.93      0.91      0.92      1064\n\n   micro avg       0.90      0.84      0.87     11702\n   macro avg       0.90      0.84      0.87     11702\nweighted avg       0.90      0.84      0.87     11702\n samples avg       0.82      0.84      0.83     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3162340976591865\nEpoch 19 loss is 0.23035400757661567\nEpoch 29 loss is 0.3025872000844228\nEpoch 39 loss is 0.5825874736764202\nEpoch 49 loss is 0.5903150798951642\nEpoch 59 loss is 0.5290111967616761\nEpoch 69 loss is 0.48948755510049063\nEpoch 79 loss is 0.5536584861929144\nEpoch 89 loss is 0.5714292967837328\nEpoch 99 loss is 0.6023988021147385\nEpoch 109 loss is 0.6004173386515079\nEpoch 119 loss is 0.5901975806362378\nEpoch 129 loss is 0.607343459187801\nEpoch 139 loss is 0.6290455412143036\nEpoch 149 loss is 0.598815193614319\nEpoch 159 loss is 0.5928518543280813\nEpoch 169 loss is 0.6891633674581874\nEpoch 179 loss is 0.6508888532882792\nEpoch 189 loss is 0.6662645081833795\nEpoch 199 loss is 0.6955834762648595\nTrain Acc.:  0.7916550943234987\n              precision    recall  f1-score   support\n\n           0       0.96      0.92      0.94      1064\n           1       0.78      0.69      0.73      1064\n           2       0.89      0.81      0.85      1064\n           3       0.94      0.87      0.91      1064\n           4       0.95      0.89      0.92      1064\n           5       0.82      0.78      0.80      1064\n           6       0.77      0.78      0.78      1063\n           7       0.99      0.98      0.98      1064\n           8       1.00      0.98      0.99      1063\n           9       0.78      0.75      0.77      1064\n          10       0.93      0.82      0.87      1064\n\n   micro avg       0.89      0.84      0.87     11702\n   macro avg       0.89      0.84      0.87     11702\nweighted avg       0.89      0.84      0.87     11702\n samples avg       0.82      0.84      0.82     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=e0d8b1347f9249748b5b9f57cb06d03e\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/e0d8b1347f9249748b5b9f57cb06d03e/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 23:19:30,934 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.28872390806294274\nEpoch 19 loss is 0.17694585572096805\nEpoch 29 loss is 0.1425961563405807\nEpoch 39 loss is 0.12819857234772986\nEpoch 49 loss is 0.127805551287352\nEpoch 59 loss is 0.11392432030870624\nEpoch 69 loss is 0.09727040211071439\nEpoch 79 loss is 0.09686726724688864\nEpoch 89 loss is 0.09030257402693799\nEpoch 99 loss is 0.09027066309985042\nEpoch 109 loss is 0.08883324592656906\nEpoch 119 loss is 0.08907481171375083\nEpoch 129 loss is 0.08302457843559832\nEpoch 139 loss is 0.07895430465486417\nEpoch 149 loss is 0.07612614811554756\nEpoch 159 loss is 0.0712898657302396\nEpoch 169 loss is 0.08380639698034706\nEpoch 179 loss is 0.07950043128200916\nEpoch 189 loss is 0.08736764655981065\nEpoch 199 loss is 0.09124728414202767\nTrain Acc.:  0.9411199179609887\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.93      0.91      0.92      1064\n           2       0.99      0.98      0.98      1064\n           3       0.97      0.97      0.97      1064\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.94      0.93      1063\n           6       0.90      0.91      0.91      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.96      0.93      0.94      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.96      0.95      0.96     11702\n   macro avg       0.96      0.95      0.96     11702\nweighted avg       0.96      0.95      0.96     11702\n samples avg       0.94      0.95      0.95     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2861673749427943\nEpoch 19 loss is 0.2201654650800628\nEpoch 29 loss is 0.1594589809702913\nEpoch 39 loss is 0.13540895566103234\nEpoch 49 loss is 0.11925421445742292\nEpoch 59 loss is 0.11962160650088925\nEpoch 69 loss is 0.11841529734011198\nEpoch 79 loss is 0.1179027595753925\nEpoch 89 loss is 0.1180392008192653\nEpoch 99 loss is 0.11441886133599923\nEpoch 109 loss is 0.10782047828315887\nEpoch 119 loss is 0.10705718301534183\nEpoch 129 loss is 0.10427072867703205\nEpoch 139 loss is 0.10474216368094652\nEpoch 149 loss is 0.10551091888301718\nEpoch 159 loss is 0.10684330984217699\nEpoch 169 loss is 0.09317834493277641\nEpoch 179 loss is 0.0892558164282839\nEpoch 189 loss is 0.08739703451707423\nEpoch 199 loss is 0.09199028421820245\nTrain Acc.:  0.9275749353729144\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      1063\n           1       0.92      0.88      0.90      1064\n           2       0.98      0.98      0.98      1064\n           3       0.97      0.95      0.96      1063\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.95      0.94      1064\n           6       0.89      0.89      0.89      1064\n           7       0.99      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.93      0.94      1064\n          10       0.96      0.91      0.93      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.27623336473650534\nEpoch 19 loss is 0.16289381252737087\nEpoch 29 loss is 0.14757027886946592\nEpoch 39 loss is 0.1330734318315473\nEpoch 49 loss is 0.11679790998549922\nEpoch 59 loss is 0.11167467752516336\nEpoch 69 loss is 0.11268039833791481\nEpoch 79 loss is 0.10041605954925376\nEpoch 89 loss is 0.10264455966541974\nEpoch 99 loss is 0.10157209891611255\nEpoch 109 loss is 0.09768154280892237\nEpoch 119 loss is 0.09983357506318141\nEpoch 129 loss is 0.09887546788082555\nEpoch 139 loss is 0.0888411840117014\nEpoch 149 loss is 0.10038184391363476\nEpoch 159 loss is 0.08949124574344279\nEpoch 169 loss is 0.09784304006405145\nEpoch 179 loss is 0.09877557601509657\nEpoch 189 loss is 0.10217435454327402\nEpoch 199 loss is 0.1057261382783362\nTrain Acc.:  0.9322750870596278\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1064\n           1       0.91      0.91      0.91      1064\n           2       0.98      0.96      0.97      1064\n           3       0.97      0.96      0.96      1063\n           4       0.91      0.91      0.91      1064\n           5       0.93      0.93      0.93      1064\n           6       0.91      0.88      0.89      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.94      0.94      1064\n          10       0.98      0.95      0.97      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28693840838047774\nEpoch 19 loss is 0.20104936966743658\nEpoch 29 loss is 0.15755177961335037\nEpoch 39 loss is 0.1288596346987444\nEpoch 49 loss is 0.12720331433512763\nEpoch 59 loss is 0.12023218874962663\nEpoch 69 loss is 0.11721781830240018\nEpoch 79 loss is 0.10656089263296499\nEpoch 89 loss is 0.09562316810246828\nEpoch 99 loss is 0.09102231637919944\nEpoch 109 loss is 0.09233467007446912\nEpoch 119 loss is 0.09115643363442122\nEpoch 129 loss is 0.08620270800916764\nEpoch 139 loss is 0.11083394580512573\nEpoch 149 loss is 0.10150878655964381\nEpoch 159 loss is 0.10271093352938551\nEpoch 169 loss is 0.10444855459263125\nEpoch 179 loss is 0.09594796640621853\nEpoch 189 loss is 0.09849182497560287\nEpoch 199 loss is 0.1036780848914241\nTrain Acc.:  0.924904394641827\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.97      1063\n           1       0.91      0.88      0.89      1064\n           2       0.98      0.96      0.97      1064\n           3       0.96      0.97      0.97      1063\n           4       0.91      0.90      0.91      1064\n           5       0.91      0.91      0.91      1064\n           6       0.89      0.89      0.89      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.93      0.93      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3034027193057795\nEpoch 19 loss is 0.17692890093552524\nEpoch 29 loss is 0.13786514238212272\nEpoch 39 loss is 0.12331394422598127\nEpoch 49 loss is 0.1237987767889628\nEpoch 59 loss is 0.11048595570606672\nEpoch 69 loss is 0.09570147391418123\nEpoch 79 loss is 0.09912258426639196\nEpoch 89 loss is 0.09599648621108799\nEpoch 99 loss is 0.0873050642910436\nEpoch 109 loss is 0.10142020273437256\nEpoch 119 loss is 0.1003337232560557\nEpoch 129 loss is 0.10045847144167147\nEpoch 139 loss is 0.09709811021654463\nEpoch 149 loss is 0.0970311573500608\nEpoch 159 loss is 0.09226866990186595\nEpoch 169 loss is 0.08758580671534028\nEpoch 179 loss is 0.08792805539701785\nEpoch 189 loss is 0.08792941901672958\nEpoch 199 loss is 0.09815684767604878\nTrain Acc.:  0.9300959258230607\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1064\n           1       0.91      0.90      0.91      1064\n           2       0.98      0.97      0.97      1064\n           3       0.98      0.95      0.97      1064\n           4       0.94      0.93      0.93      1064\n           5       0.93      0.93      0.93      1064\n           6       0.90      0.88      0.89      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.93      0.89      0.91      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=f1a6a6d4401f4f458770c1073780e682\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/f1a6a6d4401f4f458770c1073780e682/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]',\n 'loss': 'ComplexMSE_adjusted_error'}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-28 00:04:38,312 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.38493101436237465\nEpoch 19 loss is 0.1983475963903122\nEpoch 29 loss is 0.15240658259145864\nEpoch 39 loss is 0.12849809119734149\nEpoch 49 loss is 0.11291338514495056\nEpoch 59 loss is 0.10350603182093213\nEpoch 69 loss is 0.09896006818913682\nEpoch 79 loss is 0.09073580641413742\nEpoch 89 loss is 0.08624265189956253\nEpoch 99 loss is 0.07990884906610474\nEpoch 109 loss is 0.07774202154480073\nEpoch 119 loss is 0.0778207451859103\nEpoch 129 loss is 0.07292320971236593\nEpoch 139 loss is 0.08220096647261978\nEpoch 149 loss is 0.07259278084390995\nEpoch 159 loss is 0.07333743296400583\nEpoch 169 loss is 0.06768991486550634\nEpoch 179 loss is 0.06486413256485078\nEpoch 189 loss is 0.0663716789708822\nEpoch 199 loss is 0.06601747100440022\nTrain Acc.:  0.9551562800435832\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1063\n           1       0.93      0.92      0.93      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.98      0.98      1064\n           4       0.95      0.92      0.93      1064\n           5       0.94      0.94      0.94      1063\n           6       0.92      0.91      0.92      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.92      0.93      1064\n          10       0.96      0.96      0.96      1064\n\n   micro avg       0.96      0.95      0.96     11702\n   macro avg       0.96      0.95      0.96     11702\nweighted avg       0.96      0.95      0.96     11702\n samples avg       0.94      0.95      0.95     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.4015647773116825\nEpoch 19 loss is 0.21324193953061343\nEpoch 29 loss is 0.16244134629623966\nEpoch 39 loss is 0.13968662879021496\nEpoch 49 loss is 0.1220346592068794\nEpoch 59 loss is 0.11857350072574051\nEpoch 69 loss is 0.10683356304935711\nEpoch 79 loss is 0.09876512760527531\nEpoch 89 loss is 0.10028214707730271\nEpoch 99 loss is 0.09386611501220359\nEpoch 109 loss is 0.09131180958378789\nEpoch 119 loss is 0.08078602213530815\nEpoch 129 loss is 0.07812674516622654\nEpoch 139 loss is 0.07368491314841398\nEpoch 149 loss is 0.07233171681695082\nEpoch 159 loss is 0.07153585909897314\nEpoch 169 loss is 0.07045204344853243\nEpoch 179 loss is 0.07031483192734578\nEpoch 189 loss is 0.06762652182875349\nEpoch 199 loss is 0.06935222633418742\nTrain Acc.:  0.9489606255474609\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.94      0.90      0.92      1064\n           2       0.98      0.97      0.97      1064\n           3       0.98      0.97      0.97      1063\n           4       0.95      0.94      0.94      1064\n           5       0.94      0.93      0.93      1064\n           6       0.91      0.89      0.90      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      0.99      1.00      1064\n           9       0.96      0.94      0.95      1064\n          10       0.98      0.95      0.97      1064\n\n   micro avg       0.96      0.95      0.96     11702\n   macro avg       0.96      0.95      0.96     11702\nweighted avg       0.96      0.95      0.96     11702\n samples avg       0.94      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.35830112541567166\nEpoch 19 loss is 0.20503370425712947\nEpoch 29 loss is 0.1615004405052233\nEpoch 39 loss is 0.14478501770353844\nEpoch 49 loss is 0.13139543735043546\nEpoch 59 loss is 0.12132454251093122\nEpoch 69 loss is 0.11744575716216751\nEpoch 79 loss is 0.11623840924937114\nEpoch 89 loss is 0.11783017747510419\nEpoch 99 loss is 0.11593023945022461\nEpoch 109 loss is 0.11583587087231882\nEpoch 119 loss is 0.11662312303478367\nEpoch 129 loss is 0.11691969687131881\nEpoch 139 loss is 0.10854572296958728\nEpoch 149 loss is 0.10796596240824434\nEpoch 159 loss is 0.11262345830304454\nEpoch 169 loss is 0.10184479776708627\nEpoch 179 loss is 0.09605088202087457\nEpoch 189 loss is 0.09299509987805853\nEpoch 199 loss is 0.08898415836437445\nTrain Acc.:  0.9210374516632127\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1064\n           1       0.91      0.89      0.90      1064\n           2       0.98      0.95      0.97      1064\n           3       0.96      0.94      0.95      1063\n           4       0.92      0.88      0.90      1064\n           5       0.90      0.90      0.90      1064\n           6       0.89      0.88      0.89      1063\n           7       1.00      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.93      0.88      0.90      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.95      0.93      0.94     11702\n   macro avg       0.95      0.93      0.94     11702\nweighted avg       0.95      0.93      0.94     11702\n samples avg       0.91      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3732331507452995\nEpoch 19 loss is 0.2151142782886408\nEpoch 29 loss is 0.1693622907950432\nEpoch 39 loss is 0.14709019657419897\nEpoch 49 loss is 0.13360652890401162\nEpoch 59 loss is 0.12738859159159088\nEpoch 69 loss is 0.11634673967810212\nEpoch 79 loss is 0.10559853905524143\nEpoch 89 loss is 0.10185721106569144\nEpoch 99 loss is 0.10076346263612189\nEpoch 109 loss is 0.0951012106512096\nEpoch 119 loss is 0.0863301443776767\nEpoch 129 loss is 0.08374395534767522\nEpoch 139 loss is 0.08192660695129174\nEpoch 149 loss is 0.0808713694226275\nEpoch 159 loss is 0.08485596063007796\nEpoch 169 loss is 0.07577642509867422\nEpoch 179 loss is 0.07399703079870644\nEpoch 189 loss is 0.07316694645295278\nEpoch 199 loss is 0.07004600393121947\nTrain Acc.:  0.9438972803213195\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1063\n           1       0.93      0.91      0.92      1064\n           2       0.98      0.96      0.97      1064\n           3       0.97      0.97      0.97      1063\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.92      0.93      1064\n           6       0.90      0.89      0.89      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.93      0.90      0.92      1064\n          10       0.96      0.95      0.96      1064\n\n   micro avg       0.96      0.95      0.95     11702\n   macro avg       0.96      0.95      0.95     11702\nweighted avg       0.96      0.95      0.95     11702\n samples avg       0.93      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_25607/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3860148494765397\nEpoch 19 loss is 0.20866529165214284\nEpoch 29 loss is 0.16247025892118347\nEpoch 39 loss is 0.13564982800723918\nEpoch 49 loss is 0.1240512973257284\nEpoch 59 loss is 0.113934101224775\nEpoch 69 loss is 0.11061596169703837\nEpoch 79 loss is 0.10461476570112285\nEpoch 89 loss is 0.10298748909019034\nEpoch 99 loss is 0.09786684313663066\nEpoch 109 loss is 0.0988537740332937\nEpoch 119 loss is 0.09090053269267047\nEpoch 129 loss is 0.08742530763329877\nEpoch 139 loss is 0.0841747421040788\nEpoch 149 loss is 0.08610848018045644\nEpoch 159 loss is 0.07819941018533352\nEpoch 169 loss is 0.07596165720615952\nEpoch 179 loss is 0.07426260096075489\nEpoch 189 loss is 0.07633927124679218\nEpoch 199 loss is 0.07405352525764859\nTrain Acc.:  0.9421454056017262\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1064\n           1       0.91      0.90      0.91      1064\n           2       0.97      0.96      0.96      1064\n           3       0.97      0.96      0.97      1064\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.94      0.94      1064\n           6       0.91      0.88      0.90      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.93      0.90      0.92      1064\n          10       0.98      0.95      0.97      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.94     11702\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip_pa_loss.html",
    "href": "examples/autass/autass_single_run_angle_clip_pa_loss.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip_pa_loss.html#config",
    "href": "examples/autass/autass_single_run_angle_clip_pa_loss.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1\nclip_angle_value = 1000000"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip_pa_loss.html#single-layer",
    "href": "examples/autass/autass_single_run_angle_clip_pa_loss.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr, clip_angle_value=clip_angle_value)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=605b9b0974d14aa08dac5c7e3519c776\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/605b9b0974d14aa08dac5c7e3519c776/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 22:34:24,956 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.2580754619244136\nEpoch 19 loss is 0.21683012693345186\nEpoch 29 loss is 0.24423590104284812\nEpoch 39 loss is 0.27051340395520046\nEpoch 49 loss is 0.2290261834515428\nEpoch 59 loss is 0.2985538988735455\nEpoch 69 loss is 0.3595716436054556\nEpoch 79 loss is 0.4444515348247725\nEpoch 89 loss is 0.38659444896523715\nEpoch 99 loss is 0.38398743291105353\nEpoch 109 loss is 0.3754874847210009\nEpoch 119 loss is 0.3750323965195017\nEpoch 129 loss is 0.37294807279904635\nEpoch 139 loss is 0.37278288673665216\nEpoch 149 loss is 0.37282104403105726\nEpoch 159 loss is 0.3728814936841066\nEpoch 169 loss is 0.37325801241579976\nEpoch 179 loss is 0.37343925531907135\nEpoch 189 loss is 0.3743756613217295\nEpoch 199 loss is 0.3748686972032385\nTrain Acc.:  0.8230782378327565\nVal Acc.:  0.8171409040416987\n              precision    recall  f1-score   support\n\n           0       0.88      0.91      0.89      1074\n           1       0.92      0.71      0.80      1089\n           2       0.97      0.90      0.93      1044\n           3       0.94      0.93      0.94      1048\n           4       0.86      0.70      0.77      1057\n           5       0.81      0.82      0.82      1072\n           6       0.84      0.79      0.82      1066\n           7       0.99      0.98      0.99      1103\n           8       1.00      0.99      0.99      1108\n           9       0.81      0.83      0.82      1030\n          10       0.93      0.82      0.87      1012\n\n   micro avg       0.91      0.85      0.88     11703\n   macro avg       0.91      0.85      0.88     11703\nweighted avg       0.91      0.85      0.88     11703\n samples avg       0.84      0.85      0.84     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=24a6f11cfe9346f89c139a54af6465f8\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/24a6f11cfe9346f89c139a54af6465f8/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 22:37:34,712 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.24476062344530095\nEpoch 19 loss is 0.17524997121406452\nEpoch 29 loss is 0.17326589625589803\nEpoch 39 loss is 0.1250403376833135\nEpoch 49 loss is 0.10869102016395354\nEpoch 59 loss is 0.12357040169954497\nEpoch 69 loss is 0.1093577461378208\nEpoch 79 loss is 0.12643380921745745\nEpoch 89 loss is 0.11328518219237213\nEpoch 99 loss is 0.10813876603811017\nEpoch 109 loss is 0.10728799886326053\nEpoch 119 loss is 0.09921635073504151\nEpoch 129 loss is 0.1086356377434932\nEpoch 139 loss is 0.10239754285657386\nEpoch 149 loss is 0.09024639101528273\nEpoch 159 loss is 0.10786458972685774\nEpoch 169 loss is 0.10565815323356467\nEpoch 179 loss is 0.10199340886787205\nEpoch 189 loss is 0.10471597717547888\nEpoch 199 loss is 0.10269092835171191\nTrain Acc.:  0.9379994017860958\nVal Acc.:  0.9320687003332478\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1074\n           1       0.95      0.92      0.93      1089\n           2       0.99      0.97      0.98      1044\n           3       0.97      0.97      0.97      1048\n           4       0.96      0.93      0.95      1057\n           5       0.91      0.91      0.91      1072\n           6       0.92      0.89      0.90      1066\n           7       1.00      0.99      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.95      0.92      0.93      1030\n          10       0.98      0.96      0.97      1012\n\n   micro avg       0.96      0.95      0.95     11703\n   macro avg       0.96      0.95      0.95     11703\nweighted avg       0.96      0.95      0.95     11703\n samples avg       0.94      0.95      0.94     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=6702fbaba3574fb49613b2911035194b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/6702fbaba3574fb49613b2911035194b/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 22:41:10,743 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.23981090673183392\nEpoch 19 loss is 0.1350937123107344\nEpoch 29 loss is 0.10171720961260702\nEpoch 39 loss is 0.10397829965296723\nEpoch 49 loss is 0.09251700067492151\nEpoch 59 loss is 0.07677589171885311\nEpoch 69 loss is 0.07428011077625409\nEpoch 79 loss is 0.07424593369765878\nEpoch 89 loss is 0.06974386731484813\nEpoch 99 loss is 0.0625939765088816\nEpoch 109 loss is 0.06981443338641498\nEpoch 119 loss is 0.06748007620098263\nEpoch 129 loss is 0.05657005163013897\nEpoch 139 loss is 0.06359593600342729\nEpoch 149 loss is 0.05271322280799125\nEpoch 159 loss is 0.046950683837239236\nEpoch 169 loss is 0.045860277729382115\nEpoch 179 loss is 0.041940594701453865\nEpoch 189 loss is 0.042831541937416555\nEpoch 199 loss is 0.04799205469350261\nTrain Acc.:  0.9769687646882879\nVal Acc.:  0.9669315560112791\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1074\n           1       0.96      0.96      0.96      1089\n           2       1.00      0.99      0.99      1044\n           3       0.99      0.98      0.98      1048\n           4       0.96      0.98      0.97      1057\n           5       0.96      0.96      0.96      1072\n           6       0.97      0.95      0.96      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.97      0.97      0.97      1030\n          10       0.99      0.97      0.98      1012\n\n   micro avg       0.98      0.98      0.98     11703\n   macro avg       0.98      0.98      0.98     11703\nweighted avg       0.98      0.98      0.98     11703\n samples avg       0.97      0.98      0.97     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=ce81f8c4a4d444eb8de416771703c3f3\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/ce81f8c4a4d444eb8de416771703c3f3/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 22:45:48,058 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.19675702136134435\nEpoch 19 loss is 0.12691860293194868\nEpoch 29 loss is 0.09963694001283371\nEpoch 39 loss is 0.08266232549928246\nEpoch 49 loss is 0.07721942842928552\nEpoch 59 loss is 0.07040153803166502\nEpoch 69 loss is 0.06772988802277076\nEpoch 79 loss is 0.06808096965213609\nEpoch 89 loss is 0.06277160699643843\nEpoch 99 loss is 0.06304586082936255\nEpoch 109 loss is 0.05947888365928943\nEpoch 119 loss is 0.056828984384148414\nEpoch 129 loss is 0.05502314843135909\nEpoch 139 loss is 0.04954769492662839\nEpoch 149 loss is 0.04978222914593748\nEpoch 159 loss is 0.05478295147543594\nEpoch 169 loss is 0.04662054887355213\nEpoch 179 loss is 0.05929304252375296\nEpoch 189 loss is 0.05981274953089023\nEpoch 199 loss is 0.05613814120733131\nTrain Acc.:  0.9749177455881725\nVal Acc.:  0.9561650858754166\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1074\n           1       0.94      0.95      0.94      1089\n           2       0.99      0.98      0.99      1044\n           3       0.99      0.99      0.99      1048\n           4       0.96      0.95      0.96      1057\n           5       0.96      0.93      0.94      1072\n           6       0.96      0.95      0.95      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.96      0.96      1030\n          10       0.99      0.97      0.98      1012\n\n   micro avg       0.97      0.97      0.97     11703\n   macro avg       0.97      0.97      0.97     11703\nweighted avg       0.97      0.97      0.97     11703\n samples avg       0.96      0.97      0.96     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_single_run_angle_clip_pa_loss.html#multi-layer",
    "href": "examples/autass/autass_single_run_angle_clip_pa_loss.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=a6128e9bc7c64377a790bf906a1bb800\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/a6128e9bc7c64377a790bf906a1bb800/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 22:52:36,115 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.5352860994082685\nEpoch 19 loss is 0.8573220308220786\nEpoch 29 loss is 0.8846553943619413\nEpoch 39 loss is 0.9282479485173039\nEpoch 49 loss is 0.9304585998948428\nEpoch 59 loss is 0.8896554528724867\nEpoch 69 loss is 0.9020712551731598\nEpoch 79 loss is 0.8780626206812697\nEpoch 89 loss is 0.9192603465644281\nEpoch 99 loss is 0.9080221700448701\nEpoch 109 loss is 0.8979155400653883\nEpoch 119 loss is 0.9165361645534543\nEpoch 129 loss is 0.8875104726098959\nEpoch 139 loss is 0.9474128621314876\nEpoch 149 loss is 0.8798036890266178\nEpoch 159 loss is 0.870499564750111\nEpoch 169 loss is 0.8812218883029092\nEpoch 179 loss is 0.8765249923026144\nEpoch 189 loss is 0.8565456302997135\nEpoch 199 loss is 0.8486741464838758\nTrain Acc.:  0.5802033927274282\nVal Acc.:  0.571990088011621\n              precision    recall  f1-score   support\n\n           0       0.79      0.73      0.76      1074\n           1       0.64      0.35      0.45      1089\n           2       0.92      0.74      0.82      1044\n           3       0.84      0.80      0.82      1048\n           4       0.49      0.32      0.38      1057\n           5       0.70      0.56      0.62      1072\n           6       0.66      0.49      0.57      1066\n           7       0.93      0.83      0.88      1103\n           8       0.91      0.86      0.88      1108\n           9       0.72      0.90      0.80      1030\n          10       0.82      0.59      0.69      1012\n\n   micro avg       0.78      0.65      0.71     11703\n   macro avg       0.77      0.65      0.70     11703\nweighted avg       0.77      0.65      0.70     11703\n samples avg       0.61      0.65      0.62     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=b237a248793b43efbec3300c283c1b12\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b237a248793b43efbec3300c283c1b12/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 22:56:11,994 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.33310382354735135\nEpoch 19 loss is 0.3045317527345633\nEpoch 29 loss is 0.26208047114462196\nEpoch 39 loss is 0.5633030280355086\nEpoch 49 loss is 0.6604212666659914\nEpoch 59 loss is 0.6350045622908638\nEpoch 69 loss is 0.6371642678164225\nEpoch 79 loss is 0.6625240454923111\nEpoch 89 loss is 0.618989009951792\nEpoch 99 loss is 0.5788566858980941\nEpoch 109 loss is 0.5580994790124282\nEpoch 119 loss is 0.5665000001440373\nEpoch 129 loss is 0.6303235022304811\nEpoch 139 loss is 0.6471938510996637\nEpoch 149 loss is 0.6645542731394183\nEpoch 159 loss is 0.6454662272330662\nEpoch 169 loss is 0.6336318409892531\nEpoch 179 loss is 0.6200715265569947\nEpoch 189 loss is 0.6395423856912091\nEpoch 199 loss is 0.5969744437662491\nTrain Acc.:  0.7452036063752511\nVal Acc.:  0.7395539605229429\n              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.93      1074\n           1       0.87      0.79      0.83      1089\n           2       0.88      0.75      0.81      1044\n           3       0.89      0.84      0.87      1048\n           4       0.79      0.71      0.75      1057\n           5       0.88      0.83      0.86      1072\n           6       0.77      0.69      0.73      1066\n           7       0.96      0.91      0.93      1103\n           8       1.00      0.98      0.99      1108\n           9       0.81      0.73      0.77      1030\n          10       0.81      0.61      0.69      1012\n\n   micro avg       0.88      0.80      0.84     11703\n   macro avg       0.87      0.80      0.83     11703\nweighted avg       0.87      0.80      0.83     11703\n samples avg       0.77      0.80      0.78     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=fd3418e73a054cb2882dd2956cc509d3\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/fd3418e73a054cb2882dd2956cc509d3/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 23:00:39,760 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.228445869053326\nEpoch 19 loss is 0.1769025202901553\nEpoch 29 loss is 0.13638551487260145\nEpoch 39 loss is 0.12389236050933781\nEpoch 49 loss is 0.10762237564656339\nEpoch 59 loss is 0.09565055571155039\nEpoch 69 loss is 0.08563549226626824\nEpoch 79 loss is 0.07961991527154336\nEpoch 89 loss is 0.08409453002137111\nEpoch 99 loss is 0.11271795819004764\nEpoch 109 loss is 0.09878662956475265\nEpoch 119 loss is 0.0966812858882719\nEpoch 129 loss is 0.08962629585333381\nEpoch 139 loss is 0.08695483521719193\nEpoch 149 loss is 0.08539091657127361\nEpoch 159 loss is 0.09094220320688623\nEpoch 169 loss is 0.09666773124329389\nEpoch 179 loss is 0.0897644800428595\nEpoch 189 loss is 0.09589186592403481\nEpoch 199 loss is 0.09725851786233339\nTrain Acc.:  0.9344528479254797\nVal Acc.:  0.9253182944544134\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97      1074\n           1       0.93      0.92      0.92      1089\n           2       0.98      0.96      0.97      1044\n           3       0.98      0.96      0.97      1048\n           4       0.93      0.94      0.94      1057\n           5       0.94      0.92      0.93      1072\n           6       0.92      0.89      0.91      1066\n           7       0.99      0.99      0.99      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.92      0.93      1030\n          10       0.98      0.94      0.96      1012\n\n   micro avg       0.96      0.95      0.95     11703\n   macro avg       0.96      0.95      0.95     11703\nweighted avg       0.96      0.95      0.95     11703\n samples avg       0.94      0.95      0.94     11703\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\nmodel = Model(categories=categories, periodicity=periodicity)\ncriterion = ComplexMSE_adjusted_error.apply\noptimizer = ECL(model.parameters(), lr=lr)\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"single_run\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=8b2cde0afc44486290f4e94181fbdeed\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/8b2cde0afc44486290f4e94181fbdeed/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nx_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\nlosses, scores = fit(\n    model,\n    x_train,\n    y_train,\n    epochs=epochs,\n    batch_size=batch_size,\n    optimizer=optimizer,\n    criterion=criterion,\n    categories=categories,\n    periodicity=periodicity,\n)\n\nmodel.load_state_dict(torch.load(PATH))\n\ny_pred = model.predict(x_train)\nacc = accuracy(y_pred.squeeze(), y_train)\nprint(\"Train Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Train Acc.\",\n    value=acc,\n)\n\ny_pred = model.predict(x_valid)\nacc = accuracy(y_pred.squeeze(), y_valid)\nprint(\"Val Acc.: \", acc)\nLogger.current_logger().report_single_value(\n    name=\"Val Acc.\",\n    value=acc,\n)\nprint(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n/tmp/ipykernel_4428/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-22 23:07:15,673 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.3850133302732534\nEpoch 19 loss is 0.22928350502576889\nEpoch 29 loss is 0.18473120489159367\nEpoch 39 loss is 0.151906876717256\nEpoch 49 loss is 0.13306868079012138\nEpoch 59 loss is 0.12176301270741918\nEpoch 69 loss is 0.11425308696387655\nEpoch 79 loss is 0.1074521977072848\nEpoch 89 loss is 0.09860157716737963\nEpoch 99 loss is 0.09525496381491966\nEpoch 109 loss is 0.09652682279150973\nEpoch 119 loss is 0.09278257367639477\nEpoch 129 loss is 0.09085598133404435\nEpoch 139 loss is 0.0890178305214046\nEpoch 149 loss is 0.08561603038010733\nEpoch 159 loss is 0.08194544436834562\nEpoch 169 loss is 0.07805339709847767\nEpoch 179 loss is 0.08145146698926171\nEpoch 189 loss is 0.07395142641579694\nEpoch 199 loss is 0.07222291978897587\nTrain Acc.:  0.945583899500064\nVal Acc.:  0.924292916346236\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.93      0.92      0.92      1089\n           2       0.99      0.96      0.97      1044\n           3       0.98      0.96      0.97      1048\n           4       0.96      0.95      0.96      1057\n           5       0.94      0.92      0.93      1072\n           6       0.93      0.87      0.90      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.91      0.91      0.91      1030\n          10       0.97      0.94      0.96      1012\n\n   micro avg       0.96      0.94      0.95     11703\n   macro avg       0.96      0.94      0.95     11703\nweighted avg       0.96      0.94      0.95     11703\n samples avg       0.93      0.94      0.94     11703\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_initializers.html",
    "href": "examples/autass/autass_initializers.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_initializers.html#config",
    "href": "examples/autass/autass_initializers.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 50\nbatch_size = 538\nlr = 1"
  },
  {
    "objectID": "examples/autass/autass_initializers.html#single-layer",
    "href": "examples/autass/autass_initializers.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n    def initialize_weights(self, initilizer=\"uniform\"):\n        if initilizer == \"uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_uniform_independent_(m.weights, -0.5, 0.5)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_normal_independent_(\n                        m.weights,\n                    )\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"ones\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.ones_(m.weights, imag_zero=True)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"zeros\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.zeros_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"kaiming_normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_kaiming_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"kaiming_uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_kaiming_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"xavier_normal\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_xavier_normal_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"xavier_uniform\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_xavier_uniform_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_glorot\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_glorot\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights)\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_xavier\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_xavier\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"xavier\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_kaiming\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_kaiming\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"kaiming\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_standard_he\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_standard_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"trabelsi_independent_he\":\n            for m in self.modules():\n                if isinstance(m, FirstLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, HiddenLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, OutputLayer):\n                    cmplx_init.cplx_trabelsi_independent_(m.weights, kind=\"he\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n        elif initilizer == \"standard\":\n            pass\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Loss\", iteration=i, value=losses[-1]\n        )\n        writer.add_scalar(\"Loss\", losses[-1], i)\n        Logger.current_logger().report_scalar(\n            \"Loss/Acc\", \"Acc\", iteration=i, value=scores[-1]\n        )\n        writer.add_scalar(\"Accuracy\", scores[-1], i)\n\n        for key in model_dict:\n            for key_layer in model_dict[key]:\n                if key_layer in [\"weights\", \"bias\"]:\n                    log_label = str(key) + \"_\" + str(key_layer)\n                    log_label.replace(\" \", \"\")\n                    writer.add_histogram(\n                        log_label + \"_real\", model_dict[key][key_layer].real, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_imag\", model_dict[key][key_layer].imag, i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_mag\", torch.abs(model_dict[key][key_layer]), i\n                    )\n                    writer.add_histogram(\n                        log_label + \"_angle\", torch.angle(model_dict[key][key_layer]), i\n                    )\n\n        # writer.add_histogram(\"distribution centers\", x + n_iter, i)\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n\n    writer.close()\n    return losses, scores\n\n\n\nInit\n\ninitilizers = [\n    \"uniform\",\n    \"normal\",\n    # \"zeros\",\n    # \"ones\",\n    \"kaiming_normal\",\n    \"kaiming_uniform\",\n    \"xavier_normal\",\n    \"xavier_uniform\",\n    \"trabelsi_standard_glorot\",\n    \"trabelsi_independent_glorot\",\n    \"trabelsi_standard_xavier\",\n    \"trabelsi_independent_xavier\",\n    \"trabelsi_standard_kaiming\",\n    \"trabelsi_independent_kaiming\",\n    \"trabelsi_standard_he\",\n    \"trabelsi_independent_he\",\n    \"standard\",\n]\n\n\nfor initilizer in initilizers:\n    model = Model(categories=categories, periodicity=periodicity)\n    model.initialize_weights(initilizer=initilizer)\n    criterion = ComplexMSELoss.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    task = Task.init(\n        project_name=\"mlmvn\",\n        task_name=\"SDD-mlmvn-[48-100-11]\",\n        tags=[\"mlmvn\", \"SDD\", \"initilizer\"],\n    )\n    writer = SummaryWriter()\n\n    #  capture a dictionary of hyperparameters with config\n    config_dict = {\n        \"learning_rate\": lr,\n        \"epochs\": epochs,\n        \"batch_size\": batch_size,\n        \"optim\": \"ECL\",\n        \"categories\": categories,\n        \"periodicity\": periodicity,\n        \"layer\": \"[48-100-11]\",\n        \"initilizer\": initilizer,\n    }\n    task.connect(config_dict)\n\n    x_train, x_valid, y_train, y_valid = get_splitted_data(X, y, neuronCats)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n    Logger.current_logger().report_single_value(\n        name=\"Train Acc.\",\n        value=acc,\n    )\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    print(\"Val Acc.: \", acc)\n    Logger.current_logger().report_single_value(\n        name=\"Val Acc.\",\n        value=acc,\n    )\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n\n    task.mark_completed()\n    task.close()\n\nClearML Task: created new task id=44d8ea5cbf604878b92f402cc8b0eddc\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/44d8ea5cbf604878b92f402cc8b0eddc/output/log\n\n\n/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-20 19:22:00,485 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.12282228621301199\nEpoch 19 loss is 0.09224345408146964\nEpoch 29 loss is 0.10864849515390713\nEpoch 39 loss is 0.0810078859787397\nEpoch 49 loss is 0.0898406030411643\nTrain Acc.:  0.9418023330342263\nVal Acc.:  0.9291634623600786\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97      1074\n           1       0.92      0.91      0.91      1089\n           2       0.98      0.96      0.97      1044\n           3       0.98      0.97      0.98      1048\n           4       0.96      0.92      0.94      1057\n           5       0.93      0.92      0.92      1072\n           6       0.93      0.91      0.92      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.93      0.93      1030\n          10       0.99      0.94      0.96      1012\n\n   micro avg       0.96      0.95      0.96     11703\n   macro avg       0.96      0.95      0.96     11703\nweighted avg       0.96      0.95      0.96     11703\n samples avg       0.94      0.95      0.94     11703\n\nClearML Task: created new task id=2775d88ebe644c9eb35a314339d589e5\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2775d88ebe644c9eb35a314339d589e5/output/log\n\n\n/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.6195961883261095\nEpoch 19 loss is 0.4368921566730075\nEpoch 29 loss is 0.26458930689298554\nEpoch 39 loss is 0.18611469218135965\nEpoch 49 loss is 0.1550662377159612\nTrain Acc.:  0.8358543776438918\nVal Acc.:  0.8256002734341622\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      1074\n           1       0.85      0.76      0.80      1089\n           2       0.93      0.79      0.86      1044\n           3       0.93      0.92      0.92      1048\n           4       0.91      0.84      0.88      1057\n           5       0.83      0.80      0.82      1072\n           6       0.90      0.86      0.88      1066\n           7       0.98      0.98      0.98      1103\n           8       1.00      1.00      1.00      1108\n           9       0.88      0.88      0.88      1030\n          10       0.90      0.80      0.84      1012\n\n   micro avg       0.92      0.87      0.89     11703\n   macro avg       0.92      0.87      0.89     11703\nweighted avg       0.92      0.87      0.89     11703\n samples avg       0.85      0.87      0.86     11703\n\nClearML Task: created new task id=8173403b25a34259a0174431dc02b39f\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/8173403b25a34259a0174431dc02b39f/output/log\n\n\n/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.13417986131187382\nEpoch 19 loss is 0.0787341998435113\nEpoch 29 loss is 0.07534739460774055\nEpoch 39 loss is 0.05243504478153372\nEpoch 49 loss is 0.049628723627474024\nTrain Acc.:  0.9610092723155151\nVal Acc.:  0.9474493719559087\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.95      0.95      0.95      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.97      0.98      1048\n           4       0.95      0.93      0.94      1057\n           5       0.96      0.93      0.95      1072\n           6       0.94      0.92      0.93      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.94      0.95      1030\n          10       0.98      0.95      0.97      1012\n\n   micro avg       0.97      0.96      0.97     11703\n   macro avg       0.97      0.96      0.97     11703\nweighted avg       0.97      0.96      0.97     11703\n samples avg       0.95      0.96      0.96     11703\n\nClearML Task: created new task id=baafad649ee94838be2247186b0ec755\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/baafad649ee94838be2247186b0ec755/output/log\n\n\n/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.1285782571954889\nEpoch 19 loss is 0.09804718683267999\nEpoch 29 loss is 0.07395459259074805\nEpoch 39 loss is 0.060897612847309844\nEpoch 49 loss is 0.07065708804235743\nTrain Acc.:  0.9491945477075588\nVal Acc.:  0.9430060668204734\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1074\n           1       0.94      0.92      0.93      1089\n           2       0.99      0.98      0.99      1044\n           3       0.98      0.96      0.97      1048\n           4       0.95      0.95      0.95      1057\n           5       0.94      0.93      0.93      1072\n           6       0.96      0.92      0.94      1066\n           7       1.00      1.00      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.96      0.93      0.94      1030\n          10       0.99      0.96      0.97      1012\n\n   micro avg       0.97      0.96      0.96     11703\n   macro avg       0.97      0.96      0.96     11703\nweighted avg       0.97      0.96      0.96     11703\n samples avg       0.95      0.96      0.95     11703\n\nClearML Task: created new task id=743c4265b9124dc5aa3806305df1c7cd\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/743c4265b9124dc5aa3806305df1c7cd/output/log\n\n\n/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.12129981536387262\nEpoch 19 loss is 0.07506399494782308\nEpoch 29 loss is 0.06446026524658817\nEpoch 39 loss is 0.06043589024096862\nEpoch 49 loss is 0.0524866021964434\nTrain Acc.:  0.9561594667350339\nVal Acc.:  0.9452277193881911\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1074\n           1       0.96      0.91      0.93      1089\n           2       0.99      0.98      0.98      1044\n           3       0.99      0.97      0.98      1048\n           4       0.97      0.95      0.96      1057\n           5       0.95      0.93      0.94      1072\n           6       0.96      0.94      0.95      1066\n           7       1.00      0.99      1.00      1103\n           8       1.00      1.00      1.00      1108\n           9       0.94      0.95      0.95      1030\n          10       0.99      0.95      0.97      1012\n\n   micro avg       0.98      0.96      0.97     11703\n   macro avg       0.98      0.96      0.97     11703\nweighted avg       0.98      0.96      0.97     11703\n samples avg       0.95      0.96      0.95     11703\n\nClearML Task: created new task id=b744dddaa16c4529b914b6b3de300c96\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/b744dddaa16c4529b914b6b3de300c96/output/log\n\n\n/tmp/ipykernel_20526/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.1182056404424841\nEpoch 19 loss is 0.07147076034313422\nEpoch 29 loss is 0.05620083264376806\nEpoch 39 loss is 0.05679742307294903"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html",
    "href": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html",
    "title": "Sensorless Drive Diagnosis",
    "section": "",
    "text": "train_csv = pd.read_csv(\n    \"data/autass_data2.csv\",\n    header=None,\n    dtype=np.double,\n)\ndata = np.array(train_csv.values[:, 1:50])\ndel train_csv\n\n\nX = data[:, 0:48]\ny = data[:, 48].astype(int) - 1\n\nyt = copy.copy(y)\nyt[yt == 0] = 20\nyt[yt == 1] = 21\nyt[yt == 2] = 22\nyt[yt == 3] = 23\nyt[yt == 4] = 26\nyt[yt == 5] = 24\nyt[yt == 6] = 27\nyt[yt == 7] = 29\nyt[yt == 8] = 30\nyt[yt == 9] = 25\nyt[yt == 10] = 28\nyt -= 20\ny = yt\ndel yt"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html#config",
    "href": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html#config",
    "title": "Sensorless Drive Diagnosis",
    "section": "Config",
    "text": "Config\n\nepochs = 200\nbatch_size = 538\nlr = 1\nclip_angle_value = 1000000"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html#single-layer",
    "href": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html#single-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Single Layer",
    "text": "Single Layer\n\nMLMVN [48-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=aed2c1e548d44d02876e693409da4ac3\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/aed2c1e548d44d02876e693409da4ac3/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 22:37:31,629 - clearml.frameworks - INFO - Found existing registered model id=caa96da5a415490ca1ea0f95b383f403 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-11.pt] reusing it.\nEpoch 9 loss is 0.24486756715526192\nEpoch 19 loss is 0.16472245788019146\nEpoch 29 loss is 0.16197286684855702\nEpoch 39 loss is 0.15777335760350167\nEpoch 49 loss is 0.16362277181734114\nEpoch 59 loss is 0.1599623075779962\nEpoch 69 loss is 0.15505627729450555\nEpoch 79 loss is 0.1516815979779308\nEpoch 89 loss is 0.14756507300426192\nEpoch 99 loss is 0.14990614337584326\nEpoch 109 loss is 0.14464806634677907\nEpoch 119 loss is 0.14414441882839768\nEpoch 129 loss is 0.133414154667594\nEpoch 139 loss is 0.1568121289545272\nEpoch 149 loss is 0.13148384624669937\nEpoch 159 loss is 0.15520714324892884\nEpoch 169 loss is 0.15488150667967812\nEpoch 179 loss is 0.16196303491616978\nEpoch 189 loss is 0.17048013425110728\nEpoch 199 loss is 0.253481823204663\nTrain Acc.:  0.9061037878949729\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95      1063\n           1       0.88      0.91      0.90      1064\n           2       0.97      0.95      0.96      1064\n           3       0.92      0.93      0.93      1064\n           4       0.90      0.87      0.88      1064\n           5       0.92      0.89      0.90      1063\n           6       0.88      0.85      0.86      1064\n           7       0.99      0.97      0.98      1064\n           8       1.00      0.99      1.00      1064\n           9       0.92      0.90      0.91      1064\n          10       0.95      0.97      0.96      1064\n\n   micro avg       0.93      0.93      0.93     11702\n   macro avg       0.93      0.93      0.93     11702\nweighted avg       0.93      0.93      0.93     11702\n samples avg       0.91      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23206087884330276\nEpoch 19 loss is 0.223541648173787\nEpoch 29 loss is 0.19793911944454762\nEpoch 39 loss is 0.20525953919872755\nEpoch 49 loss is 0.19172214725700548\nEpoch 59 loss is 0.180403134615211\nEpoch 69 loss is 0.20454411746859222\nEpoch 79 loss is 0.22207294107387418\nEpoch 89 loss is 0.1823581614612617\nEpoch 99 loss is 0.1984208152000532\nEpoch 109 loss is 0.21770198580729927\nEpoch 119 loss is 0.26321624687915784\nEpoch 129 loss is 0.2366645070320064\nEpoch 139 loss is 0.26308797516611215\nEpoch 149 loss is 0.2796785803565688\nEpoch 159 loss is 0.19642007623052968\nEpoch 169 loss is 0.3310147275301174\nEpoch 179 loss is 0.2874490546335206\nEpoch 189 loss is 0.2595535203540301\nEpoch 199 loss is 0.3217917178123802\nTrain Acc.:  0.8804452325506869\n              precision    recall  f1-score   support\n\n           0       0.97      0.91      0.94      1063\n           1       0.89      0.87      0.88      1064\n           2       0.98      0.96      0.97      1064\n           3       0.98      0.95      0.97      1063\n           4       0.84      0.71      0.77      1064\n           5       0.92      0.86      0.89      1064\n           6       0.84      0.83      0.83      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.87      0.90      0.88      1064\n          10       0.92      0.93      0.93      1064\n\n   micro avg       0.93      0.90      0.91     11702\n   macro avg       0.93      0.90      0.91     11702\nweighted avg       0.93      0.90      0.91     11702\n samples avg       0.89      0.90      0.89     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.23712313444477043\nEpoch 19 loss is 0.21309815317807554\nEpoch 29 loss is 0.2151226041409042\nEpoch 39 loss is 0.22794682641424854\nEpoch 49 loss is 0.18590490342052546\nEpoch 59 loss is 0.17194472537314034\nEpoch 69 loss is 0.18656052422193878\nEpoch 79 loss is 0.1756717054349332\nEpoch 89 loss is 0.19215676606052032\nEpoch 99 loss is 0.19421889588535315\nEpoch 109 loss is 0.23849424654511947\nEpoch 119 loss is 0.21172349809503013\nEpoch 129 loss is 0.310951138936457\nEpoch 139 loss is 0.40137440459088775\nEpoch 149 loss is 0.3257708635272683\nEpoch 159 loss is 0.3058673858224232\nEpoch 169 loss is 0.3393692446068119\nEpoch 179 loss is 0.3696819123265627\nEpoch 189 loss is 0.3599098499384412\nEpoch 199 loss is 0.4304120007418079\nTrain Acc.:  0.8721558741213921\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95      1064\n           1       0.84      0.81      0.82      1064\n           2       0.96      0.91      0.93      1064\n           3       0.96      0.92      0.94      1063\n           4       0.89      0.84      0.86      1064\n           5       0.87      0.84      0.86      1064\n           6       0.88      0.76      0.81      1063\n           7       1.00      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.86      0.86      0.86      1064\n          10       0.92      0.91      0.91      1064\n\n   micro avg       0.92      0.89      0.90     11702\n   macro avg       0.92      0.89      0.90     11702\nweighted avg       0.92      0.89      0.90     11702\n samples avg       0.88      0.89      0.88     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.22525142483966729\nEpoch 19 loss is 0.21769357020501426\nEpoch 29 loss is 0.2577133298481112\nEpoch 39 loss is 0.22213481954372305\nEpoch 49 loss is 0.23841117493258804\nEpoch 59 loss is 0.309014874649911\nEpoch 69 loss is 0.35371038376681546\nEpoch 79 loss is 0.3699092508438563\nEpoch 89 loss is 0.36114378942190123\nEpoch 99 loss is 0.3760639842052328\nEpoch 109 loss is 0.289880557557834\nEpoch 119 loss is 0.3542478901455959\nEpoch 129 loss is 0.2415444508855365\nEpoch 139 loss is 0.25469037973175684\nEpoch 149 loss is 0.23138965174619833\nEpoch 159 loss is 0.21245593786661174\nEpoch 169 loss is 0.28983951582336215\nEpoch 179 loss is 0.2624560468178941\nEpoch 189 loss is 0.22517652315304382\nEpoch 199 loss is 0.30955816883738974\nTrain Acc.:  0.830303159783793\n              precision    recall  f1-score   support\n\n           0       0.95      0.94      0.95      1063\n           1       0.89      0.81      0.85      1064\n           2       0.95      0.85      0.90      1064\n           3       0.96      0.94      0.95      1063\n           4       0.89      0.81      0.85      1064\n           5       0.88      0.81      0.85      1064\n           6       0.77      0.70      0.73      1064\n           7       1.00      0.99      1.00      1064\n           8       0.99      0.99      0.99      1064\n           9       0.86      0.89      0.87      1064\n          10       0.91      0.84      0.87      1064\n\n   micro avg       0.92      0.87      0.89     11702\n   macro avg       0.91      0.87      0.89     11702\nweighted avg       0.91      0.87      0.89     11702\n samples avg       0.85      0.87      0.86     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2563851438763014\nEpoch 19 loss is 0.21581594458135714\nEpoch 29 loss is 0.2354525347648366\nEpoch 39 loss is 0.19813069646116785\nEpoch 49 loss is 0.23640784807386372\nEpoch 59 loss is 0.2855586216205039\nEpoch 69 loss is 0.31419042978716966\nEpoch 79 loss is 0.3396274724821675\nEpoch 89 loss is 0.3805541273460192\nEpoch 99 loss is 0.2594859077291225\nEpoch 109 loss is 0.27091326668934435\nEpoch 119 loss is 0.26640856164548926\nEpoch 129 loss is 0.2599984885766787\nEpoch 139 loss is 0.24273455139343672\nEpoch 149 loss is 0.3339575152030492\nEpoch 159 loss is 0.2597825247475023\nEpoch 169 loss is 0.2548200790331675\nEpoch 179 loss is 0.2912208067110613\nEpoch 189 loss is 0.29534861061989254\nEpoch 199 loss is 0.35203137025199627\nTrain Acc.:  0.8437840493943214\n              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.94      1064\n           1       0.88      0.80      0.84      1064\n           2       0.92      0.85      0.88      1064\n           3       0.93      0.81      0.87      1064\n           4       0.89      0.81      0.85      1064\n           5       0.85      0.81      0.83      1064\n           6       0.84      0.81      0.83      1063\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1063\n           9       0.86      0.86      0.86      1064\n          10       0.94      0.85      0.89      1064\n\n   micro avg       0.91      0.86      0.89     11702\n   macro avg       0.91      0.86      0.89     11702\nweighted avg       0.91      0.86      0.89     11702\n samples avg       0.85      0.86      0.86     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=7e81250b8f8a4d31b43661579158b428\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/7e81250b8f8a4d31b43661579158b428/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 22:55:28,643 - clearml.frameworks - INFO - Found existing registered model id=c337b94a22444d809d449783726d8ee2 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-11.pt] reusing it.\nEpoch 9 loss is 0.2058236759317573\nEpoch 19 loss is 0.1825360529148008\nEpoch 29 loss is 0.13331677743733528\nEpoch 39 loss is 0.11695840664945023\nEpoch 49 loss is 0.11103621118112407\nEpoch 59 loss is 0.12555862590235578\nEpoch 69 loss is 0.12181158857175331\nEpoch 79 loss is 0.09544202089592545\nEpoch 89 loss is 0.11178487401744581\nEpoch 99 loss is 0.12958967446816022\nEpoch 109 loss is 0.11905161904742857\nEpoch 119 loss is 0.10873579923307998\nEpoch 129 loss is 0.09673697400248864\nEpoch 139 loss is 0.09588223402448456\nEpoch 149 loss is 0.11162506774666282\nEpoch 159 loss is 0.09059592728792225\nEpoch 169 loss is 0.09758258176950994\nEpoch 179 loss is 0.10536165971500439\nEpoch 189 loss is 0.10205764671734051\nEpoch 199 loss is 0.14389611907866479\nTrain Acc.:  0.9354583716110838\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      1063\n           1       0.93      0.93      0.93      1064\n           2       0.98      0.95      0.96      1064\n           3       0.96      0.95      0.96      1064\n           4       0.94      0.93      0.94      1064\n           5       0.94      0.94      0.94      1063\n           6       0.88      0.88      0.88      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.92      0.93      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.94      0.94      0.94     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2307281778017605\nEpoch 19 loss is 0.18615664526599873\nEpoch 29 loss is 0.16207854951403944\nEpoch 39 loss is 0.15162521963532127\nEpoch 49 loss is 0.16081494726357065\nEpoch 59 loss is 0.1257449738051931\nEpoch 69 loss is 0.10937442608829037\nEpoch 79 loss is 0.12329823825151687\nEpoch 89 loss is 0.12930987857639764\nEpoch 99 loss is 0.12765360621548777\nEpoch 109 loss is 0.15596338287786382\nEpoch 119 loss is 0.16634696725086548\nEpoch 129 loss is 0.1495299076006498\nEpoch 139 loss is 0.1502898510399668\nEpoch 149 loss is 0.15832158268949942\nEpoch 159 loss is 0.17899469216655411\nEpoch 169 loss is 0.23058719525799198\nEpoch 179 loss is 0.2102188014453179\nEpoch 189 loss is 0.23088950030054886\nEpoch 199 loss is 0.21150270029799098\nTrain Acc.:  0.9071933685132566\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1063\n           1       0.91      0.89      0.90      1064\n           2       0.98      0.97      0.97      1064\n           3       0.94      0.94      0.94      1063\n           4       0.91      0.92      0.92      1064\n           5       0.93      0.93      0.93      1064\n           6       0.90      0.89      0.90      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.92      0.88      0.90      1064\n          10       0.92      0.89      0.90      1064\n\n   micro avg       0.94      0.93      0.94     11702\n   macro avg       0.94      0.93      0.94     11702\nweighted avg       0.94      0.93      0.94     11702\n samples avg       0.92      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.21257633823646344\nEpoch 19 loss is 0.17318814963579834\nEpoch 29 loss is 0.1458617958714956\nEpoch 39 loss is 0.14125668374187184\nEpoch 49 loss is 0.128772752498416\nEpoch 59 loss is 0.1352171863065471\nEpoch 69 loss is 0.10681416437897072\nEpoch 79 loss is 0.13216534508015265\nEpoch 89 loss is 0.11111060725019\nEpoch 99 loss is 0.11440525031960108\nEpoch 109 loss is 0.1094888755315214\nEpoch 119 loss is 0.11533819118199232\nEpoch 129 loss is 0.10736030126824322\nEpoch 139 loss is 0.10061210309481297\nEpoch 149 loss is 0.10068345790438905\nEpoch 159 loss is 0.09078661822690025\nEpoch 169 loss is 0.09375741469930386\nEpoch 179 loss is 0.09420665138442623\nEpoch 189 loss is 0.09327934878515685\nEpoch 199 loss is 0.09550257254442705\nTrain Acc.:  0.9378939047578354\n              precision    recall  f1-score   support\n\n           0       0.97      0.94      0.96      1064\n           1       0.94      0.91      0.92      1064\n           2       0.97      0.96      0.97      1064\n           3       0.99      0.97      0.98      1063\n           4       0.94      0.92      0.93      1064\n           5       0.94      0.93      0.93      1064\n           6       0.91      0.86      0.88      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.92      0.91      0.92      1064\n          10       0.96      0.95      0.96      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.94     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20641349230841313\nEpoch 19 loss is 0.15738830801178577\nEpoch 29 loss is 0.1799417678605428\nEpoch 39 loss is 0.13007205544481562\nEpoch 49 loss is 0.11737783309073208\nEpoch 59 loss is 0.1114466828734566\nEpoch 69 loss is 0.11852908050617135\nEpoch 79 loss is 0.11542474814310127\nEpoch 89 loss is 0.10236658536166438\nEpoch 99 loss is 0.12045357416608558\nEpoch 109 loss is 0.1051365830820148\nEpoch 119 loss is 0.09761249375093389\nEpoch 129 loss is 0.09216019224495184\nEpoch 139 loss is 0.09421224180251014\nEpoch 149 loss is 0.09233837313904565\nEpoch 159 loss is 0.09223922889858933\nEpoch 169 loss is 0.08829034240388713\nEpoch 179 loss is 0.09375694157319493\nEpoch 189 loss is 0.08536116014844801\nEpoch 199 loss is 0.08500084532665717\nTrain Acc.:  0.94060717414062\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1063\n           1       0.93      0.93      0.93      1064\n           2       0.98      0.96      0.97      1064\n           3       0.97      0.95      0.96      1063\n           4       0.96      0.92      0.94      1064\n           5       0.95      0.92      0.93      1064\n           6       0.90      0.90      0.90      1064\n           7       0.99      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.92      0.93      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.96      0.95      0.95     11702\n   macro avg       0.96      0.95      0.95     11702\nweighted avg       0.96      0.95      0.95     11702\n samples avg       0.94      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20548555472102373\nEpoch 19 loss is 0.15323632791255185\nEpoch 29 loss is 0.1295082539268068\nEpoch 39 loss is 0.12081646040093838\nEpoch 49 loss is 0.10243967732402894\nEpoch 59 loss is 0.10126855908153241\nEpoch 69 loss is 0.11296332021354512\nEpoch 79 loss is 0.09185533808063474\nEpoch 89 loss is 0.09732833351838127\nEpoch 99 loss is 0.09188653614455912\nEpoch 109 loss is 0.10085898797344649\nEpoch 119 loss is 0.10443676031698373\nEpoch 129 loss is 0.1023311360663922\nEpoch 139 loss is 0.10306230769293823\nEpoch 149 loss is 0.11354442989486692\nEpoch 159 loss is 0.10403630260181586\nEpoch 169 loss is 0.09833004415491228\nEpoch 179 loss is 0.14203489341950598\nEpoch 189 loss is 0.10320350983155654\nEpoch 199 loss is 0.11371829480246762\nTrain Acc.:  0.9411626466126861\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96      1064\n           1       0.93      0.92      0.93      1064\n           2       0.98      0.94      0.96      1064\n           3       0.98      0.96      0.97      1064\n           4       0.95      0.91      0.93      1064\n           5       0.94      0.93      0.93      1064\n           6       0.90      0.90      0.90      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.94      0.91      0.93      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.94      0.94      0.94     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=2d2bd92d33c3458882439af599c0d219\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2d2bd92d33c3458882439af599c0d219/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-27 23:20:32,297 - clearml.frameworks - INFO - Found existing registered model id=bb96e63090904339bf87c4852d30bdb6 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-11.pt] reusing it.\nEpoch 9 loss is 0.24396016163019188\nEpoch 19 loss is 0.1408100184642106\nEpoch 29 loss is 0.10649778199752649\nEpoch 39 loss is 0.0854672719392131\nEpoch 49 loss is 0.09677528437761192\nEpoch 59 loss is 0.08491636262762711\nEpoch 69 loss is 0.07073853121933514\nEpoch 79 loss is 0.0623588742935536\nEpoch 89 loss is 0.06493405545145113\nEpoch 99 loss is 0.06570635803032455\nEpoch 109 loss is 0.05703714881850874\nEpoch 119 loss is 0.06868247092465077\nEpoch 129 loss is 0.05820049851283152\nEpoch 139 loss is 0.057654175689656244\nEpoch 149 loss is 0.054585099955762065\nEpoch 159 loss is 0.07306441308956363\nEpoch 169 loss is 0.06311269411213584\nEpoch 179 loss is 0.0785263104691008\nEpoch 189 loss is 0.08227202587346817\nEpoch 199 loss is 0.055809321412454364\nTrain Acc.:  0.9670775738671566\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.98      0.98      1064\n           4       0.96      0.92      0.94      1064\n           5       0.97      0.95      0.96      1063\n           6       0.95      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2661679163999582\nEpoch 19 loss is 0.14625262471743808\nEpoch 29 loss is 0.12002121900326053\nEpoch 39 loss is 0.11372154790877459\nEpoch 49 loss is 0.09963077571846711\nEpoch 59 loss is 0.08699985949055491\nEpoch 69 loss is 0.08908419619519271\nEpoch 79 loss is 0.077337234526038\nEpoch 89 loss is 0.0786930436278074\nEpoch 99 loss is 0.06691134614071381\nEpoch 109 loss is 0.07431928775441791\nEpoch 119 loss is 0.059303313881029024\nEpoch 129 loss is 0.05999768811650241\nEpoch 139 loss is 0.05597988277724091\nEpoch 149 loss is 0.05467993967785231\nEpoch 159 loss is 0.054380003065822906\nEpoch 169 loss is 0.055407265453249616\nEpoch 179 loss is 0.05802883371582689\nEpoch 189 loss is 0.05513524843130078\nEpoch 199 loss is 0.04906545557802202\nTrain Acc.:  0.9664793727433931\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1063\n           1       0.96      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.98      0.98      1063\n           4       0.94      0.94      0.94      1064\n           5       0.97      0.96      0.97      1064\n           6       0.97      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20502371384654208\nEpoch 19 loss is 0.1600604545972537\nEpoch 29 loss is 0.17638293655801232\nEpoch 39 loss is 0.11561468644927743\nEpoch 49 loss is 0.09013697049000392\nEpoch 59 loss is 0.08748177132875291\nEpoch 69 loss is 0.07197819254783605\nEpoch 79 loss is 0.0702369190086657\nEpoch 89 loss is 0.07234675932731245\nEpoch 99 loss is 0.068253828507892\nEpoch 109 loss is 0.07305692741178821\nEpoch 119 loss is 0.07672600907945662\nEpoch 129 loss is 0.07097353389268429\nEpoch 139 loss is 0.06347196049334779\nEpoch 149 loss is 0.0644738520961493\nEpoch 159 loss is 0.05376314275695037\nEpoch 169 loss is 0.05644687125414726\nEpoch 179 loss is 0.048773048522865105\nEpoch 189 loss is 0.04476576239785017\nEpoch 199 loss is 0.04786218107136711\nTrain Acc.:  0.9728032131946076\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1064\n           1       0.96      0.96      0.96      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.97      0.98      1063\n           4       0.97      0.95      0.96      1064\n           5       0.96      0.96      0.96      1064\n           6       0.96      0.94      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.95      0.95      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.22543501013461756\nEpoch 19 loss is 0.17068695324254124\nEpoch 29 loss is 0.12409391685149763\nEpoch 39 loss is 0.11697496548175042\nEpoch 49 loss is 0.08876241864971807\nEpoch 59 loss is 0.08153087295328582\nEpoch 69 loss is 0.07483510813769084\nEpoch 79 loss is 0.10710111668977772\nEpoch 89 loss is 0.08042401739956408\nEpoch 99 loss is 0.07369310521635002\nEpoch 109 loss is 0.07451935209149846\nEpoch 119 loss is 0.059885478707408256\nEpoch 129 loss is 0.0567862421178645\nEpoch 139 loss is 0.05093684395615367\nEpoch 149 loss is 0.06306265519934738\nEpoch 159 loss is 0.05432974760807225\nEpoch 169 loss is 0.04892963285147231\nEpoch 179 loss is 0.04994840531613505\nEpoch 189 loss is 0.051194054698050454\nEpoch 199 loss is 0.044596025063632284\nTrain Acc.:  0.9714572606661397\n              precision    recall  f1-score   support\n\n           0       0.99      0.98      0.98      1063\n           1       0.96      0.96      0.96      1064\n           2       0.99      0.99      0.99      1064\n           3       0.98      0.99      0.98      1063\n           4       0.96      0.96      0.96      1064\n           5       0.96      0.95      0.95      1064\n           6       0.95      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.94      0.95      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2782503201838837\nEpoch 19 loss is 0.16120408621727206\nEpoch 29 loss is 0.12418159289722493\nEpoch 39 loss is 0.09063661342210186\nEpoch 49 loss is 0.08274663620794244\nEpoch 59 loss is 0.07859951940472655\nEpoch 69 loss is 0.07256532240445507\nEpoch 79 loss is 0.06916262503726474\nEpoch 89 loss is 0.0630058410204737\nEpoch 99 loss is 0.07026817485377791\nEpoch 109 loss is 0.05949501562367503\nEpoch 119 loss is 0.053246533281885174\nEpoch 129 loss is 0.05572973071008147\nEpoch 139 loss is 0.05570382762746428\nEpoch 149 loss is 0.08662761110902355\nEpoch 159 loss is 0.057920531802435\nEpoch 169 loss is 0.06671310002777765\nEpoch 179 loss is 0.06361666252378624\nEpoch 189 loss is 0.0634835982068112\nEpoch 199 loss is 0.05273022961513594\nTrain Acc.:  0.9670989381930053\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1064\n           1       0.95      0.94      0.95      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.98      0.98      1064\n           4       0.96      0.96      0.96      1064\n           5       0.95      0.95      0.95      1064\n           6       0.94      0.95      0.94      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.98      0.94      0.96      1064\n          10       0.98      0.96      0.97      1064\n\n   micro avg       0.97      0.97      0.97     11702\n   macro avg       0.97      0.97      0.97     11702\nweighted avg       0.97      0.97      0.97     11702\n samples avg       0.96      0.97      0.96     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act2 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.linear_out(x)\n        x = self.phase_act2(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=f62adc3f54574a5db40bd2a8cbd65cc0\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/f62adc3f54574a5db40bd2a8cbd65cc0/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(\n    n_splits=5,\n    test_size=0.2,\n    random_state=42,\n)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_2379/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-29 12:16:12,604 - clearml.frameworks - INFO - Found existing registered model id=0f73e6db01fc42988672e4f44c0add5f [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-11.pt] reusing it.\nEpoch 9 loss is 0.2013734097530227\nEpoch 19 loss is 0.13194389245179217\nEpoch 29 loss is 0.10095062501209577\nEpoch 39 loss is 0.09074072104898326\nEpoch 49 loss is 0.08300947225827868\nEpoch 59 loss is 0.0738162527877322\nEpoch 69 loss is 0.07072649592151344\nEpoch 79 loss is 0.07138726478663758\nEpoch 89 loss is 0.06214147198340652\nEpoch 99 loss is 0.07020503698271563\nEpoch 109 loss is 0.074644080701454\nEpoch 119 loss is 0.06734553316012935\nEpoch 129 loss is 0.059474967215059765\nEpoch 139 loss is 0.07335145914140831\nEpoch 149 loss is 0.062376795077677044\nEpoch 159 loss is 0.0526909906751619\nEpoch 169 loss is 0.04941692807072795\nEpoch 179 loss is 0.04210521010111257\nEpoch 189 loss is 0.0447379761754187\nEpoch 199 loss is 0.03882946678160469\nTrain Acc.:  0.97899886769073\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.95      0.94      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       0.99      0.99      0.99      1064\n           4       0.97      0.96      0.97      1064\n           5       0.95      0.95      0.95      1063\n           6       0.96      0.95      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.94      0.96      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\nClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n\n\n/tmp/ipykernel_2379/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.21319071325891403\nEpoch 19 loss is 0.1541410855291198\nEpoch 29 loss is 0.12536923443070858\nEpoch 39 loss is 0.09970140587708567\nEpoch 49 loss is 0.08215366375661032\nEpoch 59 loss is 0.07893182432329253\nEpoch 69 loss is 0.07569625143171067\nEpoch 79 loss is 0.07090777313842668\nEpoch 89 loss is 0.06514219463825727\nEpoch 99 loss is 0.06887248604052763\nEpoch 109 loss is 0.057908520270728236\nEpoch 119 loss is 0.057421434582441984\nEpoch 129 loss is 0.04913560375422588\nEpoch 139 loss is 0.04935810215299344\nEpoch 149 loss is 0.04237250334663373\nEpoch 159 loss is 0.040383543448331234\nEpoch 169 loss is 0.036213423181436236\nEpoch 179 loss is 0.03839429011389191\nEpoch 189 loss is 0.03846825102366323\nEpoch 199 loss is 0.037638064394336875\nTrain Acc.:  0.9823317025231268\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1063\n           1       0.96      0.96      0.96      1064\n           2       1.00      0.99      1.00      1064\n           3       0.98      0.99      0.99      1063\n           4       0.96      0.97      0.96      1064\n           5       0.97      0.96      0.96      1064\n           6       0.96      0.95      0.96      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_2379/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.20835717627283434\nEpoch 19 loss is 0.1410003145381981\nEpoch 29 loss is 0.10362160718673499\nEpoch 39 loss is 0.09535519170107241\nEpoch 49 loss is 0.08623970792143622\nEpoch 59 loss is 0.06967393203823516\nEpoch 69 loss is 0.07223157768403805\nEpoch 79 loss is 0.06292363524435325\nEpoch 89 loss is 0.06516889101094622\nEpoch 99 loss is 0.05962646936523845\nEpoch 109 loss is 0.052085959520220305\nEpoch 119 loss is 0.05127448522332268\nEpoch 129 loss is 0.04993565878003458\nEpoch 139 loss is 0.04217621548518707\nEpoch 149 loss is 0.04918457656177097\nEpoch 159 loss is 0.03850822431866483\nEpoch 169 loss is 0.038279677015932684\nEpoch 179 loss is 0.039339300558995774\nEpoch 189 loss is 0.03874658731757177\nEpoch 199 loss is 0.036737692281627654\nTrain Acc.:  0.9827589890401008\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99      1064\n           1       0.95      0.95      0.95      1064\n           2       0.99      0.99      0.99      1064\n           3       1.00      0.98      0.99      1063\n           4       0.97      0.96      0.97      1064\n           5       0.97      0.95      0.96      1064\n           6       0.96      0.94      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.97      0.97      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.98     11702\n   macro avg       0.98      0.97      0.98     11702\nweighted avg       0.98      0.97      0.98     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_2379/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2047739384042335\nEpoch 19 loss is 0.1240691449665739\nEpoch 29 loss is 0.1250756910889213\nEpoch 39 loss is 0.09980943240036731\nEpoch 49 loss is 0.07931262188990955\nEpoch 59 loss is 0.06880595699322983\nEpoch 69 loss is 0.07295263256108461\nEpoch 79 loss is 0.0657251223690606\nEpoch 89 loss is 0.06288437799940251\nEpoch 99 loss is 0.06078154801969977\nEpoch 109 loss is 0.05630173979958109\nEpoch 119 loss is 0.05059079914071474\nEpoch 129 loss is 0.0519624732081652\nEpoch 139 loss is 0.04989906966215809\nEpoch 149 loss is 0.046105722466105954\nEpoch 159 loss is 0.04479487999819714\nEpoch 169 loss is 0.05169914293629347\nEpoch 179 loss is 0.05468717561123138\nEpoch 189 loss is 0.04472661993607276\nEpoch 199 loss is 0.04167204375351288\nTrain Acc.:  0.9793193325784605\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      1063\n           1       0.93      0.96      0.94      1064\n           2       1.00      0.99      1.00      1064\n           3       0.99      0.98      0.99      1063\n           4       0.97      0.97      0.97      1064\n           5       0.97      0.94      0.95      1064\n           6       0.96      0.94      0.95      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.97      0.96      0.97      1064\n          10       0.98      0.97      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.97      0.97      0.97     11702\n\n\n\n/tmp/ipykernel_2379/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.24100019695211888\nEpoch 19 loss is 0.15365636763706683\nEpoch 29 loss is 0.11363793386653538\nEpoch 39 loss is 0.09423439262203624\nEpoch 49 loss is 0.08824477344066799\nEpoch 59 loss is 0.07361807474799302\nEpoch 69 loss is 0.07530689777501173\nEpoch 79 loss is 0.06350203232630086\nEpoch 89 loss is 0.061400948333146264\nEpoch 99 loss is 0.05771039868957064\nEpoch 109 loss is 0.057609481716596814\nEpoch 119 loss is 0.05653083207399525\nEpoch 129 loss is 0.05154005805827653\nEpoch 139 loss is 0.05072576182283894\nEpoch 149 loss is 0.046395221236291556\nEpoch 159 loss is 0.04620623101501136\nEpoch 169 loss is 0.042652961928911375\nEpoch 179 loss is 0.0574112161346133\nEpoch 189 loss is 0.044568967625431236\nEpoch 199 loss is 0.03852016601626823\nTrain Acc.:  0.9786570384771508\n              precision    recall  f1-score   support\n\n           0       0.98      0.98      0.98      1064\n           1       0.95      0.96      0.95      1064\n           2       0.99      0.98      0.99      1064\n           3       0.99      0.98      0.99      1064\n           4       0.98      0.95      0.96      1064\n           5       0.96      0.94      0.95      1064\n           6       0.94      0.95      0.95      1063\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.96      0.95      0.96      1064\n          10       0.99      0.98      0.98      1064\n\n   micro avg       0.98      0.97      0.97     11702\n   macro avg       0.98      0.97      0.97     11702\nweighted avg       0.98      0.97      0.97     11702\n samples avg       0.96      0.97      0.97     11702\n\n\n\n\nplot_loss_acc_list(\n    \"$\\mathbb{C}^{*}$: [48-100-11] \",\n    list_losses,\n    list_scores,\n    \"mlmvn-mod-48-100-11.png\",\n)\n\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html#multi-layer",
    "href": "examples/autass/autass_multiple_run_angle_clip_pa_loss.html#multi-layer",
    "title": "Sensorless Drive Diagnosis",
    "section": "Multi Layer",
    "text": "Multi Layer\n\nMLMVN [48-10-10-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-10-10-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 10)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(10, 10)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(10, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-10-10-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-10-10-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=981abbac21de4753879436acdf36c17b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/981abbac21de4753879436acdf36c17b/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-10-10-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-28 00:41:30,778 - clearml.frameworks - INFO - Found existing registered model id=410edb2915b24269b7d34f2e38593dff [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-10-10-11.pt] reusing it.\nEpoch 9 loss is 0.541454749663616\nEpoch 19 loss is 0.8791410713849316\nEpoch 29 loss is 0.8839316416940527\nEpoch 39 loss is 0.8966399825316692\nEpoch 49 loss is 0.9264105355661884\nEpoch 59 loss is 0.951371120459427\nEpoch 69 loss is 0.9412028121256291\nEpoch 79 loss is 0.9732946912045937\nEpoch 89 loss is 0.9181100154489218\nEpoch 99 loss is 0.9329806316494657\nEpoch 109 loss is 0.9328285713006949\nEpoch 119 loss is 0.9419897937541761\nEpoch 129 loss is 0.9427600691188952\nEpoch 139 loss is 0.9091315900275359\nEpoch 149 loss is 0.9324776865430842\nEpoch 159 loss is 0.9134668719369948\nEpoch 169 loss is 0.9403598662976411\nEpoch 179 loss is 0.9422566273044115\nEpoch 189 loss is 0.9543148096619407\nEpoch 199 loss is 0.9549645974775849\nTrain Acc.:  0.5539342406050377\n              precision    recall  f1-score   support\n\n           0       0.77      0.60      0.68      1063\n           1       0.65      0.47      0.55      1064\n           2       0.94      0.67      0.78      1064\n           3       0.89      0.62      0.73      1064\n           4       0.32      0.16      0.21      1064\n           5       0.73      0.69      0.71      1063\n           6       0.82      0.44      0.57      1064\n           7       0.91      0.69      0.78      1064\n           8       0.98      0.95      0.97      1064\n           9       0.68      0.90      0.77      1064\n          10       0.76      0.69      0.72      1064\n\n   micro avg       0.78      0.63      0.69     11702\n   macro avg       0.77      0.63      0.68     11702\nweighted avg       0.77      0.63      0.68     11702\n samples avg       0.59      0.63      0.60     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.34781102538165637\nEpoch 19 loss is 0.4926261439071808\nEpoch 29 loss is 0.6328775813367924\nEpoch 39 loss is 0.7170839574504813\nEpoch 49 loss is 0.6934163739177756\nEpoch 59 loss is 0.7141162567964217\nEpoch 69 loss is 0.7691114368900678\nEpoch 79 loss is 0.7562372003321447\nEpoch 89 loss is 0.7766109368260417\nEpoch 99 loss is 0.7075230256227628\nEpoch 109 loss is 0.7393482060014152\nEpoch 119 loss is 0.7458410915657656\nEpoch 129 loss is 0.7585006377982786\nEpoch 139 loss is 0.7711069196763435\nEpoch 149 loss is 0.7341288291259812\nEpoch 159 loss is 0.7801133649364619\nEpoch 169 loss is 0.7263681341448134\nEpoch 179 loss is 0.7519011054649812\nEpoch 189 loss is 0.7048160484621695\nEpoch 199 loss is 0.7475459901191056\nTrain Acc.:  0.7177986198645502\n              precision    recall  f1-score   support\n\n           0       0.89      0.83      0.86      1063\n           1       0.83      0.74      0.78      1064\n           2       0.94      0.84      0.89      1064\n           3       0.82      0.79      0.81      1063\n           4       0.68      0.66      0.67      1064\n           5       0.83      0.83      0.83      1064\n           6       0.70      0.60      0.64      1064\n           7       0.99      0.94      0.96      1064\n           8       0.99      0.97      0.98      1064\n           9       0.83      0.75      0.79      1064\n          10       0.76      0.75      0.76      1064\n\n   micro avg       0.84      0.79      0.82     11702\n   macro avg       0.84      0.79      0.81     11702\nweighted avg       0.84      0.79      0.81     11702\n samples avg       0.75      0.79      0.77     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.608739696487545\nEpoch 19 loss is 0.7696193880760408\nEpoch 29 loss is 0.803278156062957\nEpoch 39 loss is 0.8387883156242925\nEpoch 49 loss is 0.7807021280520069\nEpoch 59 loss is 0.8088241619223006\nEpoch 69 loss is 0.8070463633451938\nEpoch 79 loss is 0.7945587323244073\nEpoch 89 loss is 0.8392897794392442\nEpoch 99 loss is 0.8214029347151293\nEpoch 109 loss is 0.8340895850326423\nEpoch 119 loss is 0.8219225005001145\nEpoch 129 loss is 0.8507321513973675\nEpoch 139 loss is 0.8635722140704808\nEpoch 149 loss is 0.8446035024910838\nEpoch 159 loss is 0.8153800610100258\nEpoch 169 loss is 0.860221271499099\nEpoch 179 loss is 0.8713979258843838\nEpoch 189 loss is 0.8423941483588755\nEpoch 199 loss is 0.7982078285674677\nTrain Acc.:  0.5234687119447946\n              precision    recall  f1-score   support\n\n           0       0.82      0.76      0.79      1064\n           1       0.73      0.59      0.66      1064\n           2       0.59      0.22      0.33      1064\n           3       0.75      0.68      0.71      1063\n           4       0.55      0.52      0.54      1064\n           5       0.49      0.48      0.48      1064\n           6       0.70      0.56      0.62      1063\n           7       0.88      0.77      0.83      1064\n           8       0.99      0.92      0.95      1064\n           9       0.76      0.57      0.65      1064\n          10       0.85      0.73      0.79      1064\n\n   micro avg       0.74      0.62      0.68     11702\n   macro avg       0.74      0.62      0.67     11702\nweighted avg       0.74      0.62      0.67     11702\n samples avg       0.57      0.62      0.58     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2881004625544315\nEpoch 19 loss is 0.257601894880915\nEpoch 29 loss is 0.3070432292199646\nEpoch 39 loss is 1.0114987272533784\nEpoch 49 loss is 1.0626524147227299\nEpoch 59 loss is 0.7220235506940849\nEpoch 69 loss is 0.8701604350827594\nEpoch 79 loss is 1.2108727994663198\nEpoch 89 loss is 1.138239338460986\nEpoch 99 loss is 1.4603121925675742\nEpoch 109 loss is 1.4954496178854135\nEpoch 119 loss is 1.2679945725843622\nEpoch 129 loss is 1.0164146082349543\nEpoch 139 loss is 1.1297298068650685\nEpoch 149 loss is 1.253219169435164\nEpoch 159 loss is 1.1736291050368979\nEpoch 169 loss is 1.1826341111065124\nEpoch 179 loss is 1.4031484346107788\nEpoch 189 loss is 1.2205340600894026\nEpoch 199 loss is 1.3048065655004846\nTrain Acc.:  0.7068600850300168\n              precision    recall  f1-score   support\n\n           0       0.75      0.86      0.80      1063\n           1       0.81      0.79      0.80      1064\n           2       0.94      0.85      0.89      1064\n           3       0.87      0.84      0.85      1063\n           4       0.73      0.48      0.58      1064\n           5       0.85      0.77      0.81      1064\n           6       0.84      0.65      0.73      1064\n           7       0.97      0.97      0.97      1064\n           8       0.99      0.95      0.97      1064\n           9       0.53      0.97      0.69      1064\n          10       0.92      0.76      0.83      1064\n\n   micro avg       0.81      0.81      0.81     11702\n   macro avg       0.84      0.81      0.81     11702\nweighted avg       0.84      0.81      0.81     11702\n samples avg       0.75      0.81      0.77     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.5949722022582887\nEpoch 19 loss is 0.8542236273844039\nEpoch 29 loss is 0.8749541100255886\nEpoch 39 loss is 0.9226380212308712\nEpoch 49 loss is 0.9189451568276324\nEpoch 59 loss is 0.9365599144446709\nEpoch 69 loss is 0.9302873031362572\nEpoch 79 loss is 0.9133083164488084\nEpoch 89 loss is 0.867765838931149\nEpoch 99 loss is 0.9202292373715069\nEpoch 109 loss is 0.8840360283963321\nEpoch 119 loss is 0.8900084272652079\nEpoch 129 loss is 0.8624389865444407\nEpoch 139 loss is 0.86264759032102\nEpoch 149 loss is 0.877634036686458\nEpoch 159 loss is 0.8911980209206599\nEpoch 169 loss is 0.8815296026365007\nEpoch 179 loss is 0.8890566057952398\nEpoch 189 loss is 0.8732226294946295\nEpoch 199 loss is 0.9232258855476119\nTrain Acc.:  0.5140470442455188\n              precision    recall  f1-score   support\n\n           0       0.80      0.66      0.72      1064\n           1       0.72      0.55      0.62      1064\n           2       0.75      0.53      0.62      1064\n           3       0.75      0.55      0.64      1064\n           4       0.49      0.36      0.42      1064\n           5       0.76      0.72      0.74      1064\n           6       0.56      0.38      0.45      1063\n           7       0.94      0.90      0.92      1064\n           8       0.98      0.97      0.98      1063\n           9       0.64      0.47      0.54      1064\n          10       0.75      0.50      0.60      1064\n\n   micro avg       0.75      0.60      0.67     11702\n   macro avg       0.74      0.60      0.66     11702\nweighted avg       0.74      0.60      0.66     11702\n samples avg       0.55      0.60      0.57     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-20-20-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-20-20-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 20)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(20, 20)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(20, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-20-20-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-20-20-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=21bd2a05ea18403b8c16084955a62c7b\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/21bd2a05ea18403b8c16084955a62c7b/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-20-20-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-28 01:07:31,491 - clearml.frameworks - INFO - Found existing registered model id=22ba5a4169ed406a9e74f40200bd29a1 [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-20-20-11.pt] reusing it.\nEpoch 9 loss is 0.37705022557280987\nEpoch 19 loss is 0.2740187803883329\nEpoch 29 loss is 0.45516830949325443\nEpoch 39 loss is 0.6148356752471408\nEpoch 49 loss is 0.6222640142110359\nEpoch 59 loss is 0.6982017039915205\nEpoch 69 loss is 0.6373358815772685\nEpoch 79 loss is 0.6311755328631491\nEpoch 89 loss is 0.6082573092923741\nEpoch 99 loss is 0.603828268133041\nEpoch 109 loss is 0.6389025164997437\nEpoch 119 loss is 0.6260305680358405\nEpoch 129 loss is 0.6324748094532601\nEpoch 139 loss is 0.5943924798856283\nEpoch 149 loss is 0.6344508674597645\nEpoch 159 loss is 0.6141789833696603\nEpoch 169 loss is 0.6086970307893493\nEpoch 179 loss is 0.5824319841233837\nEpoch 189 loss is 0.6191231138277477\nEpoch 199 loss is 0.5666659157355745\nTrain Acc.:  0.730681308351315\n              precision    recall  f1-score   support\n\n           0       0.94      0.91      0.92      1063\n           1       0.82      0.70      0.76      1064\n           2       0.86      0.73      0.79      1064\n           3       0.89      0.83      0.86      1064\n           4       0.84      0.81      0.82      1064\n           5       0.86      0.76      0.81      1063\n           6       0.75      0.87      0.81      1064\n           7       0.97      0.91      0.94      1064\n           8       1.00      0.97      0.99      1064\n           9       0.80      0.68      0.73      1064\n          10       0.70      0.53      0.60      1064\n\n   micro avg       0.86      0.79      0.82     11702\n   macro avg       0.86      0.79      0.82     11702\nweighted avg       0.86      0.79      0.82     11702\n samples avg       0.76      0.79      0.77     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28806597272279794\nEpoch 19 loss is 0.22970233591793254\nEpoch 29 loss is 0.28585426998250507\nEpoch 39 loss is 0.38502680568849784\nEpoch 49 loss is 0.7625332656829702\nEpoch 59 loss is 0.6732264557756368\nEpoch 69 loss is 0.6217295541960539\nEpoch 79 loss is 0.6893322196897432\nEpoch 89 loss is 0.6622742723169387\nEpoch 99 loss is 0.6960203626199706\nEpoch 109 loss is 0.6777245937457624\nEpoch 119 loss is 0.7176237176692954\nEpoch 129 loss is 0.742547024761961\nEpoch 139 loss is 0.7291200957471028\nEpoch 149 loss is 0.7470137933371792\nEpoch 159 loss is 0.7220883556014549\nEpoch 169 loss is 0.7159556336891169\nEpoch 179 loss is 0.7108887748361287\nEpoch 189 loss is 0.7436282315327294\nEpoch 199 loss is 0.694954785146677\nTrain Acc.:  0.7865276561198111\n              precision    recall  f1-score   support\n\n           0       0.95      0.94      0.94      1063\n           1       0.80      0.74      0.77      1064\n           2       0.90      0.76      0.82      1064\n           3       0.93      0.90      0.92      1063\n           4       0.85      0.81      0.83      1064\n           5       0.81      0.72      0.76      1064\n           6       0.76      0.70      0.73      1064\n           7       0.99      0.93      0.96      1064\n           8       1.00      0.98      0.99      1064\n           9       0.87      0.88      0.87      1064\n          10       0.91      0.89      0.90      1064\n\n   micro avg       0.89      0.84      0.86     11702\n   macro avg       0.89      0.84      0.86     11702\nweighted avg       0.89      0.84      0.86     11702\n samples avg       0.81      0.84      0.82     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2511134256926335\nEpoch 19 loss is 0.22011394441410292\nEpoch 29 loss is 0.22056851414206183\nEpoch 39 loss is 0.2551644861115903\nEpoch 49 loss is 0.47249132940476685\nEpoch 59 loss is 0.6799162460754719\nEpoch 69 loss is 0.6906088651202789\nEpoch 79 loss is 0.7231533868835432\nEpoch 89 loss is 0.738398383750411\nEpoch 99 loss is 0.7811543296378222\nEpoch 109 loss is 0.7535960303277581\nEpoch 119 loss is 0.7163676381713118\nEpoch 129 loss is 0.7103477423606941\nEpoch 139 loss is 0.7078374401246286\nEpoch 149 loss is 0.7199227568498184\nEpoch 159 loss is 0.7107330170199827\nEpoch 169 loss is 0.7056031770853555\nEpoch 179 loss is 0.7327502640763232\nEpoch 189 loss is 0.7094453790723616\nEpoch 199 loss is 0.7120579963814156\nTrain Acc.:  0.7947956502232572\n              precision    recall  f1-score   support\n\n           0       0.94      0.91      0.93      1064\n           1       0.85      0.80      0.82      1064\n           2       0.87      0.77      0.82      1064\n           3       0.93      0.86      0.89      1063\n           4       0.84      0.82      0.83      1064\n           5       0.85      0.86      0.86      1064\n           6       0.82      0.70      0.75      1063\n           7       0.96      0.90      0.93      1064\n           8       0.99      0.95      0.97      1064\n           9       0.86      0.87      0.86      1064\n          10       0.90      0.87      0.88      1064\n\n   micro avg       0.89      0.85      0.87     11702\n   macro avg       0.89      0.85      0.87     11702\nweighted avg       0.89      0.85      0.87     11702\n samples avg       0.82      0.85      0.83     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28920254058590406\nEpoch 19 loss is 0.20422793527357072\nEpoch 29 loss is 0.23514562707658523\nEpoch 39 loss is 0.30951769319469474\nEpoch 49 loss is 0.802884197886128\nEpoch 59 loss is 0.629426963749978\nEpoch 69 loss is 0.6618189152336303\nEpoch 79 loss is 0.639523350080704\nEpoch 89 loss is 0.6033113390242072\nEpoch 99 loss is 0.6365305132984823\nEpoch 109 loss is 0.6149712207622507\nEpoch 119 loss is 0.6332012921437696\nEpoch 129 loss is 0.6520249413523534\nEpoch 139 loss is 0.6450135768078111\nEpoch 149 loss is 0.6724773114718959\nEpoch 159 loss is 0.637959073743463\nEpoch 169 loss is 0.5887977689032348\nEpoch 179 loss is 0.6275123627575196\nEpoch 189 loss is 0.6157577124370395\nEpoch 199 loss is 0.5571599018372312\nTrain Acc.:  0.7962056957292712\n              precision    recall  f1-score   support\n\n           0       0.95      0.89      0.92      1063\n           1       0.84      0.81      0.83      1064\n           2       0.93      0.86      0.90      1064\n           3       0.89      0.87      0.88      1063\n           4       0.87      0.77      0.82      1064\n           5       0.84      0.81      0.83      1064\n           6       0.76      0.61      0.68      1064\n           7       0.99      0.98      0.98      1064\n           8       1.00      0.99      1.00      1064\n           9       0.83      0.77      0.80      1064\n          10       0.93      0.91      0.92      1064\n\n   micro avg       0.90      0.84      0.87     11702\n   macro avg       0.90      0.84      0.87     11702\nweighted avg       0.90      0.84      0.87     11702\n samples avg       0.82      0.84      0.83     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3162340976591865\nEpoch 19 loss is 0.23035400757661567\nEpoch 29 loss is 0.3025872000844228\nEpoch 39 loss is 0.5825874736764202\nEpoch 49 loss is 0.5903150798951642\nEpoch 59 loss is 0.5290111967616761\nEpoch 69 loss is 0.48948755510049063\nEpoch 79 loss is 0.5536584861929144\nEpoch 89 loss is 0.5714292967837328\nEpoch 99 loss is 0.6023988021147385\nEpoch 109 loss is 0.6004173386515079\nEpoch 119 loss is 0.5901975806362378\nEpoch 129 loss is 0.607343459187801\nEpoch 139 loss is 0.6290455412143036\nEpoch 149 loss is 0.598815193614319\nEpoch 159 loss is 0.5928518543280813\nEpoch 169 loss is 0.6891633674581874\nEpoch 179 loss is 0.6508888532882792\nEpoch 189 loss is 0.6662645081833795\nEpoch 199 loss is 0.6955834762648595\nTrain Acc.:  0.7916550943234987\n              precision    recall  f1-score   support\n\n           0       0.96      0.92      0.94      1064\n           1       0.78      0.69      0.73      1064\n           2       0.89      0.81      0.85      1064\n           3       0.94      0.87      0.91      1064\n           4       0.95      0.89      0.92      1064\n           5       0.82      0.78      0.80      1064\n           6       0.77      0.78      0.78      1063\n           7       0.99      0.98      0.98      1064\n           8       1.00      0.98      0.99      1063\n           9       0.78      0.75      0.77      1064\n          10       0.93      0.82      0.87      1064\n\n   micro avg       0.89      0.84      0.87     11702\n   macro avg       0.89      0.84      0.87     11702\nweighted avg       0.89      0.84      0.87     11702\n samples avg       0.82      0.84      0.82     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-50-50-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-50-50-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 50)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(50, 50)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(50, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-50-50-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": 1,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-50-50-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=fad0aa6b39774a1a941093cdfe9dbafe\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/fad0aa6b39774a1a941093cdfe9dbafe/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-50-50-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-28 01:32:30,550 - clearml.frameworks - INFO - Found existing registered model id=f13061c5d03a4e96b788becd5e54443a [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-50-50-11.pt] reusing it.\nEpoch 9 loss is 0.28872390806294274\nEpoch 19 loss is 0.17694585572096805\nEpoch 29 loss is 0.1425961563405807\nEpoch 39 loss is 0.12819857234772986\nEpoch 49 loss is 0.127805551287352\nEpoch 59 loss is 0.11392432030870624\nEpoch 69 loss is 0.09727040211071439\nEpoch 79 loss is 0.09686726724688864\nEpoch 89 loss is 0.09030257402693799\nEpoch 99 loss is 0.09027066309985042\nEpoch 109 loss is 0.08883324592656906\nEpoch 119 loss is 0.08907481171375083\nEpoch 129 loss is 0.08302457843559832\nEpoch 139 loss is 0.07895430465486417\nEpoch 149 loss is 0.07612614811554756\nEpoch 159 loss is 0.0712898657302396\nEpoch 169 loss is 0.08380639698034706\nEpoch 179 loss is 0.07950043128200916\nEpoch 189 loss is 0.08736764655981065\nEpoch 199 loss is 0.09124728414202767\nTrain Acc.:  0.9411199179609887\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.93      0.91      0.92      1064\n           2       0.99      0.98      0.98      1064\n           3       0.97      0.97      0.97      1064\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.94      0.93      1063\n           6       0.90      0.91      0.91      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.96      0.93      0.94      1064\n          10       0.99      0.97      0.98      1064\n\n   micro avg       0.96      0.95      0.96     11702\n   macro avg       0.96      0.95      0.96     11702\nweighted avg       0.96      0.95      0.96     11702\n samples avg       0.94      0.95      0.95     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.2861673749427943\nEpoch 19 loss is 0.2201654650800628\nEpoch 29 loss is 0.1594589809702913\nEpoch 39 loss is 0.13540895566103234\nEpoch 49 loss is 0.11925421445742292\nEpoch 59 loss is 0.11962160650088925\nEpoch 69 loss is 0.11841529734011198\nEpoch 79 loss is 0.1179027595753925\nEpoch 89 loss is 0.1180392008192653\nEpoch 99 loss is 0.11441886133599923\nEpoch 109 loss is 0.10782047828315887\nEpoch 119 loss is 0.10705718301534183\nEpoch 129 loss is 0.10427072867703205\nEpoch 139 loss is 0.10474216368094652\nEpoch 149 loss is 0.10551091888301718\nEpoch 159 loss is 0.10684330984217699\nEpoch 169 loss is 0.09317834493277641\nEpoch 179 loss is 0.0892558164282839\nEpoch 189 loss is 0.08739703451707423\nEpoch 199 loss is 0.09199028421820245\nTrain Acc.:  0.9275749353729144\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      1063\n           1       0.92      0.88      0.90      1064\n           2       0.98      0.98      0.98      1064\n           3       0.97      0.95      0.96      1063\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.95      0.94      1064\n           6       0.89      0.89      0.89      1064\n           7       0.99      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.93      0.94      1064\n          10       0.96      0.91      0.93      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.27623336473650534\nEpoch 19 loss is 0.16289381252737087\nEpoch 29 loss is 0.14757027886946592\nEpoch 39 loss is 0.1330734318315473\nEpoch 49 loss is 0.11679790998549922\nEpoch 59 loss is 0.11167467752516336\nEpoch 69 loss is 0.11268039833791481\nEpoch 79 loss is 0.10041605954925376\nEpoch 89 loss is 0.10264455966541974\nEpoch 99 loss is 0.10157209891611255\nEpoch 109 loss is 0.09768154280892237\nEpoch 119 loss is 0.09983357506318141\nEpoch 129 loss is 0.09887546788082555\nEpoch 139 loss is 0.0888411840117014\nEpoch 149 loss is 0.10038184391363476\nEpoch 159 loss is 0.08949124574344279\nEpoch 169 loss is 0.09784304006405145\nEpoch 179 loss is 0.09877557601509657\nEpoch 189 loss is 0.10217435454327402\nEpoch 199 loss is 0.1057261382783362\nTrain Acc.:  0.9322750870596278\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1064\n           1       0.91      0.91      0.91      1064\n           2       0.98      0.96      0.97      1064\n           3       0.97      0.96      0.96      1063\n           4       0.91      0.91      0.91      1064\n           5       0.93      0.93      0.93      1064\n           6       0.91      0.88      0.89      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.95      0.94      0.94      1064\n          10       0.98      0.95      0.97      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.28693840838047774\nEpoch 19 loss is 0.20104936966743658\nEpoch 29 loss is 0.15755177961335037\nEpoch 39 loss is 0.1288596346987444\nEpoch 49 loss is 0.12720331433512763\nEpoch 59 loss is 0.12023218874962663\nEpoch 69 loss is 0.11721781830240018\nEpoch 79 loss is 0.10656089263296499\nEpoch 89 loss is 0.09562316810246828\nEpoch 99 loss is 0.09102231637919944\nEpoch 109 loss is 0.09233467007446912\nEpoch 119 loss is 0.09115643363442122\nEpoch 129 loss is 0.08620270800916764\nEpoch 139 loss is 0.11083394580512573\nEpoch 149 loss is 0.10150878655964381\nEpoch 159 loss is 0.10271093352938551\nEpoch 169 loss is 0.10444855459263125\nEpoch 179 loss is 0.09594796640621853\nEpoch 189 loss is 0.09849182497560287\nEpoch 199 loss is 0.1036780848914241\nTrain Acc.:  0.924904394641827\n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.97      1063\n           1       0.91      0.88      0.89      1064\n           2       0.98      0.96      0.97      1064\n           3       0.96      0.97      0.97      1063\n           4       0.91      0.90      0.91      1064\n           5       0.91      0.91      0.91      1064\n           6       0.89      0.89      0.89      1064\n           7       1.00      0.99      0.99      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.93      0.93      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.95      0.94      0.95     11702\n   macro avg       0.95      0.94      0.95     11702\nweighted avg       0.95      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n/tmp/ipykernel_13327/161459083.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3034027193057795\nEpoch 19 loss is 0.17692890093552524\nEpoch 29 loss is 0.13786514238212272\nEpoch 39 loss is 0.12331394422598127\nEpoch 49 loss is 0.1237987767889628\nEpoch 59 loss is 0.11048595570606672\nEpoch 69 loss is 0.09570147391418123\nEpoch 79 loss is 0.09912258426639196\nEpoch 89 loss is 0.09599648621108799\nEpoch 99 loss is 0.0873050642910436\nEpoch 109 loss is 0.10142020273437256\nEpoch 119 loss is 0.1003337232560557\nEpoch 129 loss is 0.10045847144167147\nEpoch 139 loss is 0.09709811021654463\nEpoch 149 loss is 0.0970311573500608\nEpoch 159 loss is 0.09226866990186595\nEpoch 169 loss is 0.08758580671534028\nEpoch 179 loss is 0.08792805539701785\nEpoch 189 loss is 0.08792941901672958\nEpoch 199 loss is 0.09815684767604878\nTrain Acc.:  0.9300959258230607\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1064\n           1       0.91      0.90      0.91      1064\n           2       0.98      0.97      0.97      1064\n           3       0.98      0.95      0.97      1064\n           4       0.94      0.93      0.93      1064\n           5       0.93      0.93      0.93      1064\n           6       0.90      0.88      0.89      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.93      0.89      0.91      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.93     11702\n\n\n\n\ntask.mark_completed()\ntask.close()\n\n\n\nMLMVN [48-100-100-11]\n\nPATH = str(Path.cwd() / \"models/autass-mlmvn_48-100-100-11.pt\")\n\n\nclass Model(nn.Module):\n    def __init__(self, categories, periodicity):\n        super().__init__()\n        self.categories = categories\n        self.periodicity = periodicity\n        self.first_linear = FirstLayer(48, 100)\n        self.phase_act1 = cmplx_phase_activation()\n        self.hidden_layer = HiddenLayer(100, 100)\n        self.phase_act2 = cmplx_phase_activation()\n        self.linear_out = OutputLayer(100, 11)\n        self.phase_act3 = cmplx_phase_activation()\n        # Hooks\n        self.first_layer_hook_handle = self.first_linear.register_full_backward_hook(\n            self.first_layer_backward_hook\n        )\n        self.hidden_layer_hook_handle = self.hidden_layer.register_full_backward_hook(\n            self.hidden_layer_backward_hook\n        )\n        self.output_hook_handle = self.linear_out.register_full_backward_hook(\n            self.output_layer_backward_hook\n        )\n\n    def forward(self, x):\n        x = self.first_linear(x)\n        x = self.phase_act1(x)\n        x = self.hidden_layer(x)\n        x = self.phase_act2(x)\n        x = self.linear_out(x)\n        x = self.phase_act3(x)\n        return x\n\n    def first_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"first_layer\", module, grad_input, grad_output)\n\n    def hidden_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"hidden_layer\", module, grad_input, grad_output)\n\n    def output_layer_backward_hook(self, module, grad_input, grad_output):\n        fc_hook(\"output_layer\", module, grad_input, grad_output)\n\n    def angle2class(self, x: torch.tensor) -> torch.tensor:\n        tmp = x.angle() + 2 * np.pi\n        angle = torch.remainder(tmp, 2 * np.pi)\n\n        # This will be the discrete output (the number of sector)\n        o = torch.floor(self.categories * self.periodicity * angle / (2 * np.pi))\n        return torch.remainder(o, self.categories)\n\n    def predict(self, x):\n        \"\"\"\n        Performs the prediction task of the network\n\n        Args:\n          x: torch.Tensor\n            Input tensor of size ([3])\n\n        Returns:\n          Most likely class i.e., Label with the highest score\n        \"\"\"\n        # Pass the data through the networks\n        output = self.forward(x)\n\n        # # Choose the label with the highest score\n        # return torch.argmax(output, 1)\n        return self.angle2class(output)\n\n\ndef fit(model, X, y, epochs, batch_size, optimizer, criterion, categories, periodicity):\n    # List of losses for visualization\n    losses = []\n    scores = []\n    acc_best = 0\n\n    for i in range(epochs):\n        # Pass the data through the network and compute the loss\n        # We'll use the whole dataset during the training instead of using batches\n        # in to order to keep the code simple for now.\n\n        batch_loss = []\n\n        for j in range((X.shape[0] - 1) // batch_size + 1):\n            start_j = j * batch_size\n            end_j = start_j + batch_size\n            xb = X[start_j:end_j]\n            yb = y[start_j:end_j]\n\n            y_pred = model(xb)\n            loss = criterion(y_pred, yb, categories, periodicity)\n            batch_loss.append((torch.abs(loss)).detach().numpy())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step(inputs=xb, layers=list(model.children()))\n\n        losses.append(sum(batch_loss) / len(batch_loss))\n        if i % 10 == 9:\n            print(f\"Epoch {i} loss is {losses[-1]}\")\n        y_pred = model.predict(X)\n        scores.append(accuracy(y_pred.squeeze(), y))\n\n        if scores[-1] > acc_best:\n            acc_best = scores[-1]\n            torch.save(model.state_dict(), PATH)\n    return losses, scores\n\n\ntask = Task.init(\n    project_name=\"mlmvn\",\n    task_name=\"SDD-mlmvn-[48-100-100-11]\",\n    tags=[\"mlmvn\", \"SDD\", \"multiple_runs\", \"adjusted_loss_clip_angle_value\"],\n)\nwriter = SummaryWriter()\n\n#  capture a dictionary of hyperparameters with config\nconfig_dict = {\n    \"learning_rate\": lr,\n    \"epochs\": epochs,\n    \"batch_size\": batch_size,\n    \"optim\": \"ECL\",\n    \"categories\": categories,\n    \"periodicity\": periodicity,\n    \"layer\": \"[48-100-100-11]\",\n    \"loss\": \"ComplexMSE_adjusted_error\",\n    \"clip_angle_value\": clip_angle_value,\n}\ntask.connect(config_dict)\n\nClearML Task: created new task id=2153a3d90993455089c1f2cf7305bb59\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2153a3d90993455089c1f2cf7305bb59/output/log\n\n\n{'learning_rate': 1,\n 'epochs': 200,\n 'batch_size': 538,\n 'optim': 'ECL',\n 'categories': 2,\n 'periodicity': 1,\n 'layer': '[48-100-100-11]',\n 'loss': 'ComplexMSE_adjusted_error',\n 'clip_angle_value': 1000000}\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nlist_losses = []\nlist_scores = []\nlist_acc = []\nlist_loss = []\nlist_f1 = []\nlist_precision = []\nlist_recall = []\n\nfor train_index, test_index in sss.split(X, y):\n    model_dict: dict = {}\n    x_train, x_valid, y_train, y_valid = get_splitted_data_by_index(\n        X, y, neuronCats, train_index, test_index\n    )\n\n    model = Model(categories=categories, periodicity=periodicity)\n    criterion = ComplexMSE_adjusted_error.apply\n    optimizer = ECL(model.parameters(), lr=lr)\n\n    losses, scores = fit(\n        model,\n        x_train,\n        y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        optimizer=optimizer,\n        criterion=criterion,\n        categories=categories,\n        periodicity=periodicity,\n    )\n\n    model.load_state_dict(torch.load(PATH))\n\n    list_scores.append(scores)\n    list_losses.append(losses)\n\n    y_pred = model.predict(x_train)\n    acc = accuracy(y_pred.squeeze(), y_train)\n    print(\"Train Acc.: \", acc)\n\n    y_pred = model.predict(x_valid)\n    acc = accuracy(y_pred.squeeze(), y_valid)\n    list_acc.append(acc)\n\n    print(classification_report(y_valid, y_pred.detach().numpy(), zero_division=0))\n    list_f1.append(\n        f1_score(y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0)\n    )\n    list_precision.append(\n        precision_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n    list_recall.append(\n        recall_score(\n            y_valid, y_pred.detach().numpy(), average=\"weighted\", zero_division=0\n        )\n    )\n\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_mean\",\n    value=np.mean(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_accuracy_std\",\n    value=np.std(list_acc),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_mean\",\n    value=np.mean(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_f1_std\",\n    value=np.std(list_f1),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_mean\",\n    value=np.mean(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_precision_std\",\n    value=np.std(list_precision),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_mean\",\n    value=np.mean(list_recall),\n)\nLogger.current_logger().report_single_value(\n    name=\"val_recall_std\",\n    value=np.std(list_recall),\n)\n\n/tmp/ipykernel_2379/74840955.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\n2022-09-29 14:17:44,211 - clearml.frameworks - INFO - Found existing registered model id=bbd65d869dea4025af46d264d3c7bdee [/home/antonpfeifer/Documents/mlmvn/nbs/examples/autass/models/autass-mlmvn_48-100-100-11.pt] reusing it.\nEpoch 9 loss is 0.38493101436237465\nEpoch 19 loss is 0.1983475963903122\nEpoch 29 loss is 0.15240658259145864\nEpoch 39 loss is 0.12849809119734149\nEpoch 49 loss is 0.11291338514495056\nEpoch 59 loss is 0.10350603182093213\nEpoch 69 loss is 0.09896006818913682\nEpoch 79 loss is 0.09073580641413742\nEpoch 89 loss is 0.08624265189956253\nEpoch 99 loss is 0.07990884906610474\nEpoch 109 loss is 0.07774202154480073\nEpoch 119 loss is 0.0778207451859103\nEpoch 129 loss is 0.07292320971236593\nEpoch 139 loss is 0.08220096647261978\nEpoch 149 loss is 0.07259278084390995\nEpoch 159 loss is 0.07333743296400583\nEpoch 169 loss is 0.06768991486550634\nEpoch 179 loss is 0.06486413256485078\nEpoch 189 loss is 0.0663716789708822\nEpoch 199 loss is 0.06601747100440022\nTrain Acc.:  0.9551562800435832\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      1063\n           1       0.93      0.92      0.93      1064\n           2       0.99      0.98      0.98      1064\n           3       0.99      0.98      0.98      1064\n           4       0.95      0.92      0.93      1064\n           5       0.94      0.94      0.94      1063\n           6       0.92      0.91      0.92      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.94      0.92      0.93      1064\n          10       0.96      0.96      0.96      1064\n\n   micro avg       0.96      0.95      0.96     11702\n   macro avg       0.96      0.95      0.96     11702\nweighted avg       0.96      0.95      0.96     11702\n samples avg       0.94      0.95      0.95     11702\n\n\n\nClearML results page: http://194.94.231.172:8080/projects/cdefd6ee85454e49be01962ad715eca0/experiments/2153a3d90993455089c1f2cf7305bb59/output/log\nClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n\n\n/tmp/ipykernel_2379/74840955.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.4015647773116825\nEpoch 19 loss is 0.21324193953061343\nEpoch 29 loss is 0.16244134629623966\nEpoch 39 loss is 0.13968662879021496\nEpoch 49 loss is 0.1220346592068794\nEpoch 59 loss is 0.11857350072574051\nEpoch 69 loss is 0.10683356304935711\nEpoch 79 loss is 0.09876512760527531\nEpoch 89 loss is 0.10028214707730271\nEpoch 99 loss is 0.09386611501220359\nEpoch 109 loss is 0.09131180958378789\nEpoch 119 loss is 0.08078602213530815\nEpoch 129 loss is 0.07812674516622654\nEpoch 139 loss is 0.07368491314841398\nEpoch 149 loss is 0.07233171681695082\nEpoch 159 loss is 0.07153585909897314\nEpoch 169 loss is 0.07045204344853243\nEpoch 179 loss is 0.07031483192734578\nEpoch 189 loss is 0.06762652182875349\nEpoch 199 loss is 0.06935222633418742\nTrain Acc.:  0.9489606255474609\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.97      1063\n           1       0.94      0.90      0.92      1064\n           2       0.98      0.97      0.97      1064\n           3       0.98      0.97      0.97      1063\n           4       0.95      0.94      0.94      1064\n           5       0.94      0.93      0.93      1064\n           6       0.91      0.89      0.90      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      0.99      1.00      1064\n           9       0.96      0.94      0.95      1064\n          10       0.98      0.95      0.97      1064\n\n   micro avg       0.96      0.95      0.96     11702\n   macro avg       0.96      0.95      0.96     11702\nweighted avg       0.96      0.95      0.96     11702\n samples avg       0.94      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_2379/74840955.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.35830112541567166\nEpoch 19 loss is 0.20503370425712947\nEpoch 29 loss is 0.1615004405052233\nEpoch 39 loss is 0.14478501770353844\nEpoch 49 loss is 0.13139543735043546\nEpoch 59 loss is 0.12132454251093122\nEpoch 69 loss is 0.11744575716216751\nEpoch 79 loss is 0.11623840924937114\nEpoch 89 loss is 0.11783017747510419\nEpoch 99 loss is 0.11593023945022461\nEpoch 109 loss is 0.11583587087231882\nEpoch 119 loss is 0.11662312303478367\nEpoch 129 loss is 0.11691969687131881\nEpoch 139 loss is 0.10854572296958728\nEpoch 149 loss is 0.10796596240824434\nEpoch 159 loss is 0.11262345830304454\nEpoch 169 loss is 0.10184479776708627\nEpoch 179 loss is 0.09605088202087457\nEpoch 189 loss is 0.09299509987805853\nEpoch 199 loss is 0.08898415836437445\nTrain Acc.:  0.9210374516632127\n              precision    recall  f1-score   support\n\n           0       0.97      0.95      0.96      1064\n           1       0.91      0.89      0.90      1064\n           2       0.98      0.95      0.97      1064\n           3       0.96      0.94      0.95      1063\n           4       0.92      0.88      0.90      1064\n           5       0.90      0.90      0.90      1064\n           6       0.89      0.88      0.89      1063\n           7       1.00      0.99      0.99      1064\n           8       1.00      0.99      1.00      1064\n           9       0.93      0.88      0.90      1064\n          10       0.97      0.95      0.96      1064\n\n   micro avg       0.95      0.93      0.94     11702\n   macro avg       0.95      0.93      0.94     11702\nweighted avg       0.95      0.93      0.94     11702\n samples avg       0.91      0.93      0.92     11702\n\n\n\n/tmp/ipykernel_2379/74840955.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3732331507452995\nEpoch 19 loss is 0.2151142782886408\nEpoch 29 loss is 0.1693622907950432\nEpoch 39 loss is 0.14709019657419897\nEpoch 49 loss is 0.13360652890401162\nEpoch 59 loss is 0.12738859159159088\nEpoch 69 loss is 0.11634673967810212\nEpoch 79 loss is 0.10559853905524143\nEpoch 89 loss is 0.10185721106569144\nEpoch 99 loss is 0.10076346263612189\nEpoch 109 loss is 0.0951012106512096\nEpoch 119 loss is 0.0863301443776767\nEpoch 129 loss is 0.08374395534767522\nEpoch 139 loss is 0.08192660695129174\nEpoch 149 loss is 0.0808713694226275\nEpoch 159 loss is 0.08485596063007796\nEpoch 169 loss is 0.07577642509867422\nEpoch 179 loss is 0.07399703079870644\nEpoch 189 loss is 0.07316694645295278\nEpoch 199 loss is 0.07004600393121947\nTrain Acc.:  0.9438972803213195\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1063\n           1       0.93      0.91      0.92      1064\n           2       0.98      0.96      0.97      1064\n           3       0.97      0.97      0.97      1063\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.92      0.93      1064\n           6       0.90      0.89      0.89      1064\n           7       1.00      1.00      1.00      1064\n           8       1.00      1.00      1.00      1064\n           9       0.93      0.90      0.92      1064\n          10       0.96      0.95      0.96      1064\n\n   micro avg       0.96      0.95      0.95     11702\n   macro avg       0.96      0.95      0.95     11702\nweighted avg       0.96      0.95      0.95     11702\n samples avg       0.93      0.95      0.94     11702\n\n\n\n/tmp/ipykernel_2379/74840955.py:46: UserWarning:\n\nTo copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n\n\n\nEpoch 9 loss is 0.3860148494765397\nEpoch 19 loss is 0.20866529165214284\nEpoch 29 loss is 0.16247025892118347\nEpoch 39 loss is 0.13564982800723918\nEpoch 49 loss is 0.1240512973257284\nEpoch 59 loss is 0.113934101224775\nEpoch 69 loss is 0.11061596169703837\nEpoch 79 loss is 0.10461476570112285\nEpoch 89 loss is 0.10298748909019034\nEpoch 99 loss is 0.09786684313663066\nEpoch 109 loss is 0.0988537740332937\nEpoch 119 loss is 0.09090053269267047\nEpoch 129 loss is 0.08742530763329877\nEpoch 139 loss is 0.0841747421040788\nEpoch 149 loss is 0.08610848018045644\nEpoch 159 loss is 0.07819941018533352\nEpoch 169 loss is 0.07596165720615952\nEpoch 179 loss is 0.07426260096075489\nEpoch 189 loss is 0.07633927124679218\nEpoch 199 loss is 0.07405352525764859\nTrain Acc.:  0.9421454056017262\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      1064\n           1       0.91      0.90      0.91      1064\n           2       0.97      0.96      0.96      1064\n           3       0.97      0.96      0.97      1064\n           4       0.94      0.92      0.93      1064\n           5       0.93      0.94      0.94      1064\n           6       0.91      0.88      0.90      1063\n           7       1.00      0.99      1.00      1064\n           8       1.00      1.00      1.00      1063\n           9       0.93      0.90      0.92      1064\n          10       0.98      0.95      0.97      1064\n\n   micro avg       0.96      0.94      0.95     11702\n   macro avg       0.96      0.94      0.95     11702\nweighted avg       0.96      0.94      0.95     11702\n samples avg       0.93      0.94      0.94     11702\n\n\n\n\nplot_loss_acc_list(\n    \"$\\mathbb{C}^{*}$: [48-100-100-11] \",\n    list_losses,\n    list_scores,\n    \"mlmvn-mod-48-100-100-11.png\",\n)\n\n\n\n\n\ntask.mark_completed()\ntask.close()"
  },
  {
    "objectID": "optim.html",
    "href": "optim.html",
    "title": "Optimizer",
    "section": "",
    "text": "source\n\n\n\n ecl (params:List[torch.Tensor], d_p_list:List[torch.Tensor],\n      inputs:torch.Tensor, layers:List,\n      momentum_buffer_list:List[Optional[torch.Tensor]],\n      has_sparse_grad:bool=None, foreach:bool=None, weight_decay:float,\n      momentum:float, lr:float, dampening:float, nesterov:bool,\n      maximize:bool, clip_angle_value:float)\n\nFunctional API that performs SGD algorithm computation.\nSee :class:~torch.optim.SGD for details.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nList\n\n\n\n\nd_p_list\nList\n\n\n\n\ninputs\nTensor\n\n\n\n\nlayers\nList\n\n\n\n\nmomentum_buffer_list\nList\n\n\n\n\nhas_sparse_grad\nbool\nNone\nkwonly args with defaults are not supported by functions compiled with torchscript issue #70627setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n\n\nforeach\nbool\nNone\n\n\n\nweight_decay\nfloat\n\n\n\n\nmomentum\nfloat\n\n\n\n\nlr\nfloat\n\n\n\n\ndampening\nfloat\n\n\n\n\nnesterov\nbool\n\n\n\n\nmaximize\nbool\n\n\n\n\nclip_angle_value\nfloat\n\n\n\n\n\n\nsource\n\n\n\n\n ECL (params, lr=<required parameter>, momentum=0, dampening=0,\n      weight_decay=0, nesterov=False, maximize=False,\n      foreach:Optional[bool]=None, clip_angle_value=0)\n\nBase class for all optimizers.\n.. warning:: Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries.\nArgs: params (iterable): an iterable of :class:torch.Tensor s or :class:dict s. Specifies what Tensors should be optimized. defaults: (dict): a dict containing default values of optimization options (used when a parameter group doesn’t specify them)."
  },
  {
    "objectID": "optim.html#custom-sgd",
    "href": "optim.html#custom-sgd",
    "title": "Optimizer",
    "section": "Custom SGD",
    "text": "Custom SGD\n\nsource\n\nsgd\n\n sgd (params:List[torch.Tensor], d_p_list:List[torch.Tensor],\n      momentum_buffer_list:List[Optional[torch.Tensor]],\n      has_sparse_grad:bool=None, foreach:bool=None, weight_decay:float,\n      momentum:float, lr:float, dampening:float, nesterov:bool,\n      maximize:bool)\n\nFunctional API that performs SGD algorithm computation.\nSee :class:~torch.optim.SGD for details.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparams\nList\n\n\n\n\nd_p_list\nList\n\n\n\n\nmomentum_buffer_list\nList\n\n\n\n\nhas_sparse_grad\nbool\nNone\nkwonly args with defaults are not supported by functions compiled with torchscript issue #70627setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n\n\nforeach\nbool\nNone\n\n\n\nweight_decay\nfloat\n\n\n\n\nmomentum\nfloat\n\n\n\n\nlr\nfloat\n\n\n\n\ndampening\nfloat\n\n\n\n\nnesterov\nbool\n\n\n\n\nmaximize\nbool\n\n\n\n\n\n\nsource\n\n\nMySGD\n\n MySGD (params, lr=<required parameter>, momentum=0, dampening=0,\n        weight_decay=0, nesterov=False, maximize=False,\n        foreach:Optional[bool]=None)\n\nBase class for all optimizers.\n.. warning:: Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries.\nArgs: params (iterable): an iterable of :class:torch.Tensor s or :class:dict s. Specifies what Tensors should be optimized. defaults: (dict): a dict containing default values of optimization options (used when a parameter group doesn’t specify them)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLMVN",
    "section": "",
    "text": "Clone this repository\ngit clone https://github.com/antonpf/mlmvn.git\nand install the required packages\ncd mlmvn\nconda env create -f environment.yml\nconda activate mlmvn\nNext, the mlmvn package can be installed with pip\npip install ."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "MLMVN",
    "section": "How to use",
    "text": "How to use\nAs a small example, the XOR problem is described here. The XOR problem is an example of how a single real-valued neuron cannot learn a simple but non-linear relationship. At least, this holds if we do not extend the dimensionality of the feature space.\n\nSetup\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom mlmvn.layers import FirstLayer, OutputLayer, cmplx_phase_activation\nfrom mlmvn.loss import ComplexMSELoss\nfrom mlmvn.optim import ECL\n\n\n\nLoading Data\nThe dataset contains four input-output mappings with binary classes. The two-dimensional input \\(x\\) is mapped to a class label \\(y\\). The following table shows the truth table with associated labels for the XOR gate.\n\\[\n\\begin{aligned}\n    \\begin{array}{cc|c|cc}\n        x_1 & x_2 & y & z & arg(z) \\\\\n        \\hline\n        1 &  1  & 0 &  1+j &  45° \\\\\n        1 & -1  & 1 &  1-j & 315° \\\\\n        -1 &  1 & 1 & -1+j & 135° \\\\\n        -1 & -1 & 0 & -1-j & 225° \\\\\n    \\end{array}\n\\end{aligned}\n\\]\n\n# Create data\nx = torch.Tensor([[1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0]])\nx = x.type(torch.cdouble)\ny = torch.Tensor([0.0, 1.0, 1.0, 0.0]).reshape(x.shape[0], 1)\n\n\n\nCreating Models\n\nclass BasicModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = FirstLayer(2, 2)\n        self.phase_act = cmplx_phase_activation()\n        self.linear1 = OutputLayer(2, 1)\n        self.phase_act = cmplx_phase_activation()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.linear1(x)\n        x = self.phase_act(x)\n        return x\n\n\nmodel = BasicModel()\ncriterion = ComplexMSELoss.apply\noptimizer = ECL(model.parameters(), lr=1)\ncategories = 2\nperiodicity = 2"
  },
  {
    "objectID": "index.html#training",
    "href": "index.html#training",
    "title": "MLMVN",
    "section": "Training",
    "text": "Training\n\nfor t in range(5):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n    loss = criterion(y_pred, y, categories, periodicity)\n    print(t, torch.abs(loss))\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step(inputs=x, layers=list(model.children()))\n\n0 tensor(0.2833, dtype=torch.float64, grad_fn=<AbsBackward0>)\n1 tensor(1.3938, dtype=torch.float64, grad_fn=<AbsBackward0>)\n2 tensor(0.3198, dtype=torch.float64, grad_fn=<AbsBackward0>)\n3 tensor(0.0371, dtype=torch.float64, grad_fn=<AbsBackward0>)\n4 tensor(0.0036, dtype=torch.float64, grad_fn=<AbsBackward0>)\n\n\n\nEvaluation\n\npredictions = model(x)\n\n\ndef angle2class(x: torch.tensor, categories, periodicity) -> torch.tensor:\n    tmp = x.angle() + 2 * np.pi\n    angle = torch.remainder(tmp, 2 * np.pi)\n\n    # This will be the discrete output (the number of sector)\n    o = torch.floor(categories * periodicity * angle / (2 * np.pi))\n    return torch.remainder(o, categories)\n\n\nangle2class(predictions, 2, 2)\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]], dtype=torch.float64, grad_fn=<RemainderBackward0>)"
  }
]